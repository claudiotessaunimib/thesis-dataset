/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.wlm;
import com.amazonaws.ml.mms.metrics.Dimension;
import com.amazonaws.ml.mms.metrics.Metric;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.util.Connector;
import com.amazonaws.ml.mms.util.NettyUtils;
import com.amazonaws.ml.mms.util.codec.ModelRequestEncoder;
import com.amazonaws.ml.mms.util.codec.ModelResponseDecoder;
import com.amazonaws.ml.mms.util.messages.BaseModelRequest;
import com.amazonaws.ml.mms.util.messages.InputParameter;
import com.amazonaws.ml.mms.util.messages.ModelWorkerResponse;
import com.amazonaws.ml.mms.util.messages.RequestInput;
import com.amazonaws.ml.mms.util.messages.WorkerCommands;
import io.netty.bootstrap.Bootstrap;
import io.netty.channel.Channel;
import io.netty.channel.ChannelFutureListener;
import io.netty.channel.ChannelHandler;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.ChannelPipeline;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.SimpleChannelInboundHandler;
import io.netty.handler.codec.http.HttpResponseStatus;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.net.SocketAddress;
import java.nio.channels.Channels;
import java.util.UUID;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicReference;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class WorkerThread implements Runnable {
    static final Logger logger = LoggerFactory.getLogger(WorkerThread.class);
    private static final Logger loggerMmsMetrics =
            LoggerFactory.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
    private Metric workerLoadTime;
    private static final int[] BACK_OFF = {
        0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597
    };
    static final long WORKER_TIMEOUT = ConfigManager.getInstance().isDebug() ? Long.MAX_VALUE : 2L;
    static final ModelRequestEncoder ENCODER =
            new ModelRequestEncoder(ConfigManager.getInstance().getPreferDirectBuffer());
    private EventLoopGroup backendEventGroup;
    private int port;
    private Model model;
    private Channel backendChannel;
    private AtomicBoolean running = new AtomicBoolean(true);
    private int backoffIdx;
    private BatchAggregator aggregator;
    private WorkerStateListener listener;
    ArrayBlockingQueue<ModelWorkerResponse> replies;
    private int gpuId;
    private long memory;
    private long startTime;
    private AtomicReference<Thread> currentThread = new AtomicReference<>();
    private String workerId;
    private String threadName;
    private BaseModelRequest req;
    private WorkerState state;
    private WorkerLifeCycle lifeCycle;
    private boolean serverThread;
    private RandomAccessFile out;
    private RandomAccessFile err;
    private Connector connector;
    public WorkerState getState() {
        return state;
    }
    public WorkerLifeCycle getLifeCycle() {
        return lifeCycle;
    }
    public WorkerThread(
            ConfigManager configManager,
            EventLoopGroup backendEventGroup,
            int port,
            int gpuId,
            Model model,
            BatchAggregator aggregator,
            WorkerStateListener listener,
            int threadNumber,
            boolean serverThread) {
        this.workerId = String.valueOf(port); // Unique across all workers.
        this.backendEventGroup = backendEventGroup;
        this.port = port;
        this.model = model;
        this.aggregator = aggregator;
        this.gpuId = gpuId;
        this.listener = listener;
        startTime = System.currentTimeMillis();
        lifeCycle = new WorkerLifeCycle(configManager, model);
        replies = new ArrayBlockingQueue<>(1);
        this.serverThread = serverThread;
        this.threadName =
                !serverThread
                        ? "W-"
                                + model.getModelName()
                                        .substring(0, Math.min(model.getModelName().length(), 25))
                                + '-'
                                + threadNumber
                        : "BackendServer-" + model.getModelName();
        workerLoadTime =
                new Metric(
                        getWorkerName(),
                        String.valueOf(System.currentTimeMillis()),
                        "ms",
                        ConfigManager.getInstance().getHostName(),
                        new Dimension("Level", "Host"));
    }
    private void runWorker()
            throws WorkerInitializationException, InterruptedException, FileNotFoundException {
        int responseTimeoutSeconds = model.getResponseTimeoutSeconds();
        while (isRunning()) {
            req = aggregator.getRequest(backendChannel.id().asLongText(), state);
            backendChannel.writeAndFlush(req).sync();
            long begin = System.currentTimeMillis();
            // TODO: Change this to configurable param
            ModelWorkerResponse reply = replies.poll(responseTimeoutSeconds, TimeUnit.SECONDS);
            long duration = System.currentTimeMillis() - begin;
            logger.info("Backend response time: {}", duration);
            if (reply != null) {
                aggregator.sendResponse(reply);
            } else {
                int val = model.incrFailedInfReqs();
                logger.error("Number or consecutive unsuccessful inference {}", val);
                throw new WorkerInitializationException(
                        "Backend worker did not respond in given time");
            }
            switch (req.getCommand()) {
                case PREDICT:
                    model.resetFailedInfReqs();
                    break;
                case LOAD:
                    String message = reply.getMessage();
                    String tmpdir = System.getProperty("java.io.tmpdir");
                    out =
                            new RandomAccessFile(
                                    tmpdir + '/' + backendChannel.id().asLongText() + "-stdout",
                                    "rw");
                    err =
                            new RandomAccessFile(
                                    tmpdir + '/' + backendChannel.id().asLongText() + "-stderr",
                                    "rw");
                    if (reply.getCode() == 200) {
                        setState(WorkerState.WORKER_MODEL_LOADED, HttpResponseStatus.OK);
                        lifeCycle.setPid(
                                Integer.parseInt(
                                        message.substring(
                                                message.indexOf("[PID]:") + 6, message.length())));
                        lifeCycle.attachIOStreams(
                                threadName,
                                Channels.newInputStream(out.getChannel()),
                                Channels.newInputStream(err.getChannel()));
                        backoffIdx = 0;
                    } else {
                        setState(
                                WorkerState.WORKER_ERROR,
                                HttpResponseStatus.valueOf(reply.getCode()));
                    }
                    break;
                case UNLOAD:
                case STATS:
                default:
                    break;
            }
            req = null;
        }
    }
    @Override
    public void run() {
        Process process = null;
        Thread thread = Thread.currentThread();
        thread.setName(getWorkerName());
        currentThread.set(thread);
        HttpResponseStatus status = HttpResponseStatus.INTERNAL_SERVER_ERROR;
        try {
            if (!serverThread) {
                connect();
                runWorker();
            } else {
                // TODO: Move this logic to a seperate ServerThread class
                // This is server thread and shouldn't come out as long as process exists in CPU.
                model.setPort(port);
                lifeCycle.startBackendServer(port);
                setState(WorkerState.WORKER_MODEL_LOADED, HttpResponseStatus.OK);
                process = lifeCycle.getProcess();
                process.waitFor();
            }
        } catch (InterruptedException e) {
            if (state == WorkerState.WORKER_SCALED_DOWN) {
                logger.debug("Shutting down the thread .. Scaling down.");
            } else {
                logger.debug(
                        "Backend worker monitoring thread interrupted or backend worker process died.",
                        e);
            }
        } catch (WorkerInitializationException e) {
            logger.error("Backend worker error", e);
        } catch (OutOfMemoryError oom) {
            logger.error("Out of memory error when creating workers", oom);
            status = HttpResponseStatus.INSUFFICIENT_STORAGE;
        } catch (Throwable t) {
            logger.warn("Backend worker thread exception.", t);
        } finally {
            // WorkerThread is running in thread pool, the thread will be assigned to next
            // Runnable once this worker is finished. If currentThread keep holding the reference
            // of the thread, currentThread.interrupt() might kill next worker.
            backendChannel.disconnect();
            currentThread.set(null);
            Integer exitValue = lifeCycle.getExitValue();
            if (exitValue != null && exitValue == 137) {
                status = HttpResponseStatus.INSUFFICIENT_STORAGE;
            }
            if (!serverThread && req != null) {
                aggregator.sendError(req, "Worker died.", status);
            } else if (serverThread) {
                model.setPort(-1);
                if (process != null && process.isAlive()) {
                    process.destroyForcibly();
                    try {
                        process.waitFor(1, TimeUnit.SECONDS);
                    } catch (InterruptedException e) {
                        logger.warn(
                                "WorkerThread interrupted during waitFor, possible asynch resource cleanup.");
                    }
                }
            }
            setState(WorkerState.WORKER_STOPPED, status);
            lifeCycle.exit();
            retry();
        }
    }
    public String getWorkerId() {
        return workerId;
    }
    public long getMemory() {
        return memory;
    }
    public void setMemory(long memory) {
        this.memory = memory;
    }
    private void connect()
            throws WorkerInitializationException, InterruptedException, FileNotFoundException {
        if (!this.serverThread && (model.getPort() == -1)) {
            throw new WorkerInitializationException("Backend server is not runniing");
        }
        String modelName = model.getModelName();
        setState(WorkerState.WORKER_STARTED, HttpResponseStatus.OK);
        final CountDownLatch latch = new CountDownLatch(1);
        final int responseBufferSize = ConfigManager.getInstance().getMaxResponseSize();
        try {
            connector = new Connector(port);
            Bootstrap b = new Bootstrap();
            b.group(backendEventGroup)
                    .channel(connector.getClientChannel())
                    .handler(
                            new ChannelInitializer<Channel>() {
                                @Override
                                public void initChannel(Channel ch) {
                                    ChannelPipeline p = ch.pipeline();
                                    p.addLast(ENCODER);
                                    p.addLast(new ModelResponseDecoder(responseBufferSize));
                                    p.addLast(new WorkerHandler());
                                }
                            });
            SocketAddress address = connector.getSocketAddress();
            logger.info("Connecting to: {}", address);
            backendChannel = b.connect(address).sync().channel();
            backendChannel
                    .closeFuture()
                    .addListener(
                            (ChannelFutureListener)
                                    future -> {
                                        latch.countDown();
                                        logger.info(
                                                "{} Worker disconnected. {}", getWorkerId(), state);
                                        Thread thread = currentThread.getAndSet(null);
                                        if (thread != null) {
                                            thread.interrupt();
                                        }
                                    });
            backendChannel
                    .newSucceededFuture()
                    .addListener(
                            (ChannelFutureListener)
                                    future -> {
                                        // TODO:
                                        // use gpu, batch size in load model command
                                        RequestInput input =
                                                new RequestInput(UUID.randomUUID().toString());
                                        if (gpuId >= 0) {
                                            input.addParameter(
                                                    new InputParameter(
                                                            "gpu", String.valueOf(gpuId)));
                                        }
                                        Job job =
                                                new Job(
                                                        null,
                                                        modelName,
                                                        WorkerCommands.LOAD,
                                                        input);
                                        model.addJob(backendChannel.id().asLongText(), job);
                                        latch.countDown();
                                    });
            if (!latch.await(WORKER_TIMEOUT, TimeUnit.MINUTES)) {
                throw new WorkerInitializationException(
                        "Worker failed to initialize within " + WORKER_TIMEOUT + " mins");
            }
            workerId = workerId + "-" + backendChannel.id();
            running.set(true);
        } catch (Throwable t) {
            // https://github.com/netty/netty/issues/2597
            if (t instanceof IOException) {
                throw new WorkerInitializationException("Failed to connect to worker.", t);
            }
            throw t;
        }
    }
    public boolean isRunning() {
        return running.get();
    }
    public int getGpuId() {
        return gpuId;
    }
    public long getStartTime() {
        return startTime;
    }
    public int getPid() {
        return lifeCycle.getPid();
    }
    public void shutdown() {
        running.set(false);
        setState(WorkerState.WORKER_SCALED_DOWN, HttpResponseStatus.OK);
        if (backendChannel != null) {
            model.removeJobQueue(backendChannel.id().asLongText());
            backendChannel.close();
        }
        if (this.serverThread && this.connector != null) {
            logger.debug("Cleaning connector socket");
            this.connector.clean();
        }
        logger.debug("Terminating IOStreams for worker thread shutdown");
        lifeCycle.terminateIOStreams();
        try {
            if (out != null) {
                out.close();
            }
            if (err != null) {
                err.close();
            }
        } catch (IOException e) {
            logger.error("Failed to close IO file handles", e);
        }
        Thread thread = currentThread.getAndSet(null);
        if (thread != null) {
            thread.interrupt();
            aggregator.sendError(
                    null, "Worker scaled down.", HttpResponseStatus.INTERNAL_SERVER_ERROR);
        }
    }
    public boolean isServerThread() {
        return serverThread;
    }
    private final String getWorkerName() {
        String modelName = model.getModelName();
        if (modelName.length() > 25) {
            modelName = modelName.substring(0, 25);
        }
        return "W-" + port + '-' + modelName;
    }
    void setState(WorkerState newState, HttpResponseStatus status) {
        listener.notifyChangeState(model.getModelName(), newState, status);
        logger.debug("{} State change {} -> {}", getWorkerName(), state, newState);
        long timeTaken = System.currentTimeMillis() - startTime;
        if (state != WorkerState.WORKER_SCALED_DOWN) {
            // Don't update the state if it was terminated on purpose.. Scaling in..
            this.state = newState;
        }
        if (state == WorkerState.WORKER_MODEL_LOADED) {
            workerLoadTime.setValue(String.valueOf(timeTaken));
            workerLoadTime.setTimestamp(
                    String.valueOf(TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis())));
            loggerMmsMetrics.info("{}", workerLoadTime);
        }
    }
    void retry() {
        if (state == WorkerState.WORKER_SCALED_DOWN) {
            logger.debug("Worker terminated due to scale-down call.");
            return;
        }
        ModelManager manager = ModelManager.getInstance();
        if (backoffIdx < BACK_OFF.length - 1) {
            ++backoffIdx;
        }
        manager.getScheduler()
                .schedule(() -> manager.submitTask(this), BACK_OFF[backoffIdx], TimeUnit.SECONDS);
        logger.info("Retry worker: {} in {} seconds.", workerId, BACK_OFF[backoffIdx]);
    }
    @ChannelHandler.Sharable
    private class WorkerHandler extends SimpleChannelInboundHandler<ModelWorkerResponse> {
        @Override
        public void channelRead0(ChannelHandlerContext ctx, ModelWorkerResponse msg) {
            if (!replies.offer(msg)) {
                throw new IllegalStateException("Reply queue is full.");
            }
        }
        @Override
        public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
            logger.error("Unknown exception", cause);
            if (cause instanceof OutOfMemoryError) {
                NettyUtils.sendError(ctx, HttpResponseStatus.INSUFFICIENT_STORAGE, cause);
            }
            ctx.close();
        }
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.wlm;
import com.amazonaws.ml.mms.archive.Manifest;
import com.amazonaws.ml.mms.archive.ModelArchive;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.archive.ModelNotFoundException;
import com.amazonaws.ml.mms.http.ConflictStatusException;
import com.amazonaws.ml.mms.http.StatusResponse;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.util.NettyUtils;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.HttpResponseStatus;
import java.io.IOException;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeoutException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public final class ModelManager {
    private static final Logger logger = LoggerFactory.getLogger(ModelManager.class);
    private static ModelManager modelManager;
    private ConfigManager configManager;
    private WorkLoadManager wlm;
    private ConcurrentHashMap<String, Model> models;
    private HashSet<String> startupModels;
    private ScheduledExecutorService scheduler;
    private ModelManager(ConfigManager configManager, WorkLoadManager wlm) {
        this.configManager = configManager;
        this.wlm = wlm;
        models = new ConcurrentHashMap<>();
        scheduler = Executors.newScheduledThreadPool(2);
        this.startupModels = new HashSet<>();
    }
    public ScheduledExecutorService getScheduler() {
        return scheduler;
    }
    public static void init(ConfigManager configManager, WorkLoadManager wlm) {
        modelManager = new ModelManager(configManager, wlm);
    }
    public static ModelManager getInstance() {
        return modelManager;
    }
    public ModelArchive registerModel(String url, String defaultModelName, String preloadModel)
            throws ModelException, IOException, InterruptedException, ExecutionException,
                    TimeoutException {
        return registerModel(
                url,
                null,
                null,
                null,
                1,
                100,
                configManager.getDefaultResponseTimeoutSeconds(),
                defaultModelName,
                preloadModel);
    }
    public ModelArchive registerModel(
            String url,
            String modelName,
            Manifest.RuntimeType runtime,
            String handler,
            int batchSize,
            int maxBatchDelay,
            int responseTimeoutSeconds,
            String defaultModelName,
            String preloadModel)
            throws ModelException, IOException, InterruptedException, ExecutionException,
                    TimeoutException {
        ModelArchive archive = ModelArchive.downloadModel(configManager.getModelStore(), url);
        if (modelName == null || modelName.isEmpty()) {
            if (archive.getModelName() == null || archive.getModelName().isEmpty()) {
                archive.getManifest().getModel().setModelName(defaultModelName);
            }
            modelName = archive.getModelName();
        } else {
            archive.getManifest().getModel().setModelName(modelName);
        }
        if (runtime != null) {
            archive.getManifest().setRuntime(runtime);
        }
        if (handler != null) {
            archive.getManifest().getModel().setHandler(handler);
        } else if (archive.getHandler() == null || archive.getHandler().isEmpty()) {
            archive.getManifest()
                    .getModel()
                    .setHandler(configManager.getMmsDefaultServiceHandler());
        }
        archive.validate();
        Model model = new Model(archive, configManager.getJobQueueSize(), preloadModel);
        model.setBatchSize(batchSize);
        model.setMaxBatchDelay(maxBatchDelay);
        model.setResponseTimeoutSeconds(responseTimeoutSeconds);
        Model existingModel = models.putIfAbsent(modelName, model);
        if (existingModel != null) {
            // model already exists
            throw new ConflictStatusException("Model " + modelName + " is already registered.");
        }
        if (configManager.isDebug()) {
            model.setPort(9000);
        } else {
            startBackendServer(model);
        }
        models.put(modelName, model);
        logger.info("Model {} loaded.", model.getModelName());
        return archive;
    }
    public HttpResponseStatus unregisterModel(String modelName) {
        Model model = models.remove(modelName);
        if (model == null) {
            logger.warn("Model not found: " + modelName);
            return HttpResponseStatus.NOT_FOUND;
        }
        model.setMinWorkers(0);
        model.setMaxWorkers(0);
        CompletableFuture<HttpResponseStatus> futureStatus = wlm.modelChanged(model);
        HttpResponseStatus httpResponseStatus = HttpResponseStatus.OK;
        try {
            httpResponseStatus = futureStatus.get();
        } catch (InterruptedException | ExecutionException e) {
            logger.warn("Process was interrupted while cleaning resources.");
            httpResponseStatus = HttpResponseStatus.INTERNAL_SERVER_ERROR;
        }
        // Only continue cleaning if resource cleaning succeeded
        if (httpResponseStatus == HttpResponseStatus.OK) {
            model.getModelArchive().clean();
            startupModels.remove(modelName);
            logger.info("Model {} unregistered.", modelName);
        } else {
            models.put(modelName, model);
        }
        return httpResponseStatus;
    }
    public void startBackendServer(Model model)
            throws InterruptedException, ExecutionException, TimeoutException {
        CompletableFuture<HttpResponseStatus> future = new CompletableFuture<>();
        if (model == null) {
            throw new AssertionError("Model not found");
        }
        wlm.addServerThread(model, future);
    }
    public CompletableFuture<HttpResponseStatus> updateModel(
            String modelName, int minWorkers, int maxWorkers) {
        Model model = models.get(modelName);
        if (model == null) {
            throw new AssertionError("Model not found: " + modelName);
        }
        model.setMinWorkers(minWorkers);
        model.setMaxWorkers(maxWorkers);
        logger.debug("updateModel: {}, count: {}", modelName, minWorkers);
        return wlm.modelChanged(model);
    }
    public Map<String, Model> getModels() {
        return models;
    }
    public List<WorkerThread> getWorkers(String modelName) {
        return wlm.getWorkers(modelName);
    }
    public Map<Integer, WorkerThread> getWorkers() {
        return wlm.getWorkers();
    }
    public boolean addJob(Job job) throws ModelNotFoundException {
        String modelName = job.getModelName();
        Model model = models.get(modelName);
        if (model == null) {
            throw new ModelNotFoundException("Model not found: " + modelName);
        }
        if (wlm.hasNoWorker(modelName)) {
            return false;
        }
        return model.addJob(job);
    }
    public void workerStatus(final ChannelHandlerContext ctx) {
        Runnable r =
                () -> {
                    String response = "Healthy";
                    int numWorking = 0;
                    int numScaled = 0;
                    for (Map.Entry<String, Model> m : models.entrySet()) {
                        numScaled += m.getValue().getMinWorkers();
                        numWorking += wlm.getNumRunningWorkers(m.getValue().getModelName());
                    }
                    if ((numWorking > 0) && (numWorking < numScaled)) {
                        response = "Partial Healthy";
                    } else if ((numWorking == 0) && (numScaled > 0)) {
                        response = "Unhealthy";
                    }
                    // TODO: Check if its OK to send other 2xx errors to ALB for "Partial Healthy"
                    // and "Unhealthy"
                    NettyUtils.sendJsonResponse(
                            ctx, new StatusResponse(response), HttpResponseStatus.OK);
                };
        wlm.scheduleAsync(r);
    }
    public boolean scaleRequestStatus(String modelName) {
        Model model = ModelManager.getInstance().getModels().get(modelName);
        int numWorkers = wlm.getNumRunningWorkers(modelName);
        return model == null || model.getMinWorkers() <= numWorkers;
    }
    public void submitTask(Runnable runnable) {
        wlm.scheduleAsync(runnable);
    }
    public Set<String> getStartupModels() {
        return startupModels;
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.wlm;
import com.amazonaws.ml.mms.archive.ModelArchive;
import com.amazonaws.ml.mms.util.ConfigManager;
import java.io.File;
import java.util.Map;
import java.util.Objects;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.LinkedBlockingDeque;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.locks.ReentrantLock;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class Model {
    public static final String DEFAULT_DATA_QUEUE = "DATA_QUEUE";
    private static final Logger logger = LoggerFactory.getLogger(Model.class);
    private ModelArchive modelArchive;
    private int minWorkers;
    private int maxWorkers;
    private int batchSize;
    private int maxBatchDelay;
    private String preloadModel;
    private AtomicInteger port; // Port on which the model server is running
    private ReentrantLock lock;
    private int responseTimeoutSeconds;
    private WorkerThread serverThread;
    // Total number of subsequent inference request failures
    private AtomicInteger failedInfReqs;
    // Per worker thread job queue. This separates out the control queue from data queue
    private ConcurrentMap<String, LinkedBlockingDeque<Job>> jobsDb;
    public Model(ModelArchive modelArchive, int queueSize, String preloadModel) {
        this.modelArchive = modelArchive;
        this.preloadModel = preloadModel;
        batchSize = 1;
        maxBatchDelay = 100;
        jobsDb = new ConcurrentHashMap<>();
        // Always have a queue for data
        jobsDb.putIfAbsent(DEFAULT_DATA_QUEUE, new LinkedBlockingDeque<>(queueSize));
        failedInfReqs = new AtomicInteger(0);
        port = new AtomicInteger(-1);
        lock = new ReentrantLock();
    }
    public String getModelName() {
        return modelArchive.getModelName();
    }
    public File getModelDir() {
        return modelArchive.getModelDir();
    }
    public String getModelUrl() {
        return modelArchive.getUrl();
    }
    public ModelArchive getModelArchive() {
        return modelArchive;
    }
    public int getMinWorkers() {
        return minWorkers;
    }
    public void setMinWorkers(int minWorkers) {
        this.minWorkers = minWorkers;
    }
    public int getMaxWorkers() {
        return maxWorkers;
    }
    public void setMaxWorkers(int maxWorkers) {
        this.maxWorkers = maxWorkers;
    }
    public int getBatchSize() {
        return batchSize;
    }
    public void setBatchSize(int batchSize) {
        this.batchSize = batchSize;
    }
    public int getMaxBatchDelay() {
        return maxBatchDelay;
    }
    public void setMaxBatchDelay(int maxBatchDelay) {
        this.maxBatchDelay = maxBatchDelay;
    }
    public void addJob(String threadId, Job job) {
        LinkedBlockingDeque<Job> blockingDeque = jobsDb.get(threadId);
        if (blockingDeque == null) {
            blockingDeque = new LinkedBlockingDeque<>();
            jobsDb.put(threadId, blockingDeque);
        }
        blockingDeque.offer(job);
    }
    public void removeJobQueue(String threadId) {
        if (!threadId.equals(DEFAULT_DATA_QUEUE)) {
            jobsDb.remove(threadId);
        }
    }
    public boolean addJob(Job job) {
        return jobsDb.get(DEFAULT_DATA_QUEUE).offer(job);
    }
    public void addFirst(Job job) {
        jobsDb.get(DEFAULT_DATA_QUEUE).addFirst(job);
    }
    public void pollBatch(String threadId, long waitTime, Map<String, Job> jobsRepo)
            throws InterruptedException {
        if (jobsRepo == null || threadId == null || threadId.isEmpty()) {
            throw new IllegalArgumentException("Invalid input given provided");
        }
        if (!jobsRepo.isEmpty()) {
            throw new IllegalArgumentException(
                    "The jobs repo provided contains stale jobs. Clear them!!");
        }
        LinkedBlockingDeque<Job> jobsQueue = jobsDb.get(threadId);
        if (jobsQueue != null && !jobsQueue.isEmpty()) {
            Job j = jobsQueue.poll(waitTime, TimeUnit.MILLISECONDS);
            if (j != null) {
                jobsRepo.put(j.getJobId(), j);
                return;
            }
        }
        try {
            lock.lockInterruptibly();
            long maxDelay = maxBatchDelay;
            jobsQueue = jobsDb.get(DEFAULT_DATA_QUEUE);
            Job j = jobsQueue.poll(Long.MAX_VALUE, TimeUnit.MILLISECONDS);
            logger.trace("get first job: {}", Objects.requireNonNull(j).getJobId());
            jobsRepo.put(j.getJobId(), j);
            long begin = System.currentTimeMillis();
            for (int i = 0; i < batchSize - 1; ++i) {
                j = jobsQueue.poll(maxDelay, TimeUnit.MILLISECONDS);
                if (j == null) {
                    break;
                }
                long end = System.currentTimeMillis();
                maxDelay -= end - begin;
                begin = end;
                jobsRepo.put(j.getJobId(), j);
                if (maxDelay <= 0) {
                    break;
                }
            }
            logger.trace("sending jobs, size: {}", jobsRepo.size());
        } finally {
            if (lock.isHeldByCurrentThread()) {
                lock.unlock();
            }
        }
    }
    public int getPort() {
        return port.get();
    }
    public void setPort(int port) {
        this.port.set(port);
    }
    public int incrFailedInfReqs() {
        return failedInfReqs.incrementAndGet();
    }
    public void resetFailedInfReqs() {
        failedInfReqs.set(0);
    }
    public int getResponseTimeoutSeconds() {
        return ConfigManager.getInstance().isDebug() ? Integer.MAX_VALUE : responseTimeoutSeconds;
    }
    public void setResponseTimeoutSeconds(int responseTimeoutSeconds) {
        this.responseTimeoutSeconds = responseTimeoutSeconds;
    }
    public WorkerThread getServerThread() {
        return serverThread;
    }
    public void setServerThread(WorkerThread serverThread) {
        this.serverThread = serverThread;
    }
    public String preloadModel() {
        return preloadModel;
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.wlm;
import com.amazonaws.ml.mms.util.ConfigManager;
import io.netty.channel.EventLoopGroup;
import io.netty.handler.codec.http.HttpResponseStatus;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;
import java.util.concurrent.atomic.AtomicInteger;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class WorkLoadManager {
    private ExecutorService threadPool;
    private ConcurrentHashMap<String, List<WorkerThread>> workers;
    private ConfigManager configManager;
    private EventLoopGroup backendGroup;
    private AtomicInteger port;
    private AtomicInteger gpuCounter;
    private AtomicInteger threadNumber;
    private static final Logger logger = LoggerFactory.getLogger(WorkLoadManager.class);
    public WorkLoadManager(ConfigManager configManager, EventLoopGroup backendGroup) {
        this.configManager = configManager;
        this.backendGroup = backendGroup;
        this.port = new AtomicInteger(9000);
        this.gpuCounter = new AtomicInteger(0);
        threadPool = Executors.newCachedThreadPool();
        workers = new ConcurrentHashMap<>();
        threadNumber = new AtomicInteger(0);
    }
    public List<WorkerThread> getWorkers(String modelName) {
        List<WorkerThread> list = workers.get(modelName);
        if (list == null) {
            return Collections.emptyList();
        }
        return new ArrayList<>(list);
    }
    public Map<Integer, WorkerThread> getWorkers() {
        Map<Integer, WorkerThread> map = new HashMap<>();
        for (Map.Entry<String, List<WorkerThread>> entry : workers.entrySet()) {
            // Add server thread
            String modelName = entry.getKey();
            List<WorkerThread> workerThreads = entry.getValue();
            WorkerThread serverThread =
                    ModelManager.getInstance().getModels().get(modelName).getServerThread();
            map.put(serverThread.getPid(), serverThread);
            for (WorkerThread worker : workerThreads) {
                map.put(worker.getPid(), worker);
            }
        }
        return map;
    }
    public boolean hasNoWorker(String modelName) {
        List<WorkerThread> worker = workers.get(modelName);
        if (worker == null) {
            return true;
        }
        return worker.isEmpty();
    }
    public int getNumRunningWorkers(String modelName) {
        int numWorking = 0;
        List<WorkerThread> threads = workers.getOrDefault(modelName, null);
        if (threads != null) {
            for (WorkerThread thread : threads) {
                if ((thread.getState() != WorkerState.WORKER_STOPPED)
                        && (thread.getState() != WorkerState.WORKER_ERROR)
                        && (thread.getState() != WorkerState.WORKER_SCALED_DOWN)) {
                    numWorking += 1;
                }
            }
        }
        return numWorking;
    }
    public CompletableFuture<HttpResponseStatus> modelChanged(Model model) {
        synchronized (model.getModelName()) {
            CompletableFuture<HttpResponseStatus> future = new CompletableFuture<>();
            int minWorker = model.getMinWorkers();
            int maxWorker = model.getMaxWorkers();
            List<WorkerThread> threads;
            if (minWorker == 0) {
                threads = workers.remove(model.getModelName());
                if (threads == null) {
                    if (maxWorker == 0) {
                        return shutdownServerThread(model, future);
                    }
                    future.complete(HttpResponseStatus.OK);
                    return future;
                }
            } else {
                threads = workers.computeIfAbsent(model.getModelName(), k -> new ArrayList<>());
            }
            int currentWorkers = threads.size();
            if (currentWorkers < minWorker) {
                addThreads(threads, model, minWorker - currentWorkers, future);
            } else {
                for (int i = currentWorkers - 1; i >= maxWorker; --i) {
                    WorkerThread thread = threads.remove(i);
                    thread.shutdown();
                }
                if (maxWorker == 0) {
                    return shutdownServerThread(model, future);
                }
                future.complete(HttpResponseStatus.OK);
            }
            return future;
        }
    }
    private CompletableFuture<HttpResponseStatus> shutdownServerThread(
            Model model, CompletableFuture<HttpResponseStatus> future) {
        model.getServerThread().shutdown();
        WorkerLifeCycle lifecycle = model.getServerThread().getLifeCycle();
        Process workerProcess = lifecycle.getProcess();
        if (workerProcess.isAlive()) {
            boolean workerDestroyed = false;
            workerProcess.destroyForcibly();
            try {
                workerDestroyed =
                        workerProcess.waitFor(
                                configManager.getUnregisterModelTimeout(), TimeUnit.SECONDS);
            } catch (InterruptedException e) {
                logger.warn(
                        "WorkerThread interrupted during waitFor, possible asynch resource cleanup.");
                future.complete(HttpResponseStatus.INTERNAL_SERVER_ERROR);
                return future;
            }
            if (!workerDestroyed) {
                logger.warn("WorkerThread timed out while cleaning, please resend request.");
                future.complete(HttpResponseStatus.REQUEST_TIMEOUT);
                return future;
            }
        }
        future.complete(HttpResponseStatus.OK);
        return future;
    }
    public void addServerThread(Model model, CompletableFuture<HttpResponseStatus> future)
            throws InterruptedException, ExecutionException, TimeoutException {
        WorkerStateListener listener = new WorkerStateListener(future, 1);
        BatchAggregator aggregator = new BatchAggregator(model);
        synchronized (model.getModelName()) {
            model.setPort(port.getAndIncrement());
            WorkerThread thread =
                    new WorkerThread(
                            configManager,
                            backendGroup,
                            model.getPort(),
                            -1,
                            model,
                            aggregator,
                            listener,
                            threadNumber.getAndIncrement(),
                            true);
            model.setServerThread(thread);
            threadPool.submit(thread);
            future.get(1, TimeUnit.MINUTES);
        }
    }
    private void addThreads(
            List<WorkerThread> threads,
            Model model,
            int count,
            CompletableFuture<HttpResponseStatus> future) {
        WorkerStateListener listener = new WorkerStateListener(future, count);
        int maxGpu = configManager.getNumberOfGpu();
        for (int i = 0; i < count; ++i) {
            int gpuId = -1;
            if (maxGpu > 0) {
                gpuId = gpuCounter.accumulateAndGet(maxGpu, (prev, maxGpuId) -> ++prev % maxGpuId);
            }
            BatchAggregator aggregator = new BatchAggregator(model);
            WorkerThread thread =
                    new WorkerThread(
                            configManager,
                            backendGroup,
                            model.getPort(),
                            gpuId,
                            model,
                            aggregator,
                            listener,
                            threadNumber.getAndIncrement(),
                            false);
            threads.add(thread);
            threadPool.submit(thread);
        }
    }
    public void scheduleAsync(Runnable r) {
        threadPool.execute(r);
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.wlm;
import com.amazonaws.ml.mms.http.InternalServerException;
import com.amazonaws.ml.mms.util.NettyUtils;
import com.amazonaws.ml.mms.util.messages.RequestInput;
import com.amazonaws.ml.mms.util.messages.WorkerCommands;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.DefaultFullHttpResponse;
import io.netty.handler.codec.http.DefaultHttpHeadersFactory;
import io.netty.handler.codec.http.FullHttpResponse;
import io.netty.handler.codec.http.HttpHeaderNames;
import io.netty.handler.codec.http.HttpResponseStatus;
import io.netty.handler.codec.http.HttpVersion;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class Job {
    private static final Logger logger = LoggerFactory.getLogger(Job.class);
    private ChannelHandlerContext ctx;
    private String modelName;
    private WorkerCommands cmd; // Else its data msg or inf requests
    private RequestInput input;
    private long begin;
    private long scheduled;
    public Job(
            ChannelHandlerContext ctx, String modelName, WorkerCommands cmd, RequestInput input) {
        this.ctx = ctx;
        this.modelName = modelName;
        this.cmd = cmd;
        this.input = input;
        begin = System.currentTimeMillis();
        scheduled = begin;
    }
    public String getJobId() {
        return input.getRequestId();
    }
    public String getModelName() {
        return modelName;
    }
    public WorkerCommands getCmd() {
        return cmd;
    }
    public boolean isControlCmd() {
        return !WorkerCommands.PREDICT.equals(cmd);
    }
    public RequestInput getPayload() {
        return input;
    }
    public void setScheduled() {
        scheduled = System.currentTimeMillis();
    }
    public void response(
            byte[] body,
            CharSequence contentType,
            int statusCode,
            String statusPhrase,
            Map<String, String> responseHeaders) {
        HttpResponseStatus status =
                (statusPhrase == null)
                        ? HttpResponseStatus.valueOf(statusCode)
                        : HttpResponseStatus.valueOf(statusCode, statusPhrase);
        FullHttpResponse resp =
                new DefaultFullHttpResponse(
                        HttpVersion.HTTP_1_1,
                        status,
                        Unpooled.directBuffer(),
                        DefaultHttpHeadersFactory.headersFactory(),
                        DefaultHttpHeadersFactory.trailersFactory());
        if (contentType != null && contentType.length() > 0) {
            resp.headers().set(HttpHeaderNames.CONTENT_TYPE, contentType);
        }
        if (responseHeaders != null) {
            for (Map.Entry<String, String> e : responseHeaders.entrySet()) {
                resp.headers().set(e.getKey(), e.getValue());
            }
        }
        resp.content().writeBytes(body);
        /*
         * We can load the models based on the configuration file.Since this Job is
         * not driven by the external connections, we could have a empty context for
         * this job. We shouldn't try to send a response to ctx if this is not triggered
         * by external clients.
         */
        if (ctx != null) {
            NettyUtils.sendHttpResponse(ctx, resp, true);
        }
        logger.debug(
                "Waiting time: {}, Backend time: {}",
                scheduled - begin,
                System.currentTimeMillis() - scheduled);
    }
    public void sendError(HttpResponseStatus status, String error) {
        /*
         * We can load the models based on the configuration file.Since this Job is
         * not driven by the external connections, we could have a empty context for
         * this job. We shouldn't try to send a response to ctx if this is not triggered
         * by external clients.
         */
        if (ctx != null) {
            NettyUtils.sendError(ctx, status, new InternalServerException(error));
        }
        logger.debug(
                "Waiting time: {}, Inference time: {}",
                scheduled - begin,
                System.currentTimeMillis() - begin);
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.wlm;
import com.amazonaws.ml.mms.archive.Manifest;
import com.amazonaws.ml.mms.metrics.Metric;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.util.Connector;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.Scanner;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.regex.Pattern;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class WorkerLifeCycle {
    static final Logger logger = LoggerFactory.getLogger(WorkerLifeCycle.class);
    private ConfigManager configManager;
    private Model model;
    private int pid = -1;
    private Process process;
    private CountDownLatch latch;
    private boolean success;
    private Connector connector;
    private ReaderThread errReader;
    private ReaderThread outReader;
    public WorkerLifeCycle(ConfigManager configManager, Model model) {
        this.configManager = configManager;
        this.model = model;
        this.latch = new CountDownLatch(1);
    }
    private String[] getEnvString(String cwd, String modelPath, String handler) {
        ArrayList<String> envList = new ArrayList<>();
        Pattern blackList = configManager.getBlacklistPattern();
        String handlerFile = handler;
        if (handler.contains(":")) {
            handlerFile = handler.split(":")[0];
            if (handlerFile.contains("/")) {
                handlerFile = handlerFile.substring(0, handlerFile.lastIndexOf('/'));
            }
        }
        StringBuilder pythonPath = new StringBuilder();
        HashMap<String, String> environment = new HashMap<>(System.getenv());
        environment.putAll(configManager.getBackendConfiguration());
        pythonPath.append(handlerFile).append(File.pathSeparatorChar);
        if (System.getenv("PYTHONPATH") != null) {
            pythonPath.append(System.getenv("PYTHONPATH")).append(File.pathSeparatorChar);
        }
        pythonPath.append(modelPath);
        if (!cwd.contains("site-packages") && !cwd.contains("dist-packages")) {
            pythonPath.append(File.pathSeparatorChar).append(cwd);
        }
        environment.put("PYTHONPATH", pythonPath.toString());
        for (Map.Entry<String, String> entry : environment.entrySet()) {
            if (!blackList.matcher(entry.getKey()).matches()) {
                envList.add(entry.getKey() + '=' + entry.getValue());
            }
        }
        return envList.toArray(new String[0]); // NOPMD
    }
    public synchronized void attachIOStreams(
            String threadName, InputStream outStream, InputStream errStream) {
        logger.warn("attachIOStreams() threadName={}", threadName);
        errReader = new ReaderThread(threadName, errStream, true, this);
        outReader = new ReaderThread(threadName, outStream, false, this);
        errReader.start();
        outReader.start();
    }
    public synchronized void terminateIOStreams() {
        if (errReader != null) {
            logger.warn("terminateIOStreams() threadName={}", errReader.getName());
            errReader.terminate();
        }
        if (outReader != null) {
            logger.warn("terminateIOStreams() threadName={}", outReader.getName());
            outReader.terminate();
        }
    }
    public void startBackendServer(int port)
            throws WorkerInitializationException, InterruptedException {
        File workingDir = new File(configManager.getModelServerHome());
        File modelPath;
        setPort(port);
        try {
            modelPath = model.getModelDir().getCanonicalFile();
        } catch (IOException e) {
            throw new WorkerInitializationException("Failed get MMS home directory", e);
        }
        String[] args = new String[16];
        Manifest.RuntimeType runtime = model.getModelArchive().getManifest().getRuntime();
        if (runtime == Manifest.RuntimeType.PYTHON) {
            args[0] = configManager.getPythonExecutable();
        } else {
            args[0] = runtime.getValue();
        }
        args[1] = new File(workingDir, "mms/model_service_worker.py").getAbsolutePath();
        args[2] = "--sock-type";
        args[3] = connector.getSocketType();
        args[4] = connector.isUds() ? "--sock-name" : "--port";
        args[5] = connector.getSocketPath();
        args[6] = "--handler";
        args[7] = model.getModelArchive().getManifest().getModel().getHandler();
        args[8] = "--model-path";
        args[9] = model.getModelDir().getAbsolutePath();
        args[10] = "--model-name";
        args[11] = model.getModelName();
        args[12] = "--preload-model";
        args[13] = model.preloadModel();
        args[14] = "--tmp-dir";
        args[15] = System.getProperty("java.io.tmpdir");
        String[] envp =
                getEnvString(
                        workingDir.getAbsolutePath(),
                        modelPath.getAbsolutePath(),
                        model.getModelArchive().getManifest().getModel().getHandler());
        try {
            latch = new CountDownLatch(1);
            synchronized (this) {
                String threadName =
                        "W-"
                                + port
                                + '-'
                                + model.getModelName()
                                        .substring(0, Math.min(model.getModelName().length(), 25));
                process = Runtime.getRuntime().exec(args, envp, modelPath);
                attachIOStreams(threadName, process.getInputStream(), process.getErrorStream());
            }
            if (latch.await(2, TimeUnit.MINUTES)) {
                if (!success) {
                    throw new WorkerInitializationException("Backend stream closed.");
                }
                return;
            }
            throw new WorkerInitializationException("Backend worker startup time out.");
        } catch (IOException e) {
            throw new WorkerInitializationException("Failed start worker process", e);
        } finally {
            if (!success) {
                exit();
            }
        }
    }
    public synchronized void exit() {
        if (process != null) {
            process.destroyForcibly();
            connector.clean();
            terminateIOStreams();
        }
    }
    public synchronized Integer getExitValue() {
        if (process != null && !process.isAlive()) {
            return process.exitValue();
        }
        return null;
    }
    void setSuccess(boolean success) {
        this.success = success;
        latch.countDown();
    }
    public synchronized int getPid() {
        return pid;
    }
    public synchronized void setPid(int pid) {
        this.pid = pid;
    }
    private synchronized void setPort(int port) {
        connector = new Connector(port);
    }
    public Process getProcess() {
        return process;
    }
    private static final class ReaderThread extends Thread {
        private InputStream is;
        private boolean error;
        private WorkerLifeCycle lifeCycle;
        private AtomicBoolean isRunning = new AtomicBoolean(true);
        static final Logger loggerModelMetrics =
                LoggerFactory.getLogger(ConfigManager.MODEL_METRICS_LOGGER);
        public ReaderThread(String name, InputStream is, boolean error, WorkerLifeCycle lifeCycle) {
            super(name + (error ? "-stderr" : "-stdout"));
            this.is = is;
            this.error = error;
            this.lifeCycle = lifeCycle;
        }
        public void terminate() {
            isRunning.set(false);
        }
        @Override
        public void run() {
            try (Scanner scanner = new Scanner(is, StandardCharsets.UTF_8.name())) {
                while (isRunning.get() && scanner.hasNext()) {
                    String result = scanner.nextLine();
                    if (result == null) {
                        break;
                    }
                    if (result.startsWith("[METRICS]")) {
                        loggerModelMetrics.info("{}", Metric.parse(result.substring(9)));
                        continue;
                    }
                    if ("MMS worker started.".equals(result)) {
                        lifeCycle.setSuccess(true);
                    } else if (result.startsWith("[PID]")) {
                        lifeCycle.setPid(Integer.parseInt(result.substring("[PID] ".length())));
                    }
                    if (error) {
                        logger.warn(result);
                    } else {
                        logger.info(result);
                    }
                }
            } catch (Exception e) {
                logger.error("Couldn't create scanner - {}", getName(), e);
            } finally {
                logger.info("Stopped Scanner - {}", getName());
                lifeCycle.setSuccess(false);
                try {
                    is.close();
                } catch (IOException e) {
                    logger.error("Failed to close stream for thread {}", this.getName(), e);
                }
            }
        }
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.wlm;
import com.amazonaws.ml.mms.util.messages.BaseModelRequest;
import com.amazonaws.ml.mms.util.messages.ModelInferenceRequest;
import com.amazonaws.ml.mms.util.messages.ModelLoadModelRequest;
import com.amazonaws.ml.mms.util.messages.ModelWorkerResponse;
import com.amazonaws.ml.mms.util.messages.Predictions;
import com.amazonaws.ml.mms.util.messages.RequestInput;
import io.netty.handler.codec.http.HttpResponseStatus;
import java.util.LinkedHashMap;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class BatchAggregator {
    private static final Logger logger = LoggerFactory.getLogger(BatchAggregator.class);
    private Model model;
    private Map<String, Job> jobs;
    public BatchAggregator(Model model) {
        this.model = model;
        jobs = new LinkedHashMap<>();
    }
    public BaseModelRequest getRequest(String threadName, WorkerState state)
            throws InterruptedException {
        jobs.clear();
        ModelInferenceRequest req = new ModelInferenceRequest(model.getModelName());
        model.pollBatch(
                threadName, (state == WorkerState.WORKER_MODEL_LOADED) ? 0 : Long.MAX_VALUE, jobs);
        for (Job j : jobs.values()) {
            if (j.isControlCmd()) {
                if (jobs.size() > 1) {
                    throw new IllegalStateException(
                            "Received more than 1 control command. "
                                    + "Control messages should be processed/retrieved one at a time.");
                }
                RequestInput input = j.getPayload();
                int gpuId = -1;
                String gpu = input.getStringParameter("gpu");
                if (gpu != null) {
                    gpuId = Integer.parseInt(gpu);
                }
                return new ModelLoadModelRequest(model, gpuId, threadName);
            } else {
                j.setScheduled();
                req.addRequest(j.getPayload());
            }
        }
        return req;
    }
    public void sendResponse(ModelWorkerResponse message) {
        // TODO: Handle prediction level code
        if (message.getCode() == 200) {
            if (jobs.isEmpty()) {
                // this is from initial load.
                return;
            }
            for (Predictions prediction : message.getPredictions()) {
                String jobId = prediction.getRequestId();
                Job job = jobs.remove(jobId);
                if (job == null) {
                    throw new IllegalStateException("Unexpected job: " + jobId);
                }
                job.response(
                        prediction.getResp(),
                        prediction.getContentType(),
                        prediction.getStatusCode(),
                        prediction.getReasonPhrase(),
                        prediction.getHeaders());
            }
        } else {
            for (String reqId : jobs.keySet()) {
                Job j = jobs.remove(reqId);
                if (j == null) {
                    throw new IllegalStateException("Unexpected job: " + reqId);
                }
                j.sendError(HttpResponseStatus.valueOf(message.getCode()), message.getMessage());
            }
            if (!jobs.isEmpty()) {
                throw new IllegalStateException("Not all jobs get response.");
            }
        }
    }
    public void sendError(BaseModelRequest message, String error, HttpResponseStatus status) {
        if (message instanceof ModelLoadModelRequest) {
            logger.warn("Load model failed: {}, error: {}", message.getModelName(), error);
            return;
        }
        if (message != null) {
            ModelInferenceRequest msg = (ModelInferenceRequest) message;
            for (RequestInput req : msg.getRequestBatch()) {
                String requestId = req.getRequestId();
                Job job = jobs.remove(requestId);
                if (job == null) {
                    logger.error("Unexpected job: " + requestId);
                } else {
                    job.sendError(status, error);
                }
            }
            if (!jobs.isEmpty()) {
                jobs.clear();
                logger.error("Not all jobs get response.");
            }
        } else {
            // Send the error message to all the jobs
            for (Map.Entry<String, Job> j : jobs.entrySet()) {
                String jobsId = j.getValue().getJobId();
                Job job = jobs.remove(jobsId);
                if (job.isControlCmd()) {
                    job.sendError(status, error);
                } else {
                    // Data message can be handled by other workers.
                    // If batch has gone past its batch max delay timer?
                    model.addFirst(job);
                }
            }
        }
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.util.codec;
import com.amazonaws.ml.mms.util.messages.BaseModelRequest;
import com.amazonaws.ml.mms.util.messages.InputParameter;
import com.amazonaws.ml.mms.util.messages.ModelInferenceRequest;
import com.amazonaws.ml.mms.util.messages.ModelLoadModelRequest;
import com.amazonaws.ml.mms.util.messages.RequestInput;
import io.netty.buffer.ByteBuf;
import io.netty.channel.ChannelHandler;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.MessageToByteEncoder;
import java.nio.charset.StandardCharsets;
import java.util.Map;
@ChannelHandler.Sharable
public class ModelRequestEncoder extends MessageToByteEncoder<BaseModelRequest> {
    public ModelRequestEncoder(boolean preferDirect) {
        super(preferDirect);
    }
    @Override
    protected void encode(ChannelHandlerContext ctx, BaseModelRequest msg, ByteBuf out) {
        if (msg instanceof ModelLoadModelRequest) {
            out.writeByte('L');
            ModelLoadModelRequest request = (ModelLoadModelRequest) msg;
            byte[] buf = msg.getModelName().getBytes(StandardCharsets.UTF_8);
            out.writeInt(buf.length);
            out.writeBytes(buf);
            buf = request.getModelPath().getBytes(StandardCharsets.UTF_8);
            out.writeInt(buf.length);
            out.writeBytes(buf);
            int batchSize = request.getBatchSize();
            if (batchSize <= 0) {
                batchSize = 1;
            }
            out.writeInt(batchSize);
            buf = request.getHandler().getBytes(StandardCharsets.UTF_8);
            out.writeInt(buf.length);
            out.writeBytes(buf);
            out.writeInt(request.getGpuId());
            buf = request.getIoFileDescriptor().getBytes(StandardCharsets.UTF_8);
            out.writeInt(buf.length);
            out.writeBytes(buf);
        } else if (msg instanceof ModelInferenceRequest) {
            out.writeByte('I');
            ModelInferenceRequest request = (ModelInferenceRequest) msg;
            for (RequestInput input : request.getRequestBatch()) {
                encodeRequest(input, out);
            }
            out.writeInt(-1); // End of List
        }
    }
    private void encodeRequest(RequestInput req, ByteBuf out) {
        byte[] buf = req.getRequestId().getBytes(StandardCharsets.UTF_8);
        out.writeInt(buf.length);
        out.writeBytes(buf);
        for (Map.Entry<String, String> entry : req.getHeaders().entrySet()) {
            encodeField(entry.getKey(), out);
            encodeField(entry.getValue(), out);
        }
        out.writeInt(-1); // End of List
        for (InputParameter input : req.getParameters()) {
            encodeParameter(input, out);
        }
        out.writeInt(-1); // End of List
    }
    private void encodeParameter(InputParameter parameter, ByteBuf out) {
        byte[] modelInputName = parameter.getName().getBytes(StandardCharsets.UTF_8);
        out.writeInt(modelInputName.length);
        out.writeBytes(modelInputName);
        encodeField(parameter.getContentType(), out);
        byte[] buf = parameter.getValue();
        out.writeInt(buf.length);
        out.writeBytes(buf);
    }
    private static void encodeField(CharSequence field, ByteBuf out) {
        if (field == null) {
            out.writeInt(0);
            return;
        }
        byte[] buf = field.toString().getBytes(StandardCharsets.UTF_8);
        out.writeInt(buf.length);
        out.writeBytes(buf);
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.util.codec;
import com.amazonaws.ml.mms.util.messages.ModelWorkerResponse;
import com.amazonaws.ml.mms.util.messages.Predictions;
import io.netty.buffer.ByteBuf;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.ByteToMessageDecoder;
import java.util.ArrayList;
import java.util.List;
public class ModelResponseDecoder extends ByteToMessageDecoder {
    private final int maxBufferSize;
    public ModelResponseDecoder(int maxBufferSize) {
        this.maxBufferSize = maxBufferSize;
    }
    @Override
    protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {
        int size = in.readableBytes();
        if (size < 9) {
            return;
        }
        in.markReaderIndex();
        boolean completed = false;
        try {
            ModelWorkerResponse resp = new ModelWorkerResponse();
            // Get Response overall Code
            resp.setCode(in.readInt());
            int len = CodecUtils.readLength(in, maxBufferSize);
            if (len == CodecUtils.BUFFER_UNDER_RUN) {
                return;
            }
            resp.setMessage(CodecUtils.readString(in, len));
            List<Predictions> predictions = new ArrayList<>();
            while ((len = CodecUtils.readLength(in, maxBufferSize)) != CodecUtils.END) {
                if (len == CodecUtils.BUFFER_UNDER_RUN) {
                    return;
                }
                Predictions prediction = new Predictions();
                // Set response RequestId
                prediction.setRequestId(CodecUtils.readString(in, len));
                len = CodecUtils.readLength(in, maxBufferSize);
                if (len == CodecUtils.BUFFER_UNDER_RUN) {
                    return;
                }
                // Set content type
                prediction.setContentType(CodecUtils.readString(in, len));
                // Set per request response code
                int httpStatusCode = in.readInt();
                prediction.setStatusCode(httpStatusCode);
                // Set the actual message
                len = CodecUtils.readLength(in, maxBufferSize);
                if (len == CodecUtils.BUFFER_UNDER_RUN) {
                    return;
                }
                prediction.setReasonPhrase(CodecUtils.readString(in, len));
                len = CodecUtils.readLength(in, maxBufferSize);
                if (len == CodecUtils.BUFFER_UNDER_RUN) {
                    return;
                }
                prediction.setHeaders(CodecUtils.readMap(in, len));
                len = CodecUtils.readLength(in, maxBufferSize);
                if (len == CodecUtils.BUFFER_UNDER_RUN) {
                    return;
                }
                prediction.setResp(CodecUtils.read(in, len));
                predictions.add(prediction);
            }
            resp.setPredictions(predictions);
            out.add(resp);
            completed = true;
        } finally {
            if (!completed) {
                in.resetReaderIndex();
            }
        }
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import io.netty.buffer.ByteBufOutputStream;
import io.netty.handler.codec.http.FullHttpResponse;
import io.netty.handler.codec.http.HttpHeaderNames;
import io.netty.handler.codec.http.HttpResponseStatus;
import java.io.OutputStream;
import software.amazon.ai.mms.servingsdk.http.Response;
public class ModelServerResponse implements Response {
    private FullHttpResponse response;
    public ModelServerResponse(FullHttpResponse rsp) {
        response = rsp;
    }
    @Override
    public void setStatus(int i) {
        response.setStatus(HttpResponseStatus.valueOf(i));
    }
    @Override
    public void setStatus(int i, String s) {
        response.setStatus(HttpResponseStatus.valueOf(i, s));
    }
    @Override
    public void setHeader(String k, String v) {
        response.headers().set(k, v);
    }
    @Override
    public void addHeader(String k, String v) {
        response.headers().add(k, v);
    }
    @Override
    public void setContentType(String contentType) {
        response.headers().set(HttpHeaderNames.CONTENT_TYPE, contentType);
    }
    @Override
    public OutputStream getOutputStream() {
        return new ByteBufOutputStream(response.content());
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.archive.ModelNotFoundException;
import com.amazonaws.ml.mms.openapi.OpenApiUtils;
import com.amazonaws.ml.mms.util.NettyUtils;
import com.amazonaws.ml.mms.util.messages.InputParameter;
import com.amazonaws.ml.mms.util.messages.RequestInput;
import com.amazonaws.ml.mms.util.messages.WorkerCommands;
import com.amazonaws.ml.mms.wlm.Job;
import com.amazonaws.ml.mms.wlm.Model;
import com.amazonaws.ml.mms.wlm.ModelManager;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.HttpHeaderValues;
import io.netty.handler.codec.http.HttpMethod;
import io.netty.handler.codec.http.HttpUtil;
import io.netty.handler.codec.http.QueryStringDecoder;
import io.netty.handler.codec.http.multipart.DefaultHttpDataFactory;
import io.netty.handler.codec.http.multipart.HttpDataFactory;
import io.netty.handler.codec.http.multipart.HttpPostRequestDecoder;
import java.util.List;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
/**
 * A class handling inbound HTTP requests to the management API.
 *
 * <p>This class
 */
public class InferenceRequestHandler extends HttpRequestHandlerChain {
    private static final Logger logger = LoggerFactory.getLogger(InferenceRequestHandler.class);
    /** Creates a new {@code InferenceRequestHandler} instance. */
    public InferenceRequestHandler(Map<String, ModelServerEndpoint> ep) {
        endpointMap = ep;
    }
    @Override
    protected void handleRequest(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelException {
        if (isInferenceReq(segments)) {
            if (endpointMap.getOrDefault(segments[1], null) != null) {
                handleCustomEndpoint(ctx, req, segments, decoder);
            } else {
                switch (segments[1]) {
                    case "ping":
                        ModelManager.getInstance().workerStatus(ctx);
                        break;
                    case "models":
                    case "invocations":
                        validatePredictionsEndpoint(segments);
                        handleInvocations(ctx, req, decoder, segments);
                        break;
                    case "predictions":
                        handlePredictions(ctx, req, segments);
                        break;
                    default:
                        handleLegacyPredict(ctx, req, decoder, segments);
                        break;
                }
            }
        } else {
            chain.handleRequest(ctx, req, decoder, segments);
        }
    }
    private boolean isInferenceReq(String[] segments) {
        return segments.length == 0
                || segments[1].equals("ping")
                || (segments.length == 4 && segments[1].equals("models"))
                || segments[1].equals("predictions")
                || segments[1].equals("api-description")
                || segments[1].equals("invocations")
                || (segments.length == 3 && segments[2].equals("predict"))
                || endpointMap.containsKey(segments[1]);
    }
    private void validatePredictionsEndpoint(String[] segments) {
        if (segments.length == 2 && "invocations".equals(segments[1])) {
            return;
        } else if (segments.length == 4
                && "models".equals(segments[1])
                && "invoke".equals(segments[3])) {
            return;
        }
        throw new ResourceNotFoundException();
    }
    private void handlePredictions(
            ChannelHandlerContext ctx, FullHttpRequest req, String[] segments)
            throws ModelNotFoundException {
        if (segments.length < 3) {
            throw new ResourceNotFoundException();
        }
        predict(ctx, req, null, segments[2]);
    }
    private void handleInvocations(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelNotFoundException {
        String modelName =
                ("invocations".equals(segments[1]))
                        ? NettyUtils.getParameter(decoder, "model_name", null)
                        : segments[2];
        if (modelName == null || modelName.isEmpty()) {
            if (ModelManager.getInstance().getStartupModels().size() == 1) {
                modelName = ModelManager.getInstance().getStartupModels().iterator().next();
            }
        }
        predict(ctx, req, decoder, modelName);
    }
    private void handleLegacyPredict(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelNotFoundException {
        if (segments.length < 3 || !"predict".equals(segments[2])) {
            throw new ResourceNotFoundException();
        }
        predict(ctx, req, decoder, segments[1]);
    }
    private void predict(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String modelName)
            throws ModelNotFoundException, BadRequestException {
        RequestInput input = parseRequest(ctx, req, decoder);
        if (modelName == null) {
            throw new BadRequestException("Parameter model_name is required.");
        }
        if (HttpMethod.OPTIONS.equals(req.method())) {
            ModelManager modelManager = ModelManager.getInstance();
            Model model = modelManager.getModels().get(modelName);
            if (model == null) {
                throw new ModelNotFoundException("Model not found: " + modelName);
            }
            String resp = OpenApiUtils.getModelApi(model);
            NettyUtils.sendJsonResponse(ctx, resp);
            return;
        }
        Job job = new Job(ctx, modelName, WorkerCommands.PREDICT, input);
        if (!ModelManager.getInstance().addJob(job)) {
            throw new ServiceUnavailableException(
                    "No worker is available to serve request for model: "
                            + modelName
                            + ". Consider increasing job queue size.");
        }
    }
    private static RequestInput parseRequest(
            ChannelHandlerContext ctx, FullHttpRequest req, QueryStringDecoder decoder) {
        String requestId = NettyUtils.getRequestId(ctx.channel());
        RequestInput inputData = new RequestInput(requestId);
        if (decoder != null) {
            for (Map.Entry<String, List<String>> entry : decoder.parameters().entrySet()) {
                String key = entry.getKey();
                for (String value : entry.getValue()) {
                    inputData.addParameter(new InputParameter(key, value));
                }
            }
        }
        CharSequence contentType = HttpUtil.getMimeType(req);
        for (Map.Entry<String, String> entry : req.headers().entries()) {
            inputData.updateHeaders(entry.getKey(), entry.getValue());
        }
        if (HttpPostRequestDecoder.isMultipart(req)
                || HttpHeaderValues.APPLICATION_X_WWW_FORM_URLENCODED.contentEqualsIgnoreCase(
                        contentType)) {
            HttpDataFactory factory = new DefaultHttpDataFactory(6553500);
            HttpPostRequestDecoder form = new HttpPostRequestDecoder(factory, req);
            try {
                while (form.hasNext()) {
                    inputData.addParameter(NettyUtils.getFormData(form.next()));
                }
            } catch (HttpPostRequestDecoder.EndOfDataDecoderException ignore) {
                logger.trace("End of multipart items.");
            } finally {
                form.cleanFiles();
                form.destroy();
            }
        } else {
            byte[] content = NettyUtils.getBytes(req.content());
            inputData.addParameter(new InputParameter("body", content, contentType));
        }
        return inputData;
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import com.amazonaws.ml.mms.http.InvalidPluginException;
import java.lang.annotation.Annotation;
import java.util.HashMap;
import java.util.Map;
import java.util.ServiceLoader;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
import software.amazon.ai.mms.servingsdk.annotations.Endpoint;
import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
public final class PluginsManager {
    private static final PluginsManager INSTANCE = new PluginsManager();
    private Logger logger = LoggerFactory.getLogger(PluginsManager.class);
    private Map<String, ModelServerEndpoint> inferenceEndpoints;
    private Map<String, ModelServerEndpoint> managementEndpoints;
    private PluginsManager() {}
    public static PluginsManager getInstance() {
        return INSTANCE;
    }
    public void initialize() {
        inferenceEndpoints = initInferenceEndpoints();
        managementEndpoints = initManagementEndpoints();
    }
    private boolean validateEndpointPlugin(Annotation a, EndpointTypes type) {
        return a instanceof Endpoint
                && !((Endpoint) a).urlPattern().isEmpty()
                && ((Endpoint) a).endpointType().equals(type);
    }
    private HashMap<String, ModelServerEndpoint> getEndpoints(EndpointTypes type)
            throws InvalidPluginException {
        ServiceLoader<ModelServerEndpoint> loader = ServiceLoader.load(ModelServerEndpoint.class);
        HashMap<String, ModelServerEndpoint> ep = new HashMap<>();
        for (ModelServerEndpoint mep : loader) {
            Class<? extends ModelServerEndpoint> modelServerEndpointClassObj = mep.getClass();
            Annotation[] annotations = modelServerEndpointClassObj.getAnnotations();
            for (Annotation a : annotations) {
                if (validateEndpointPlugin(a, type)) {
                    if (ep.get(((Endpoint) a).urlPattern()) != null) {
                        throw new InvalidPluginException(
                                "Multiple plugins found for endpoint "
                                        + "\""
                                        + ((Endpoint) a).urlPattern()
                                        + "\"");
                    }
                    logger.info("Loading plugin for endpoint {}", ((Endpoint) a).urlPattern());
                    ep.put(((Endpoint) a).urlPattern(), mep);
                }
            }
        }
        return ep;
    }
    private HashMap<String, ModelServerEndpoint> initInferenceEndpoints() {
        return getEndpoints(EndpointTypes.INFERENCE);
    }
    private HashMap<String, ModelServerEndpoint> initManagementEndpoints() {
        return getEndpoints(EndpointTypes.MANAGEMENT);
    }
    public Map<String, ModelServerEndpoint> getInferenceEndpoints() {
        return inferenceEndpoints;
    }
    public Map<String, ModelServerEndpoint> getManagementEndpoints() {
        return managementEndpoints;
    }
}
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.archive.ModelNotFoundException;
import com.amazonaws.ml.mms.servingsdk.impl.ModelServerContext;
import com.amazonaws.ml.mms.servingsdk.impl.ModelServerRequest;
import com.amazonaws.ml.mms.servingsdk.impl.ModelServerResponse;
import com.amazonaws.ml.mms.util.NettyUtils;
import com.amazonaws.ml.mms.wlm.ModelManager;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.DefaultFullHttpResponse;
import io.netty.handler.codec.http.DefaultHttpHeadersFactory;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.FullHttpResponse;
import io.netty.handler.codec.http.HttpResponseStatus;
import io.netty.handler.codec.http.HttpVersion;
import io.netty.handler.codec.http.QueryStringDecoder;
import java.io.IOException;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
import software.amazon.ai.mms.servingsdk.ModelServerEndpointException;
public abstract class HttpRequestHandlerChain {
    private static final Logger logger = LoggerFactory.getLogger(HttpRequestHandler.class);
    protected Map<String, ModelServerEndpoint> endpointMap;
    protected HttpRequestHandlerChain chain;
    public HttpRequestHandlerChain() {}
    public HttpRequestHandlerChain(Map<String, ModelServerEndpoint> map) {
        endpointMap = map;
    }
    public HttpRequestHandlerChain setNextHandler(HttpRequestHandlerChain nextHandler) {
        chain = nextHandler;
        return chain;
    }
    protected abstract void handleRequest(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelNotFoundException, ModelException;
    private void run(
            ModelServerEndpoint endpoint,
            FullHttpRequest req,
            FullHttpResponse rsp,
            QueryStringDecoder decoder,
            String method)
            throws IOException {
        switch (method) {
            case "GET":
                endpoint.doGet(
                        new ModelServerRequest(req, decoder),
                        new ModelServerResponse(rsp),
                        new ModelServerContext());
                break;
            case "PUT":
                endpoint.doPut(
                        new ModelServerRequest(req, decoder),
                        new ModelServerResponse(rsp),
                        new ModelServerContext());
                break;
            case "DELETE":
                endpoint.doDelete(
                        new ModelServerRequest(req, decoder),
                        new ModelServerResponse(rsp),
                        new ModelServerContext());
                break;
            case "POST":
                endpoint.doPost(
                        new ModelServerRequest(req, decoder),
                        new ModelServerResponse(rsp),
                        new ModelServerContext());
                break;
            default:
                throw new ServiceUnavailableException("Invalid HTTP method received");
        }
    }
    protected void handleCustomEndpoint(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            String[] segments,
            QueryStringDecoder decoder) {
        ModelServerEndpoint endpoint = endpointMap.get(segments[1]);
        Runnable r =
                () -> {
                    Long start = System.currentTimeMillis();
                    FullHttpResponse rsp =
                            new DefaultFullHttpResponse(
                                    HttpVersion.HTTP_1_1,
                                    HttpResponseStatus.OK,
                                    Unpooled.directBuffer(),
                                    DefaultHttpHeadersFactory.headersFactory(),
                                    DefaultHttpHeadersFactory.trailersFactory());
                    try {
                        run(endpoint, req, rsp, decoder, req.method().toString());
                        NettyUtils.sendHttpResponse(ctx, rsp, true);
                        logger.info(
                                "Running \"{}\" endpoint took {} ms",
                                segments[0],
                                System.currentTimeMillis() - start);
                    } catch (ModelServerEndpointException me) {
                        NettyUtils.sendError(ctx, HttpResponseStatus.INTERNAL_SERVER_ERROR, me);
                        logger.error("Error thrown by the model endpoint plugin.", me);
                    } catch (OutOfMemoryError oom) {
                        NettyUtils.sendError(
                                ctx, HttpResponseStatus.INSUFFICIENT_STORAGE, oom, "Out of memory");
                    } catch (IOException ioe) {
                        NettyUtils.sendError(
                                ctx,
                                HttpResponseStatus.INTERNAL_SERVER_ERROR,
                                ioe,
                                "I/O error while running the custom endpoint");
                        logger.error("I/O error while running the custom endpoint.", ioe);
                    } catch (Throwable e) {
                        NettyUtils.sendError(
                                ctx,
                                HttpResponseStatus.INTERNAL_SERVER_ERROR,
                                e,
                                "Unknown exception");
                        logger.error("Unknown exception", e);
                    }
                };
        ModelManager.getInstance().submitTask(r);
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.HttpUtil;
import io.netty.handler.codec.http.QueryStringDecoder;
import java.io.ByteArrayInputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import software.amazon.ai.mms.servingsdk.http.Request;
public class ModelServerRequest implements Request {
    private FullHttpRequest req;
    private QueryStringDecoder decoder;
    public ModelServerRequest(FullHttpRequest r, QueryStringDecoder d) {
        req = r;
        decoder = d;
    }
    @Override
    public List<String> getHeaderNames() {
        return new ArrayList<>(req.headers().names());
    }
    @Override
    public String getRequestURI() {
        return req.uri();
    }
    @Override
    public Map<String, List<String>> getParameterMap() {
        return decoder.parameters();
    }
    @Override
    public List<String> getParameter(String k) {
        return decoder.parameters().get(k);
    }
    @Override
    public String getContentType() {
        return HttpUtil.getMimeType(req).toString();
    }
    @Override
    public ByteArrayInputStream getInputStream() {
        return new ByteArrayInputStream(req.content().array());
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.Manifest;
import com.amazonaws.ml.mms.archive.ModelArchive;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.archive.ModelNotFoundException;
import com.amazonaws.ml.mms.http.messages.RegisterModelRequest;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.util.JsonUtils;
import com.amazonaws.ml.mms.util.NettyUtils;
import com.amazonaws.ml.mms.wlm.Model;
import com.amazonaws.ml.mms.wlm.ModelManager;
import com.amazonaws.ml.mms.wlm.WorkerThread;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.HttpHeaderValues;
import io.netty.handler.codec.http.HttpMethod;
import io.netty.handler.codec.http.HttpResponseStatus;
import io.netty.handler.codec.http.HttpUtil;
import io.netty.handler.codec.http.QueryStringDecoder;
import io.netty.util.CharsetUtil;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeoutException;
import java.util.function.Function;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
/**
 * A class handling inbound HTTP requests to the management API.
 *
 * <p>This class
 */
public class ManagementRequestHandler extends HttpRequestHandlerChain {
    /** Creates a new {@code ManagementRequestHandler} instance. */
    public ManagementRequestHandler(Map<String, ModelServerEndpoint> ep) {
        endpointMap = ep;
    }
    @Override
    protected void handleRequest(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelException {
        if (isManagementReq(segments)) {
            if (endpointMap.getOrDefault(segments[1], null) != null) {
                handleCustomEndpoint(ctx, req, segments, decoder);
            } else {
                if (!"models".equals(segments[1])) {
                    throw new ResourceNotFoundException();
                }
                HttpMethod method = req.method();
                if (segments.length < 3) {
                    if (HttpMethod.GET.equals(method)) {
                        handleListModels(ctx, decoder);
                        return;
                    } else if (HttpMethod.POST.equals(method)) {
                        handleRegisterModel(ctx, decoder, req);
                        return;
                    }
                    throw new MethodNotAllowedException();
                }
                if (HttpMethod.GET.equals(method)) {
                    handleDescribeModel(ctx, segments[2]);
                } else if (HttpMethod.PUT.equals(method)) {
                    handleScaleModel(ctx, decoder, segments[2]);
                } else if (HttpMethod.DELETE.equals(method)) {
                    handleUnregisterModel(ctx, segments[2]);
                } else {
                    throw new MethodNotAllowedException();
                }
            }
        } else {
            chain.handleRequest(ctx, req, decoder, segments);
        }
    }
    private boolean isManagementReq(String[] segments) {
        return segments.length == 0
                || ((segments.length == 2 || segments.length == 3) && segments[1].equals("models"))
                || endpointMap.containsKey(segments[1]);
    }
    private void handleListModels(ChannelHandlerContext ctx, QueryStringDecoder decoder) {
        int limit = NettyUtils.getIntParameter(decoder, "limit", 100);
        int pageToken = NettyUtils.getIntParameter(decoder, "next_page_token", 0);
        if (limit > 100 || limit < 0) {
            limit = 100;
        }
        if (pageToken < 0) {
            pageToken = 0;
        }
        ModelManager modelManager = ModelManager.getInstance();
        Map<String, Model> models = modelManager.getModels();
        List<String> keys = new ArrayList<>(models.keySet());
        Collections.sort(keys);
        ListModelsResponse list = new ListModelsResponse();
        int last = pageToken + limit;
        if (last > keys.size()) {
            last = keys.size();
        } else {
            list.setNextPageToken(String.valueOf(last));
        }
        for (int i = pageToken; i < last; ++i) {
            String modelName = keys.get(i);
            Model model = models.get(modelName);
            list.addModel(modelName, model.getModelUrl());
        }
        NettyUtils.sendJsonResponse(ctx, list);
    }
    private void handleDescribeModel(ChannelHandlerContext ctx, String modelName)
            throws ModelNotFoundException {
        ModelManager modelManager = ModelManager.getInstance();
        Model model = modelManager.getModels().get(modelName);
        if (model == null) {
            throw new ModelNotFoundException("Model not found: " + modelName);
        }
        DescribeModelResponse resp = new DescribeModelResponse();
        resp.setModelName(modelName);
        resp.setModelUrl(model.getModelUrl());
        resp.setBatchSize(model.getBatchSize());
        resp.setMaxBatchDelay(model.getMaxBatchDelay());
        resp.setMaxWorkers(model.getMaxWorkers());
        resp.setMinWorkers(model.getMinWorkers());
        resp.setLoadedAtStartup(modelManager.getStartupModels().contains(modelName));
        Manifest manifest = model.getModelArchive().getManifest();
        Manifest.Engine engine = manifest.getEngine();
        if (engine != null) {
            resp.setEngine(engine.getEngineName());
        }
        resp.setModelVersion(manifest.getModel().getModelVersion());
        resp.setRuntime(manifest.getRuntime().getValue());
        List<WorkerThread> workers = modelManager.getWorkers(modelName);
        for (WorkerThread worker : workers) {
            String workerId = worker.getWorkerId();
            long startTime = worker.getStartTime();
            boolean isRunning = worker.isRunning();
            int gpuId = worker.getGpuId();
            long memory = worker.getMemory();
            resp.addWorker(workerId, startTime, isRunning, gpuId, memory);
        }
        NettyUtils.sendJsonResponse(ctx, resp);
    }
    private void handleRegisterModel(
            ChannelHandlerContext ctx, QueryStringDecoder decoder, FullHttpRequest req)
            throws ModelException {
        RegisterModelRequest registerModelRequest = parseRequest(req, decoder);
        String modelUrl = registerModelRequest.getModelUrl();
        if (modelUrl == null) {
            throw new BadRequestException("Parameter url is required.");
        }
        String modelName = registerModelRequest.getModelName();
        String runtime = registerModelRequest.getRuntime();
        String handler = registerModelRequest.getHandler();
        int batchSize = registerModelRequest.getBatchSize();
        int maxBatchDelay = registerModelRequest.getMaxBatchDelay();
        int initialWorkers = registerModelRequest.getInitialWorkers();
        boolean synchronous = registerModelRequest.isSynchronous();
        int responseTimeoutSeconds = registerModelRequest.getResponseTimeoutSeconds();
        String preloadModel = registerModelRequest.getPreloadModel();
        if (preloadModel == null) {
            preloadModel = ConfigManager.getInstance().getPreloadModel();
        }
        if (responseTimeoutSeconds == -1) {
            responseTimeoutSeconds = ConfigManager.getInstance().getDefaultResponseTimeoutSeconds();
        }
        Manifest.RuntimeType runtimeType = null;
        if (runtime != null) {
            try {
                runtimeType = Manifest.RuntimeType.fromValue(runtime);
            } catch (IllegalArgumentException e) {
                throw new BadRequestException(e);
            }
        }
        ModelManager modelManager = ModelManager.getInstance();
        final ModelArchive archive;
        try {
            archive =
                    modelManager.registerModel(
                            modelUrl,
                            modelName,
                            runtimeType,
                            handler,
                            batchSize,
                            maxBatchDelay,
                            responseTimeoutSeconds,
                            null,
                            preloadModel);
        } catch (IOException | InterruptedException | ExecutionException | TimeoutException e) {
            throw new InternalServerException("Failed to save model: " + modelUrl, e);
        }
        modelName = archive.getModelName();
        final String msg = "Model \"" + modelName + "\" registered";
        if (initialWorkers <= 0) {
            NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg));
            return;
        }
        updateModelWorkers(
                ctx,
                modelName,
                initialWorkers,
                initialWorkers,
                synchronous,
                f -> {
                    modelManager.unregisterModel(archive.getModelName());
                    return null;
                });
    }
    private void handleUnregisterModel(ChannelHandlerContext ctx, String modelName)
            throws ModelNotFoundException, InternalServerException, RequestTimeoutException {
        ModelManager modelManager = ModelManager.getInstance();
        HttpResponseStatus httpResponseStatus = modelManager.unregisterModel(modelName);
        if (httpResponseStatus == HttpResponseStatus.NOT_FOUND) {
            throw new ModelNotFoundException("Model not found: " + modelName);
        } else if (httpResponseStatus == HttpResponseStatus.INTERNAL_SERVER_ERROR) {
            throw new InternalServerException("Interrupted while cleaning resources: " + modelName);
        } else if (httpResponseStatus == HttpResponseStatus.REQUEST_TIMEOUT) {
            throw new RequestTimeoutException("Timed out while cleaning resources: " + modelName);
        }
        String msg = "Model \"" + modelName + "\" unregistered";
        NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg));
    }
    private void handleScaleModel(
            ChannelHandlerContext ctx, QueryStringDecoder decoder, String modelName)
            throws ModelNotFoundException {
        int minWorkers = NettyUtils.getIntParameter(decoder, "min_worker", 1);
        int maxWorkers = NettyUtils.getIntParameter(decoder, "max_worker", minWorkers);
        if (maxWorkers < minWorkers) {
            throw new BadRequestException("max_worker cannot be less than min_worker.");
        }
        boolean synchronous =
                Boolean.parseBoolean(NettyUtils.getParameter(decoder, "synchronous", null));
        ModelManager modelManager = ModelManager.getInstance();
        if (!modelManager.getModels().containsKey(modelName)) {
            throw new ModelNotFoundException("Model not found: " + modelName);
        }
        updateModelWorkers(ctx, modelName, minWorkers, maxWorkers, synchronous, null);
    }
    private void updateModelWorkers(
            final ChannelHandlerContext ctx,
            final String modelName,
            int minWorkers,
            int maxWorkers,
            boolean synchronous,
            final Function<Void, Void> onError) {
        ModelManager modelManager = ModelManager.getInstance();
        CompletableFuture<HttpResponseStatus> future =
                modelManager.updateModel(modelName, minWorkers, maxWorkers);
        if (!synchronous) {
            NettyUtils.sendJsonResponse(
                    ctx,
                    new StatusResponse("Processing worker updates..."),
                    HttpResponseStatus.ACCEPTED);
            return;
        }
        future.thenApply(
                        v -> {
                            boolean status = modelManager.scaleRequestStatus(modelName);
                            if (HttpResponseStatus.OK.equals(v)) {
                                if (status) {
                                    NettyUtils.sendJsonResponse(
                                            ctx, new StatusResponse("Workers scaled"), v);
                                } else {
                                    NettyUtils.sendJsonResponse(
                                            ctx,
                                            new StatusResponse("Workers scaling in progress..."),
                                            new HttpResponseStatus(210, "Partial Success"));
                                }
                            } else {
                                NettyUtils.sendError(
                                        ctx,
                                        v,
                                        new InternalServerException("Failed to start workers"));
                                if (onError != null) {
                                    onError.apply(null);
                                }
                            }
                            return v;
                        })
                .exceptionally(
                        (e) -> {
                            if (onError != null) {
                                onError.apply(null);
                            }
                            NettyUtils.sendError(ctx, HttpResponseStatus.INTERNAL_SERVER_ERROR, e);
                            return null;
                        });
    }
    private RegisterModelRequest parseRequest(FullHttpRequest req, QueryStringDecoder decoder) {
        RegisterModelRequest in;
        CharSequence mime = HttpUtil.getMimeType(req);
        if (HttpHeaderValues.APPLICATION_JSON.contentEqualsIgnoreCase(mime)) {
            in =
                    JsonUtils.GSON.fromJson(
                            req.content().toString(CharsetUtil.UTF_8), RegisterModelRequest.class);
        } else {
            in = new RegisterModelRequest(decoder);
        }
        return in;
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms;
import com.amazonaws.ml.mms.archive.ModelArchive;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.metrics.MetricManager;
import com.amazonaws.ml.mms.servingsdk.impl.PluginsManager;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.util.Connector;
import com.amazonaws.ml.mms.util.ConnectorType;
import com.amazonaws.ml.mms.util.ServerGroups;
import com.amazonaws.ml.mms.wlm.ModelManager;
import com.amazonaws.ml.mms.wlm.WorkLoadManager;
import io.netty.bootstrap.ServerBootstrap;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelFutureListener;
import io.netty.channel.ChannelOption;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.FixedRecvByteBufAllocator;
import io.netty.channel.ServerChannel;
import io.netty.handler.ssl.SslContext;
import io.netty.util.internal.logging.InternalLoggerFactory;
import io.netty.util.internal.logging.Slf4JLoggerFactory;
import java.io.File;
import java.io.IOException;
import java.lang.annotation.Annotation;
import java.security.GeneralSecurityException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.InvalidPropertiesFormatException;
import java.util.List;
import java.util.ServiceLoader;
import java.util.Set;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeoutException;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.DefaultParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
import software.amazon.ai.mms.servingsdk.annotations.Endpoint;
import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
public class ModelServer {
    private Logger logger = LoggerFactory.getLogger(ModelServer.class);
    private ServerGroups serverGroups;
    private List<ChannelFuture> futures = new ArrayList<>(2);
    private AtomicBoolean stopped = new AtomicBoolean(false);
    private ConfigManager configManager;
    public static final int MAX_RCVBUF_SIZE = 4096;
    /** Creates a new {@code ModelServer} instance. */
    public ModelServer(ConfigManager configManager) {
        this.configManager = configManager;
        serverGroups = new ServerGroups(configManager);
    }
    public static void main(String[] args) {
        Options options = ConfigManager.Arguments.getOptions();
        try {
            DefaultParser parser = new DefaultParser();
            CommandLine cmd = parser.parse(options, args, null, false);
            ConfigManager.Arguments arguments = new ConfigManager.Arguments(cmd);
            ConfigManager.init(arguments);
            ConfigManager configManager = ConfigManager.getInstance();
            PluginsManager.getInstance().initialize();
            InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
            new ModelServer(configManager).startAndWait();
        } catch (IllegalArgumentException e) {
            System.out.println("Invalid configuration: " + e.getMessage()); // NOPMD
        } catch (ParseException e) {
            HelpFormatter formatter = new HelpFormatter();
            formatter.setLeftPadding(1);
            formatter.setWidth(120);
            formatter.printHelp(e.getMessage(), options);
        } catch (Throwable t) {
            t.printStackTrace(); // NOPMD
        } finally {
            System.exit(1); // NOPMD
        }
    }
    public void startAndWait() throws InterruptedException, IOException, GeneralSecurityException {
        try {
            List<ChannelFuture> channelFutures = start();
            // Create and schedule metrics manager
            MetricManager.scheduleMetrics(configManager);
            System.out.println("Model server started."); // NOPMD
            channelFutures.get(0).sync();
        } catch (InvalidPropertiesFormatException e) {
            logger.error("Invalid configuration", e);
        } finally {
            serverGroups.shutdown(true);
            logger.info("Model server stopped.");
        }
    }
    private String getDefaultModelName(String name) {
        if (name.contains(".model") || name.contains(".mar")) {
            return name.substring(name.lastIndexOf('/') + 1, name.lastIndexOf('.'))
                    .replaceAll("(\\W|^_)", "_");
        } else {
            return name.substring(name.lastIndexOf('/') + 1).replaceAll("(\\W|^_)", "_");
        }
    }
    private void initModelStore() {
        WorkLoadManager wlm = new WorkLoadManager(configManager, serverGroups.getBackendGroup());
        ModelManager.init(configManager, wlm);
        Set<String> startupModels = ModelManager.getInstance().getStartupModels();
        String defaultModelName;
        String loadModels = configManager.getLoadModels();
        if (loadModels == null || loadModels.isEmpty()) {
            return;
        }
        ModelManager modelManager = ModelManager.getInstance();
        int workers = configManager.getDefaultWorkers();
        if ("ALL".equalsIgnoreCase(loadModels)) {
            String modelStore = configManager.getModelStore();
            if (modelStore == null) {
                logger.warn("Model store is not configured.");
                return;
            }
            File modelStoreDir = new File(modelStore);
            if (!modelStoreDir.exists()) {
                logger.warn("Model store path is not found: {}", modelStore);
                return;
            }
            // Check folders to see if they can be models as well
            File[] files = modelStoreDir.listFiles();
            if (files != null) {
                for (File file : files) {
                    if (file.isHidden()) {
                        continue;
                    }
                    String fileName = file.getName();
                    if (file.isFile()
                            && !fileName.endsWith(".mar")
                            && !fileName.endsWith(".model")) {
                        continue;
                    }
                    try {
                        logger.debug(
                                "Loading models from model store: {} preload_model: {}",
                                file.getName(),
                                configManager.getPreloadModel());
                        defaultModelName = getDefaultModelName(fileName);
                        ModelArchive archive =
                                modelManager.registerModel(
                                        file.getName(),
                                        defaultModelName,
                                        configManager.getPreloadModel());
                        modelManager.updateModel(archive.getModelName(), workers, workers);
                        startupModels.add(archive.getModelName());
                    } catch (ModelException
                            | IOException
                            | InterruptedException
                            | ExecutionException
                            | TimeoutException e) {
                        logger.warn("Failed to load model: " + file.getAbsolutePath(), e);
                    }
                }
            }
            return;
        }
        String[] models = loadModels.split(",");
        for (String model : models) {
            String[] pair = model.split("=", 2);
            String modelName = null;
            String url;
            if (pair.length == 1) {
                url = pair[0];
            } else {
                modelName = pair[0];
                url = pair[1];
            }
            if (url.isEmpty()) {
                continue;
            }
            try {
                logger.info(
                        "Loading initial models: {} preload_model: {}",
                        url,
                        configManager.getPreloadModel());
                defaultModelName = getDefaultModelName(url);
                ModelArchive archive =
                        modelManager.registerModel(
                                url,
                                modelName,
                                null,
                                null,
                                1,
                                100,
                                configManager.getDefaultResponseTimeoutSeconds(),
                                defaultModelName,
                                configManager.getPreloadModel());
                modelManager.updateModel(archive.getModelName(), workers, workers);
                startupModels.add(archive.getModelName());
            } catch (ModelException
                    | IOException
                    | InterruptedException
                    | ExecutionException
                    | TimeoutException e) {
                logger.warn("Failed to load model: " + url, e);
            }
        }
    }
    private void exitModelStore() {
        for (String modelName : ModelManager.getInstance().getModels().keySet()) {
            ModelManager.getInstance().unregisterModel(modelName);
        }
    }
    public ChannelFuture initializeServer(
            Connector connector,
            EventLoopGroup serverGroup,
            EventLoopGroup workerGroup,
            ConnectorType type)
            throws InterruptedException, IOException, GeneralSecurityException {
        final String purpose = connector.getPurpose();
        Class<? extends ServerChannel> channelClass = connector.getServerChannel();
        logger.info("Initialize {} server with: {}.", purpose, channelClass.getSimpleName());
        ServerBootstrap b = new ServerBootstrap();
        b.option(ChannelOption.SO_BACKLOG, 1024)
                .channel(channelClass)
                .childOption(ChannelOption.SO_LINGER, 0)
                .childOption(ChannelOption.SO_REUSEADDR, true)
                .childOption(ChannelOption.SO_KEEPALIVE, true)
                .childOption(
                        ChannelOption.RCVBUF_ALLOCATOR,
                        new FixedRecvByteBufAllocator(MAX_RCVBUF_SIZE));
        b.group(serverGroup, workerGroup);
        SslContext sslCtx = null;
        if (connector.isSsl()) {
            sslCtx = configManager.getSslContext();
        }
        b.childHandler(new ServerInitializer(sslCtx, type));
        ChannelFuture future;
        try {
            future = b.bind(connector.getSocketAddress()).sync();
        } catch (Exception e) {
            // https://github.com/netty/netty/issues/2597
            if (e instanceof IOException) {
                throw new IOException("Failed to bind to address: " + connector, e);
            }
            throw e;
        }
        future.addListener(
                (ChannelFutureListener)
                        f -> {
                            if (!f.isSuccess()) {
                                try {
                                    f.get();
                                } catch (InterruptedException | ExecutionException e) {
                                    logger.error("", e);
                                }
                                System.exit(-1); // NO PMD
                            }
                            serverGroups.registerChannel(f.channel());
                        });
        future.sync();
        ChannelFuture f = future.channel().closeFuture();
        f.addListener(
                (ChannelFutureListener)
                        listener -> logger.info("{} model server stopped.", purpose));
        logger.info("{} API bind to: {}", purpose, connector);
        return f;
    }
    /**
     * Main Method that prepares the future for the channel and sets up the ServerBootstrap.
     *
     * @return A ChannelFuture object
     * @throws InterruptedException if interrupted
     */
    public List<ChannelFuture> start()
            throws InterruptedException, IOException, GeneralSecurityException {
        stopped.set(false);
        configManager.validateConfigurations();
        logger.info(configManager.dumpConfigurations());
        initModelStore();
        Connector inferenceConnector = configManager.getListener(false);
        Connector managementConnector = configManager.getListener(true);
        inferenceConnector.clean();
        managementConnector.clean();
        EventLoopGroup serverGroup = serverGroups.getServerGroup();
        EventLoopGroup workerGroup = serverGroups.getChildGroup();
        futures.clear();
        if (!inferenceConnector.equals(managementConnector)) {
            futures.add(
                    initializeServer(
                            inferenceConnector,
                            serverGroup,
                            workerGroup,
                            ConnectorType.INFERENCE_CONNECTOR));
            futures.add(
                    initializeServer(
                            managementConnector,
                            serverGroup,
                            workerGroup,
                            ConnectorType.MANAGEMENT_CONNECTOR));
        } else {
            futures.add(
                    initializeServer(
                            inferenceConnector, serverGroup, workerGroup, ConnectorType.BOTH));
        }
        return futures;
    }
    private boolean validEndpoint(Annotation a, EndpointTypes type) {
        return a instanceof Endpoint
                && !((Endpoint) a).urlPattern().isEmpty()
                && ((Endpoint) a).endpointType().equals(type);
    }
    private HashMap<String, ModelServerEndpoint> registerEndpoints(EndpointTypes type) {
        ServiceLoader<ModelServerEndpoint> loader = ServiceLoader.load(ModelServerEndpoint.class);
        HashMap<String, ModelServerEndpoint> ep = new HashMap<>();
        for (ModelServerEndpoint mep : loader) {
            Class<? extends ModelServerEndpoint> modelServerEndpointClassObj = mep.getClass();
            Annotation[] annotations = modelServerEndpointClassObj.getAnnotations();
            for (Annotation a : annotations) {
                if (validEndpoint(a, type)) {
                    ep.put(((Endpoint) a).urlPattern(), mep);
                }
            }
        }
        return ep;
    }
    public boolean isRunning() {
        return !stopped.get();
    }
    public void stop() {
        if (stopped.get()) {
            return;
        }
        stopped.set(true);
        for (ChannelFuture future : futures) {
            future.channel().close();
        }
        serverGroups.shutdown(true);
        serverGroups.init();
        exitModelStore();
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms;
import com.amazonaws.ml.mms.http.ApiDescriptionRequestHandler;
import com.amazonaws.ml.mms.http.HttpRequestHandler;
import com.amazonaws.ml.mms.http.HttpRequestHandlerChain;
import com.amazonaws.ml.mms.http.InferenceRequestHandler;
import com.amazonaws.ml.mms.http.InvalidRequestHandler;
import com.amazonaws.ml.mms.http.ManagementRequestHandler;
import com.amazonaws.ml.mms.servingsdk.impl.PluginsManager;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.util.ConnectorType;
import io.netty.channel.Channel;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.ChannelPipeline;
import io.netty.handler.codec.http.HttpObjectAggregator;
import io.netty.handler.codec.http.HttpServerCodec;
import io.netty.handler.ssl.SslContext;
/**
 * A special {@link io.netty.channel.ChannelInboundHandler} which offers an easy way to initialize a
 * {@link io.netty.channel.Channel} once it was registered to its {@link
 * io.netty.channel.EventLoop}.
 */
public class ServerInitializer extends ChannelInitializer<Channel> {
    private ConnectorType connectorType;
    private SslContext sslCtx;
    /**
     * Creates a new {@code HttpRequestHandler} instance.
     *
     * @param sslCtx null if SSL is not enabled
     * @param type true to initialize a management server instead of an API Server
     */
    public ServerInitializer(SslContext sslCtx, ConnectorType type) {
        this.sslCtx = sslCtx;
        this.connectorType = type;
    }
    /** {@inheritDoc} */
    @Override
    public void initChannel(Channel ch) {
        ChannelPipeline pipeline = ch.pipeline();
        HttpRequestHandlerChain apiDescriptionRequestHandler =
                new ApiDescriptionRequestHandler(connectorType);
        HttpRequestHandlerChain invalidRequestHandler = new InvalidRequestHandler();
        int maxRequestSize = ConfigManager.getInstance().getMaxRequestSize();
        if (sslCtx != null) {
            pipeline.addLast("ssl", sslCtx.newHandler(ch.alloc()));
        }
        pipeline.addLast("http", new HttpServerCodec());
        pipeline.addLast("aggregator", new HttpObjectAggregator(maxRequestSize));
        HttpRequestHandlerChain httpRequestHandlerChain = apiDescriptionRequestHandler;
        if (ConnectorType.BOTH.equals(connectorType)
                || ConnectorType.INFERENCE_CONNECTOR.equals(connectorType)) {
            httpRequestHandlerChain =
                    httpRequestHandlerChain.setNextHandler(
                            new InferenceRequestHandler(
                                    PluginsManager.getInstance().getInferenceEndpoints()));
        }
        if (ConnectorType.BOTH.equals(connectorType)
                || ConnectorType.MANAGEMENT_CONNECTOR.equals(connectorType)) {
            httpRequestHandlerChain =
                    httpRequestHandlerChain.setNextHandler(
                            new ManagementRequestHandler(
                                    PluginsManager.getInstance().getManagementEndpoints()));
        }
        httpRequestHandlerChain.setNextHandler(invalidRequestHandler);
        pipeline.addLast("handler", new HttpRequestHandler(apiDescriptionRequestHandler));
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import com.amazonaws.ml.mms.wlm.WorkerState;
import com.amazonaws.ml.mms.wlm.WorkerThread;
import software.amazon.ai.mms.servingsdk.Worker;
public class ModelWorker implements Worker {
    boolean running;
    long memory;
    public ModelWorker(WorkerThread t) {
        running = t.getState() == WorkerState.WORKER_MODEL_LOADED;
        memory = t.getMemory();
    }
    @Override
    public boolean isRunning() {
        return running;
    }
    @Override
    public long getWorkerMemory() {
        return memory;
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.wlm.ModelManager;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import software.amazon.ai.mms.servingsdk.Context;
import software.amazon.ai.mms.servingsdk.Model;
public class ModelServerContext implements Context {
    @Override
    public Properties getConfig() {
        return ConfigManager.getInstance().getConfiguration();
    }
    @Override
    public Map<String, Model> getModels() {
        HashMap<String, Model> r = new HashMap<>();
        ModelManager.getInstance().getModels().forEach((k, v) -> r.put(k, new ModelServerModel(v)));
        return r;
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.Manifest;
import com.amazonaws.ml.mms.archive.ModelArchive;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.archive.ModelNotFoundException;
import com.amazonaws.ml.mms.http.messages.RegisterModelRequest;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.util.JsonUtils;
import com.amazonaws.ml.mms.util.NettyUtils;
import com.amazonaws.ml.mms.wlm.Model;
import com.amazonaws.ml.mms.wlm.ModelManager;
import com.amazonaws.ml.mms.wlm.WorkerThread;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.HttpHeaderValues;
import io.netty.handler.codec.http.HttpMethod;
import io.netty.handler.codec.http.HttpResponseStatus;
import io.netty.handler.codec.http.HttpUtil;
import io.netty.handler.codec.http.QueryStringDecoder;
import io.netty.util.CharsetUtil;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeoutException;
import java.util.function.Function;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
/**
 * A class handling inbound HTTP requests to the management API.
 *
 * <p>This class
 */
public class ManagementRequestHandler extends HttpRequestHandlerChain {
    /** Creates a new {@code ManagementRequestHandler} instance. */
    public ManagementRequestHandler(Map<String, ModelServerEndpoint> ep) {
        endpointMap = ep;
    }
    @Override
    protected void handleRequest(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelException {
        if (isManagementReq(segments)) {
            if (endpointMap.getOrDefault(segments[1], null) != null) {
                handleCustomEndpoint(ctx, req, segments, decoder);
            } else {
                if (!"models".equals(segments[1])) {
                    throw new ResourceNotFoundException();
                }
                HttpMethod method = req.method();
                if (segments.length < 3) {
                    if (HttpMethod.GET.equals(method)) {
                        handleListModels(ctx, decoder);
                        return;
                    } else if (HttpMethod.POST.equals(method)) {
                        handleRegisterModel(ctx, decoder, req);
                        return;
                    }
                    throw new MethodNotAllowedException();
                }
                if (HttpMethod.GET.equals(method)) {
                    handleDescribeModel(ctx, segments[2]);
                } else if (HttpMethod.PUT.equals(method)) {
                    handleScaleModel(ctx, decoder, segments[2]);
                } else if (HttpMethod.DELETE.equals(method)) {
                    handleUnregisterModel(ctx, segments[2]);
                } else {
                    throw new MethodNotAllowedException();
                }
            }
        } else {
            chain.handleRequest(ctx, req, decoder, segments);
        }
    }
    private boolean isManagementReq(String[] segments) {
        return segments.length == 0
                || ((segments.length == 2 || segments.length == 3) && segments[1].equals("models"))
                || endpointMap.containsKey(segments[1]);
    }
    private void handleListModels(ChannelHandlerContext ctx, QueryStringDecoder decoder) {
        int limit = NettyUtils.getIntParameter(decoder, "limit", 100);
        int pageToken = NettyUtils.getIntParameter(decoder, "next_page_token", 0);
        if (limit > 100 || limit < 0) {
            limit = 100;
        }
        if (pageToken < 0) {
            pageToken = 0;
        }
        ModelManager modelManager = ModelManager.getInstance();
        Map<String, Model> models = modelManager.getModels();
        List<String> keys = new ArrayList<>(models.keySet());
        Collections.sort(keys);
        ListModelsResponse list = new ListModelsResponse();
        int last = pageToken + limit;
        if (last > keys.size()) {
            last = keys.size();
        } else {
            list.setNextPageToken(String.valueOf(last));
        }
        for (int i = pageToken; i < last; ++i) {
            String modelName = keys.get(i);
            Model model = models.get(modelName);
            list.addModel(modelName, model.getModelUrl());
        }
        NettyUtils.sendJsonResponse(ctx, list);
    }
    private void handleDescribeModel(ChannelHandlerContext ctx, String modelName)
            throws ModelNotFoundException {
        ModelManager modelManager = ModelManager.getInstance();
        Model model = modelManager.getModels().get(modelName);
        if (model == null) {
            throw new ModelNotFoundException("Model not found: " + modelName);
        }
        DescribeModelResponse resp = new DescribeModelResponse();
        resp.setModelName(modelName);
        resp.setModelUrl(model.getModelUrl());
        resp.setBatchSize(model.getBatchSize());
        resp.setMaxBatchDelay(model.getMaxBatchDelay());
        resp.setMaxWorkers(model.getMaxWorkers());
        resp.setMinWorkers(model.getMinWorkers());
        resp.setLoadedAtStartup(modelManager.getStartupModels().contains(modelName));
        Manifest manifest = model.getModelArchive().getManifest();
        Manifest.Engine engine = manifest.getEngine();
        if (engine != null) {
            resp.setEngine(engine.getEngineName());
        }
        resp.setModelVersion(manifest.getModel().getModelVersion());
        resp.setRuntime(manifest.getRuntime().getValue());
        List<WorkerThread> workers = modelManager.getWorkers(modelName);
        for (WorkerThread worker : workers) {
            String workerId = worker.getWorkerId();
            long startTime = worker.getStartTime();
            boolean isRunning = worker.isRunning();
            int gpuId = worker.getGpuId();
            long memory = worker.getMemory();
            resp.addWorker(workerId, startTime, isRunning, gpuId, memory);
        }
        NettyUtils.sendJsonResponse(ctx, resp);
    }
    private void handleRegisterModel(
            ChannelHandlerContext ctx, QueryStringDecoder decoder, FullHttpRequest req)
            throws ModelException {
        RegisterModelRequest registerModelRequest = parseRequest(req, decoder);
        String modelUrl = registerModelRequest.getModelUrl();
        if (modelUrl == null) {
            throw new BadRequestException("Parameter url is required.");
        }
        String modelName = registerModelRequest.getModelName();
        String runtime = registerModelRequest.getRuntime();
        String handler = registerModelRequest.getHandler();
        int batchSize = registerModelRequest.getBatchSize();
        int maxBatchDelay = registerModelRequest.getMaxBatchDelay();
        int initialWorkers = registerModelRequest.getInitialWorkers();
        boolean synchronous = registerModelRequest.isSynchronous();
        int responseTimeoutSeconds = registerModelRequest.getResponseTimeoutSeconds();
        String preloadModel = registerModelRequest.getPreloadModel();
        if (preloadModel == null) {
            preloadModel = ConfigManager.getInstance().getPreloadModel();
        }
        if (responseTimeoutSeconds == -1) {
            responseTimeoutSeconds = ConfigManager.getInstance().getDefaultResponseTimeoutSeconds();
        }
        Manifest.RuntimeType runtimeType = null;
        if (runtime != null) {
            try {
                runtimeType = Manifest.RuntimeType.fromValue(runtime);
            } catch (IllegalArgumentException e) {
                throw new BadRequestException(e);
            }
        }
        ModelManager modelManager = ModelManager.getInstance();
        final ModelArchive archive;
        try {
            archive =
                    modelManager.registerModel(
                            modelUrl,
                            modelName,
                            runtimeType,
                            handler,
                            batchSize,
                            maxBatchDelay,
                            responseTimeoutSeconds,
                            null,
                            preloadModel);
        } catch (IOException | InterruptedException | ExecutionException | TimeoutException e) {
            throw new InternalServerException("Failed to save model: " + modelUrl, e);
        }
        modelName = archive.getModelName();
        final String msg = "Model \"" + modelName + "\" registered";
        if (initialWorkers <= 0) {
            NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg));
            return;
        }
        updateModelWorkers(
                ctx,
                modelName,
                initialWorkers,
                initialWorkers,
                synchronous,
                f -> {
                    modelManager.unregisterModel(archive.getModelName());
                    return null;
                });
    }
    private void handleUnregisterModel(ChannelHandlerContext ctx, String modelName)
            throws ModelNotFoundException, InternalServerException, RequestTimeoutException {
        ModelManager modelManager = ModelManager.getInstance();
        HttpResponseStatus httpResponseStatus = modelManager.unregisterModel(modelName);
        if (httpResponseStatus == HttpResponseStatus.NOT_FOUND) {
            throw new ModelNotFoundException("Model not found: " + modelName);
        } else if (httpResponseStatus == HttpResponseStatus.INTERNAL_SERVER_ERROR) {
            throw new InternalServerException("Interrupted while cleaning resources: " + modelName);
        } else if (httpResponseStatus == HttpResponseStatus.REQUEST_TIMEOUT) {
            throw new RequestTimeoutException("Timed out while cleaning resources: " + modelName);
        }
        String msg = "Model \"" + modelName + "\" unregistered";
        NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg));
    }
    private void handleScaleModel(
            ChannelHandlerContext ctx, QueryStringDecoder decoder, String modelName)
            throws ModelNotFoundException {
        int minWorkers = NettyUtils.getIntParameter(decoder, "min_worker", 1);
        int maxWorkers = NettyUtils.getIntParameter(decoder, "max_worker", minWorkers);
        if (maxWorkers < minWorkers) {
            throw new BadRequestException("max_worker cannot be less than min_worker.");
        }
        boolean synchronous =
                Boolean.parseBoolean(NettyUtils.getParameter(decoder, "synchronous", null));
        ModelManager modelManager = ModelManager.getInstance();
        if (!modelManager.getModels().containsKey(modelName)) {
            throw new ModelNotFoundException("Model not found: " + modelName);
        }
        updateModelWorkers(ctx, modelName, minWorkers, maxWorkers, synchronous, null);
    }
    private void updateModelWorkers(
            final ChannelHandlerContext ctx,
            final String modelName,
            int minWorkers,
            int maxWorkers,
            boolean synchronous,
            final Function<Void, Void> onError) {
        ModelManager modelManager = ModelManager.getInstance();
        CompletableFuture<HttpResponseStatus> future =
                modelManager.updateModel(modelName, minWorkers, maxWorkers);
        if (!synchronous) {
            NettyUtils.sendJsonResponse(
                    ctx,
                    new StatusResponse("Processing worker updates..."),
                    HttpResponseStatus.ACCEPTED);
            return;
        }
        future.thenApply(
                        v -> {
                            boolean status = modelManager.scaleRequestStatus(modelName);
                            if (HttpResponseStatus.OK.equals(v)) {
                                if (status) {
                                    NettyUtils.sendJsonResponse(
                                            ctx, new StatusResponse("Workers scaled"), v);
                                } else {
                                    NettyUtils.sendJsonResponse(
                                            ctx,
                                            new StatusResponse("Workers scaling in progress..."),
                                            new HttpResponseStatus(210, "Partial Success"));
                                }
                            } else {
                                NettyUtils.sendError(
                                        ctx,
                                        v,
                                        new InternalServerException("Failed to start workers"));
                                if (onError != null) {
                                    onError.apply(null);
                                }
                            }
                            return v;
                        })
                .exceptionally(
                        (e) -> {
                            if (onError != null) {
                                onError.apply(null);
                            }
                            NettyUtils.sendError(ctx, HttpResponseStatus.INTERNAL_SERVER_ERROR, e);
                            return null;
                        });
    }
    private RegisterModelRequest parseRequest(FullHttpRequest req, QueryStringDecoder decoder) {
        RegisterModelRequest in;
        CharSequence mime = HttpUtil.getMimeType(req);
        if (HttpHeaderValues.APPLICATION_JSON.contentEqualsIgnoreCase(mime)) {
            in =
                    JsonUtils.GSON.fromJson(
                            req.content().toString(CharsetUtil.UTF_8), RegisterModelRequest.class);
        } else {
            in = new RegisterModelRequest(decoder);
        }
        return in;
    }
}
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.openapi.OpenApiUtils;
import com.amazonaws.ml.mms.util.ConnectorType;
import com.amazonaws.ml.mms.util.NettyUtils;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.HttpMethod;
import io.netty.handler.codec.http.QueryStringDecoder;
public class ApiDescriptionRequestHandler extends HttpRequestHandlerChain {
    private ConnectorType connectorType;
    public ApiDescriptionRequestHandler(ConnectorType type) {
        connectorType = type;
    }
    @Override
    protected void handleRequest(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelException {
        if (isApiDescription(segments)) {
            String path = decoder.path();
            if (("/".equals(path) && HttpMethod.OPTIONS.equals(req.method()))
                    || (segments.length == 2 && segments[1].equals("api-description"))) {
                handleApiDescription(ctx);
                return;
            }
            throw new MethodNotAllowedException();
        } else {
            chain.handleRequest(ctx, req, decoder, segments);
        }
    }
    private boolean isApiDescription(String[] segments) {
        return segments.length == 0 || segments[1].equals("api-description");
    }
    private void handleApiDescription(ChannelHandlerContext ctx) {
        NettyUtils.sendJsonResponse(ctx, OpenApiUtils.listApis(connectorType));
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.archive.ModelNotFoundException;
import com.amazonaws.ml.mms.openapi.OpenApiUtils;
import com.amazonaws.ml.mms.util.NettyUtils;
import com.amazonaws.ml.mms.util.messages.InputParameter;
import com.amazonaws.ml.mms.util.messages.RequestInput;
import com.amazonaws.ml.mms.util.messages.WorkerCommands;
import com.amazonaws.ml.mms.wlm.Job;
import com.amazonaws.ml.mms.wlm.Model;
import com.amazonaws.ml.mms.wlm.ModelManager;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.HttpHeaderValues;
import io.netty.handler.codec.http.HttpMethod;
import io.netty.handler.codec.http.HttpUtil;
import io.netty.handler.codec.http.QueryStringDecoder;
import io.netty.handler.codec.http.multipart.DefaultHttpDataFactory;
import io.netty.handler.codec.http.multipart.HttpDataFactory;
import io.netty.handler.codec.http.multipart.HttpPostRequestDecoder;
import java.util.List;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
/**
 * A class handling inbound HTTP requests to the management API.
 *
 * <p>This class
 */
public class InferenceRequestHandler extends HttpRequestHandlerChain {
    private static final Logger logger = LoggerFactory.getLogger(InferenceRequestHandler.class);
    /** Creates a new {@code InferenceRequestHandler} instance. */
    public InferenceRequestHandler(Map<String, ModelServerEndpoint> ep) {
        endpointMap = ep;
    }
    @Override
    protected void handleRequest(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelException {
        if (isInferenceReq(segments)) {
            if (endpointMap.getOrDefault(segments[1], null) != null) {
                handleCustomEndpoint(ctx, req, segments, decoder);
            } else {
                switch (segments[1]) {
                    case "ping":
                        ModelManager.getInstance().workerStatus(ctx);
                        break;
                    case "models":
                    case "invocations":
                        validatePredictionsEndpoint(segments);
                        handleInvocations(ctx, req, decoder, segments);
                        break;
                    case "predictions":
                        handlePredictions(ctx, req, segments);
                        break;
                    default:
                        handleLegacyPredict(ctx, req, decoder, segments);
                        break;
                }
            }
        } else {
            chain.handleRequest(ctx, req, decoder, segments);
        }
    }
    private boolean isInferenceReq(String[] segments) {
        return segments.length == 0
                || segments[1].equals("ping")
                || (segments.length == 4 && segments[1].equals("models"))
                || segments[1].equals("predictions")
                || segments[1].equals("api-description")
                || segments[1].equals("invocations")
                || (segments.length == 3 && segments[2].equals("predict"))
                || endpointMap.containsKey(segments[1]);
    }
    private void validatePredictionsEndpoint(String[] segments) {
        if (segments.length == 2 && "invocations".equals(segments[1])) {
            return;
        } else if (segments.length == 4
                && "models".equals(segments[1])
                && "invoke".equals(segments[3])) {
            return;
        }
        throw new ResourceNotFoundException();
    }
    private void handlePredictions(
            ChannelHandlerContext ctx, FullHttpRequest req, String[] segments)
            throws ModelNotFoundException {
        if (segments.length < 3) {
            throw new ResourceNotFoundException();
        }
        predict(ctx, req, null, segments[2]);
    }
    private void handleInvocations(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelNotFoundException {
        String modelName =
                ("invocations".equals(segments[1]))
                        ? NettyUtils.getParameter(decoder, "model_name", null)
                        : segments[2];
        if (modelName == null || modelName.isEmpty()) {
            if (ModelManager.getInstance().getStartupModels().size() == 1) {
                modelName = ModelManager.getInstance().getStartupModels().iterator().next();
            }
        }
        predict(ctx, req, decoder, modelName);
    }
    private void handleLegacyPredict(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelNotFoundException {
        if (segments.length < 3 || !"predict".equals(segments[2])) {
            throw new ResourceNotFoundException();
        }
        predict(ctx, req, decoder, segments[1]);
    }
    private void predict(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String modelName)
            throws ModelNotFoundException, BadRequestException {
        RequestInput input = parseRequest(ctx, req, decoder);
        if (modelName == null) {
            throw new BadRequestException("Parameter model_name is required.");
        }
        if (HttpMethod.OPTIONS.equals(req.method())) {
            ModelManager modelManager = ModelManager.getInstance();
            Model model = modelManager.getModels().get(modelName);
            if (model == null) {
                throw new ModelNotFoundException("Model not found: " + modelName);
            }
            String resp = OpenApiUtils.getModelApi(model);
            NettyUtils.sendJsonResponse(ctx, resp);
            return;
        }
        Job job = new Job(ctx, modelName, WorkerCommands.PREDICT, input);
        if (!ModelManager.getInstance().addJob(job)) {
            throw new ServiceUnavailableException(
                    "No worker is available to serve request for model: "
                            + modelName
                            + ". Consider increasing job queue size.");
        }
    }
    private static RequestInput parseRequest(
            ChannelHandlerContext ctx, FullHttpRequest req, QueryStringDecoder decoder) {
        String requestId = NettyUtils.getRequestId(ctx.channel());
        RequestInput inputData = new RequestInput(requestId);
        if (decoder != null) {
            for (Map.Entry<String, List<String>> entry : decoder.parameters().entrySet()) {
                String key = entry.getKey();
                for (String value : entry.getValue()) {
                    inputData.addParameter(new InputParameter(key, value));
                }
            }
        }
        CharSequence contentType = HttpUtil.getMimeType(req);
        for (Map.Entry<String, String> entry : req.headers().entries()) {
            inputData.updateHeaders(entry.getKey(), entry.getValue());
        }
        if (HttpPostRequestDecoder.isMultipart(req)
                || HttpHeaderValues.APPLICATION_X_WWW_FORM_URLENCODED.contentEqualsIgnoreCase(
                        contentType)) {
            HttpDataFactory factory = new DefaultHttpDataFactory(6553500);
            HttpPostRequestDecoder form = new HttpPostRequestDecoder(factory, req);
            try {
                while (form.hasNext()) {
                    inputData.addParameter(NettyUtils.getFormData(form.next()));
                }
            } catch (HttpPostRequestDecoder.EndOfDataDecoderException ignore) {
                logger.trace("End of multipart items.");
            } finally {
                form.cleanFiles();
                form.destroy();
            }
        } else {
            byte[] content = NettyUtils.getBytes(req.content());
            inputData.addParameter(new InputParameter("body", content, contentType));
        }
        return inputData;
    }
}
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.archive.ModelNotFoundException;
import com.amazonaws.ml.mms.servingsdk.impl.ModelServerContext;
import com.amazonaws.ml.mms.servingsdk.impl.ModelServerRequest;
import com.amazonaws.ml.mms.servingsdk.impl.ModelServerResponse;
import com.amazonaws.ml.mms.util.NettyUtils;
import com.amazonaws.ml.mms.wlm.ModelManager;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.DefaultFullHttpResponse;
import io.netty.handler.codec.http.DefaultHttpHeadersFactory;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.FullHttpResponse;
import io.netty.handler.codec.http.HttpResponseStatus;
import io.netty.handler.codec.http.HttpVersion;
import io.netty.handler.codec.http.QueryStringDecoder;
import java.io.IOException;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
import software.amazon.ai.mms.servingsdk.ModelServerEndpointException;
public abstract class HttpRequestHandlerChain {
    private static final Logger logger = LoggerFactory.getLogger(HttpRequestHandler.class);
    protected Map<String, ModelServerEndpoint> endpointMap;
    protected HttpRequestHandlerChain chain;
    public HttpRequestHandlerChain() {}
    public HttpRequestHandlerChain(Map<String, ModelServerEndpoint> map) {
        endpointMap = map;
    }
    public HttpRequestHandlerChain setNextHandler(HttpRequestHandlerChain nextHandler) {
        chain = nextHandler;
        return chain;
    }
    protected abstract void handleRequest(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelNotFoundException, ModelException;
    private void run(
            ModelServerEndpoint endpoint,
            FullHttpRequest req,
            FullHttpResponse rsp,
            QueryStringDecoder decoder,
            String method)
            throws IOException {
        switch (method) {
            case "GET":
                endpoint.doGet(
                        new ModelServerRequest(req, decoder),
                        new ModelServerResponse(rsp),
                        new ModelServerContext());
                break;
            case "PUT":
                endpoint.doPut(
                        new ModelServerRequest(req, decoder),
                        new ModelServerResponse(rsp),
                        new ModelServerContext());
                break;
            case "DELETE":
                endpoint.doDelete(
                        new ModelServerRequest(req, decoder),
                        new ModelServerResponse(rsp),
                        new ModelServerContext());
                break;
            case "POST":
                endpoint.doPost(
                        new ModelServerRequest(req, decoder),
                        new ModelServerResponse(rsp),
                        new ModelServerContext());
                break;
            default:
                throw new ServiceUnavailableException("Invalid HTTP method received");
        }
    }
    protected void handleCustomEndpoint(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            String[] segments,
            QueryStringDecoder decoder) {
        ModelServerEndpoint endpoint = endpointMap.get(segments[1]);
        Runnable r =
                () -> {
                    Long start = System.currentTimeMillis();
                    FullHttpResponse rsp =
                            new DefaultFullHttpResponse(
                                    HttpVersion.HTTP_1_1,
                                    HttpResponseStatus.OK,
                                    Unpooled.directBuffer(),
                                    DefaultHttpHeadersFactory.headersFactory(),
                                    DefaultHttpHeadersFactory.trailersFactory());
                    try {
                        run(endpoint, req, rsp, decoder, req.method().toString());
                        NettyUtils.sendHttpResponse(ctx, rsp, true);
                        logger.info(
                                "Running \"{}\" endpoint took {} ms",
                                segments[0],
                                System.currentTimeMillis() - start);
                    } catch (ModelServerEndpointException me) {
                        NettyUtils.sendError(ctx, HttpResponseStatus.INTERNAL_SERVER_ERROR, me);
                        logger.error("Error thrown by the model endpoint plugin.", me);
                    } catch (OutOfMemoryError oom) {
                        NettyUtils.sendError(
                                ctx, HttpResponseStatus.INSUFFICIENT_STORAGE, oom, "Out of memory");
                    } catch (IOException ioe) {
                        NettyUtils.sendError(
                                ctx,
                                HttpResponseStatus.INTERNAL_SERVER_ERROR,
                                ioe,
                                "I/O error while running the custom endpoint");
                        logger.error("I/O error while running the custom endpoint.", ioe);
                    } catch (Throwable e) {
                        NettyUtils.sendError(
                                ctx,
                                HttpResponseStatus.INTERNAL_SERVER_ERROR,
                                e,
                                "Unknown exception");
                        logger.error("Unknown exception", e);
                    }
                };
        ModelManager.getInstance().submitTask(r);
    }
}
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.ModelException;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.QueryStringDecoder;
public class InvalidRequestHandler extends HttpRequestHandlerChain {
    public InvalidRequestHandler() {}
    @Override
    protected void handleRequest(
            ChannelHandlerContext ctx,
            FullHttpRequest req,
            QueryStringDecoder decoder,
            String[] segments)
            throws ModelException {
        throw new ResourceNotFoundException();
    }
}

frontend/server/src/main/java/com/amazonaws/ml/mms/http/HttpRequestHandler.java
	
com.amazonaws.ml.mms.http.HttpRequestHandler
	
com.amazonaws.ml.mms.util.NettyUtils, com.amazonaws.ml.mms.util.NettyUtils
	
10

/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.http;
import com.amazonaws.ml.mms.archive.ModelException;
import com.amazonaws.ml.mms.archive.ModelNotFoundException;
import com.amazonaws.ml.mms.util.NettyUtils;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.SimpleChannelInboundHandler;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.HttpResponseStatus;
import io.netty.handler.codec.http.QueryStringDecoder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
/**
 * A class handling inbound HTTP requests.
 *
 * <p>This class
 */
public class HttpRequestHandler extends SimpleChannelInboundHandler<FullHttpRequest> {
    private static final Logger logger = LoggerFactory.getLogger(HttpRequestHandler.class);
    HttpRequestHandlerChain handlerChain;
    /** Creates a new {@code HttpRequestHandler} instance. */
    public HttpRequestHandler() {}
    public HttpRequestHandler(HttpRequestHandlerChain chain) {
        handlerChain = chain;
    }
    /** {@inheritDoc} */
    @Override
    protected void channelRead0(ChannelHandlerContext ctx, FullHttpRequest req) {
        try {
            NettyUtils.requestReceived(ctx.channel(), req);
            if (!req.decoderResult().isSuccess()) {
                throw new BadRequestException("Invalid HTTP message.");
            }
            QueryStringDecoder decoder = new QueryStringDecoder(req.uri());
            String path = decoder.path();
            String[] segments = path.split("/");
            handlerChain.handleRequest(ctx, req, decoder, segments);
        } catch (ResourceNotFoundException | ModelNotFoundException e) {
            logger.trace("", e);
            NettyUtils.sendError(ctx, HttpResponseStatus.NOT_FOUND, e);
        } catch (BadRequestException | ModelException e) {
            logger.trace("", e);
            NettyUtils.sendError(ctx, HttpResponseStatus.BAD_REQUEST, e);
        } catch (ConflictStatusException e) {
            logger.trace("", e);
            NettyUtils.sendError(ctx, HttpResponseStatus.CONFLICT, e);
        } catch (RequestTimeoutException e) {
            logger.trace("", e);
            NettyUtils.sendError(ctx, HttpResponseStatus.REQUEST_TIMEOUT, e);
        } catch (MethodNotAllowedException e) {
            logger.trace("", e);
            NettyUtils.sendError(ctx, HttpResponseStatus.METHOD_NOT_ALLOWED, e);
        } catch (ServiceUnavailableException e) {
            logger.trace("", e);
            NettyUtils.sendError(ctx, HttpResponseStatus.SERVICE_UNAVAILABLE, e);
        } catch (OutOfMemoryError e) {
            logger.trace("", e);
            NettyUtils.sendError(ctx, HttpResponseStatus.INSUFFICIENT_STORAGE, e);
        } catch (Throwable t) {
            logger.error("", t);
            NettyUtils.sendError(ctx, HttpResponseStatus.INTERNAL_SERVER_ERROR, t);
        }
    }
    /** {@inheritDoc} */
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        logger.error("", cause);
        if (cause instanceof OutOfMemoryError) {
            NettyUtils.sendError(ctx, HttpResponseStatus.INSUFFICIENT_STORAGE, cause);
        }
        ctx.close();
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.util;
import com.amazonaws.ml.mms.http.ErrorResponse;
import com.amazonaws.ml.mms.http.Session;
import com.amazonaws.ml.mms.metrics.Dimension;
import com.amazonaws.ml.mms.metrics.Metric;
import com.amazonaws.ml.mms.util.messages.InputParameter;
import io.netty.buffer.ByteBuf;
import io.netty.buffer.Unpooled;
import io.netty.channel.Channel;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelFutureListener;
import io.netty.channel.ChannelHandlerContext;
import io.netty.handler.codec.http.DefaultFullHttpResponse;
import io.netty.handler.codec.http.DefaultHttpHeadersFactory;
import io.netty.handler.codec.http.FullHttpResponse;
import io.netty.handler.codec.http.HttpHeaderNames;
import io.netty.handler.codec.http.HttpHeaderValues;
import io.netty.handler.codec.http.HttpHeaders;
import io.netty.handler.codec.http.HttpRequest;
import io.netty.handler.codec.http.HttpResponseStatus;
import io.netty.handler.codec.http.HttpUtil;
import io.netty.handler.codec.http.HttpVersion;
import io.netty.handler.codec.http.QueryStringDecoder;
import io.netty.handler.codec.http.multipart.Attribute;
import io.netty.handler.codec.http.multipart.FileUpload;
import io.netty.handler.codec.http.multipart.InterfaceHttpData;
import io.netty.util.AttributeKey;
import io.netty.util.CharsetUtil;
import java.io.IOException;
import java.net.SocketAddress;
import java.util.List;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
/** A utility class that handling Netty request and response. */
public final class NettyUtils {
    private static final Logger logger = LoggerFactory.getLogger("ACCESS_LOG");
    private static final String REQUEST_ID = "x-request-id";
    private static final AttributeKey<Session> SESSION_KEY = AttributeKey.valueOf("session");
    private static final Dimension DIMENSION = new Dimension("Level", "Host");
    private static final Metric REQUESTS_2_XX =
            new Metric(
                    "Requests2XX",
                    "1",
                    "Count",
                    ConfigManager.getInstance().getHostName(),
                    DIMENSION);
    private static final Metric REQUESTS_4_XX =
            new Metric(
                    "Requests4XX",
                    "1",
                    "Count",
                    ConfigManager.getInstance().getHostName(),
                    DIMENSION);
    private static final Metric REQUESTS_5_XX =
            new Metric(
                    "Requests5XX",
                    "1",
                    "Count",
                    ConfigManager.getInstance().getHostName(),
                    DIMENSION);
    private static final Logger loggerMmsMetrics =
            LoggerFactory.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
    private NettyUtils() {}
    public static void requestReceived(Channel channel, HttpRequest request) {
        Session session = channel.attr(SESSION_KEY).get();
        assert session == null;
        SocketAddress address = channel.remoteAddress();
        String remoteIp;
        if (address == null) {
            // This is can be null on UDS, or on certain case in Windows
            remoteIp = "0.0.0.0";
        } else {
            remoteIp = address.toString();
        }
        channel.attr(SESSION_KEY).set(new Session(remoteIp, request));
    }
    public static String getRequestId(Channel channel) {
        Session accessLog = channel.attr(SESSION_KEY).get();
        if (accessLog != null) {
            return accessLog.getRequestId();
        }
        return null;
    }
    public static void sendJsonResponse(ChannelHandlerContext ctx, Object json) {
        sendJsonResponse(ctx, JsonUtils.GSON_PRETTY.toJson(json), HttpResponseStatus.OK);
    }
    public static void sendJsonResponse(
            ChannelHandlerContext ctx, Object json, HttpResponseStatus status) {
        sendJsonResponse(ctx, JsonUtils.GSON_PRETTY.toJson(json), status);
    }
    public static void sendJsonResponse(ChannelHandlerContext ctx, String json) {
        sendJsonResponse(ctx, json, HttpResponseStatus.OK);
    }
    public static void sendJsonResponse(
            ChannelHandlerContext ctx, String json, HttpResponseStatus status) {
        FullHttpResponse resp =
                new DefaultFullHttpResponse(
                        HttpVersion.HTTP_1_1,
                        status,
                        Unpooled.directBuffer(),
                        DefaultHttpHeadersFactory.headersFactory(),
                        DefaultHttpHeadersFactory.trailersFactory());
        resp.headers().set(HttpHeaderNames.CONTENT_TYPE, HttpHeaderValues.APPLICATION_JSON);
        ByteBuf content = resp.content();
        content.writeCharSequence(json, CharsetUtil.UTF_8);
        content.writeByte('\n');
        sendHttpResponse(ctx, resp, true);
    }
    public static void sendError(
            ChannelHandlerContext ctx, HttpResponseStatus status, Throwable t) {
        ErrorResponse error =
                new ErrorResponse(status.code(), t.getClass().getSimpleName(), t.getMessage());
        sendJsonResponse(ctx, error, status);
    }
    public static void sendError(
            ChannelHandlerContext ctx, HttpResponseStatus status, Throwable t, String msg) {
        ErrorResponse error = new ErrorResponse(status.code(), t.getClass().getSimpleName(), msg);
        sendJsonResponse(ctx, error, status);
    }
    /**
     * Send HTTP response to client.
     *
     * @param ctx ChannelHandlerContext
     * @param resp HttpResponse to send
     * @param keepAlive if keep the connection
     */
    public static void sendHttpResponse(
            ChannelHandlerContext ctx, FullHttpResponse resp, boolean keepAlive) {
        // Send the response and close the connection if necessary.
        Channel channel = ctx.channel();
        Session session = channel.attr(SESSION_KEY).getAndSet(null);
        HttpHeaders headers = resp.headers();
        ConfigManager configManager = ConfigManager.getInstance();
        if (session != null) {
            // session might be recycled if channel is closed already.
            session.setCode(resp.status().code());
            headers.set(REQUEST_ID, session.getRequestId());
            logger.info(session.toString());
        }
        int code = resp.status().code();
        if (code >= 200 && code < 300) {
            loggerMmsMetrics.info("{}", REQUESTS_2_XX);
        } else if (code >= 400 && code < 500) {
            loggerMmsMetrics.info("{}", REQUESTS_4_XX);
        } else {
            loggerMmsMetrics.info("{}", REQUESTS_5_XX);
        }
        String allowedOrigin = configManager.getCorsAllowedOrigin();
        String allowedMethods = configManager.getCorsAllowedMethods();
        String allowedHeaders = configManager.getCorsAllowedHeaders();
        if (allowedOrigin != null
                && !allowedOrigin.isEmpty()
                && !headers.contains(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN)) {
            headers.set(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN, allowedOrigin);
        }
        if (allowedMethods != null
                && !allowedMethods.isEmpty()
                && !headers.contains(HttpHeaderNames.ACCESS_CONTROL_ALLOW_METHODS)) {
            headers.set(HttpHeaderNames.ACCESS_CONTROL_ALLOW_METHODS, allowedMethods);
        }
        if (allowedHeaders != null
                && !allowedHeaders.isEmpty()
                && !headers.contains(HttpHeaderNames.ACCESS_CONTROL_ALLOW_HEADERS)) {
            headers.set(HttpHeaderNames.ACCESS_CONTROL_ALLOW_HEADERS, allowedHeaders);
        }
        // Add cache-control headers to avoid browser cache response
        headers.set("Pragma", "no-cache");
        headers.set("Cache-Control", "no-cache; no-store, must-revalidate, private");
        headers.set("Expires", "Thu, 01 Jan 1970 00:00:00 UTC");
        HttpUtil.setContentLength(resp, resp.content().readableBytes());
        if (!keepAlive || code >= 400) {
            headers.set(HttpHeaderNames.CONNECTION, HttpHeaderValues.CLOSE);
            ChannelFuture f = channel.writeAndFlush(resp);
            f.addListener(ChannelFutureListener.CLOSE);
        } else {
            headers.set(HttpHeaderNames.CONNECTION, HttpHeaderValues.KEEP_ALIVE);
            channel.writeAndFlush(resp);
        }
    }
    /** Closes the specified channel after all queued write requests are flushed. */
    public static void closeOnFlush(Channel ch) {
        if (ch.isActive()) {
            ch.writeAndFlush(Unpooled.EMPTY_BUFFER).addListener(ChannelFutureListener.CLOSE);
        }
    }
    public static byte[] getBytes(ByteBuf buf) {
        if (buf.hasArray()) {
            return buf.array();
        }
        byte[] ret = new byte[buf.readableBytes()];
        int readerIndex = buf.readerIndex();
        buf.getBytes(readerIndex, ret);
        return ret;
    }
    public static String getParameter(QueryStringDecoder decoder, String key, String def) {
        List<String> param = decoder.parameters().get(key);
        if (param != null && !param.isEmpty()) {
            return param.get(0);
        }
        return def;
    }
    public static int getIntParameter(QueryStringDecoder decoder, String key, int def) {
        String value = getParameter(decoder, key, null);
        if (value == null) {
            return def;
        }
        try {
            return Integer.parseInt(value);
        } catch (NumberFormatException e) {
            return def;
        }
    }
    public static InputParameter getFormData(InterfaceHttpData data) {
        if (data == null) {
            return null;
        }
        String name = data.getName();
        switch (data.getHttpDataType()) {
            case Attribute:
                Attribute attribute = (Attribute) data;
                try {
                    return new InputParameter(name, attribute.getValue());
                } catch (IOException e) {
                    throw new AssertionError(e);
                }
            case FileUpload:
                FileUpload fileUpload = (FileUpload) data;
                String contentType = fileUpload.getContentType();
                try {
                    return new InputParameter(name, getBytes(fileUpload.getByteBuf()), contentType);
                } catch (IOException e) {
                    throw new AssertionError(e);
                }
            default:
                throw new IllegalArgumentException(
                        "Except form field, but got " + data.getHttpDataType());
        }
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import io.netty.handler.codec.http.FullHttpRequest;
import io.netty.handler.codec.http.HttpUtil;
import io.netty.handler.codec.http.QueryStringDecoder;
import java.io.ByteArrayInputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import software.amazon.ai.mms.servingsdk.http.Request;
public class ModelServerRequest implements Request {
    private FullHttpRequest req;
    private QueryStringDecoder decoder;
    public ModelServerRequest(FullHttpRequest r, QueryStringDecoder d) {
        req = r;
        decoder = d;
    }
    @Override
    public List<String> getHeaderNames() {
        return new ArrayList<>(req.headers().names());
    }
    @Override
    public String getRequestURI() {
        return req.uri();
    }
    @Override
    public Map<String, List<String>> getParameterMap() {
        return decoder.parameters();
    }
    @Override
    public List<String> getParameter(String k) {
        return decoder.parameters().get(k);
    }
    @Override
    public String getContentType() {
        return HttpUtil.getMimeType(req).toString();
    }
    @Override
    public ByteArrayInputStream getInputStream() {
        return new ByteArrayInputStream(req.content().array());
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import io.netty.buffer.ByteBufOutputStream;
import io.netty.handler.codec.http.FullHttpResponse;
import io.netty.handler.codec.http.HttpHeaderNames;
import io.netty.handler.codec.http.HttpResponseStatus;
import java.io.OutputStream;
import software.amazon.ai.mms.servingsdk.http.Response;
public class ModelServerResponse implements Response {
    private FullHttpResponse response;
    public ModelServerResponse(FullHttpResponse rsp) {
        response = rsp;
    }
    @Override
    public void setStatus(int i) {
        response.setStatus(HttpResponseStatus.valueOf(i));
    }
    @Override
    public void setStatus(int i, String s) {
        response.setStatus(HttpResponseStatus.valueOf(i, s));
    }
    @Override
    public void setHeader(String k, String v) {
        response.headers().set(k, v);
    }
    @Override
    public void addHeader(String k, String v) {
        response.headers().add(k, v);
    }
    @Override
    public void setContentType(String contentType) {
        response.headers().set(HttpHeaderNames.CONTENT_TYPE, contentType);
    }
    @Override
    public OutputStream getOutputStream() {
        return new ByteBufOutputStream(response.content());
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.wlm.ModelManager;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import software.amazon.ai.mms.servingsdk.Context;
import software.amazon.ai.mms.servingsdk.Model;
public class ModelServerContext implements Context {
    @Override
    public Properties getConfig() {
        return ConfigManager.getInstance().getConfiguration();
    }
    @Override
    public Map<String, Model> getModels() {
        HashMap<String, Model> r = new HashMap<>();
        ModelManager.getInstance().getModels().forEach((k, v) -> r.put(k, new ModelServerModel(v)));
        return r;
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import com.amazonaws.ml.mms.wlm.WorkerState;
import com.amazonaws.ml.mms.wlm.WorkerThread;
import software.amazon.ai.mms.servingsdk.Worker;
public class ModelWorker implements Worker {
    boolean running;
    long memory;
    public ModelWorker(WorkerThread t) {
        running = t.getState() == WorkerState.WORKER_MODEL_LOADED;
        memory = t.getMemory();
    }
    @Override
    public boolean isRunning() {
        return running;
    }
    @Override
    public long getWorkerMemory() {
        return memory;
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import com.amazonaws.ml.mms.wlm.ModelManager;
import java.util.ArrayList;
import java.util.List;
import software.amazon.ai.mms.servingsdk.Model;
import software.amazon.ai.mms.servingsdk.Worker;
public class ModelServerModel implements Model {
    private final com.amazonaws.ml.mms.wlm.Model model;
    public ModelServerModel(com.amazonaws.ml.mms.wlm.Model m) {
        model = m;
    }
    @Override
    public String getModelName() {
        return model.getModelName();
    }
    @Override
    public String getModelUrl() {
        return model.getModelUrl();
    }
    @Override
    public String getModelHandler() {
        return model.getModelArchive().getHandler();
    }
    @Override
    public List<Worker> getModelWorkers() {
        ArrayList<Worker> list = new ArrayList<>();
        ModelManager.getInstance()
                .getWorkers(model.getModelName())
                .forEach(r -> list.add(new ModelWorker(r)));
        return list;
    }
}
/*
 * Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.servingsdk.impl;
import com.amazonaws.ml.mms.http.InvalidPluginException;
import java.lang.annotation.Annotation;
import java.util.HashMap;
import java.util.Map;
import java.util.ServiceLoader;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
import software.amazon.ai.mms.servingsdk.annotations.Endpoint;
import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
public final class PluginsManager {
    private static final PluginsManager INSTANCE = new PluginsManager();
    private Logger logger = LoggerFactory.getLogger(PluginsManager.class);
    private Map<String, ModelServerEndpoint> inferenceEndpoints;
    private Map<String, ModelServerEndpoint> managementEndpoints;
    private PluginsManager() {}
    public static PluginsManager getInstance() {
        return INSTANCE;
    }
    public void initialize() {
        inferenceEndpoints = initInferenceEndpoints();
        managementEndpoints = initManagementEndpoints();
    }
    private boolean validateEndpointPlugin(Annotation a, EndpointTypes type) {
        return a instanceof Endpoint
                && !((Endpoint) a).urlPattern().isEmpty()
                && ((Endpoint) a).endpointType().equals(type);
    }
    private HashMap<String, ModelServerEndpoint> getEndpoints(EndpointTypes type)
            throws InvalidPluginException {
        ServiceLoader<ModelServerEndpoint> loader = ServiceLoader.load(ModelServerEndpoint.class);
        HashMap<String, ModelServerEndpoint> ep = new HashMap<>();
        for (ModelServerEndpoint mep : loader) {
            Class<? extends ModelServerEndpoint> modelServerEndpointClassObj = mep.getClass();
            Annotation[] annotations = modelServerEndpointClassObj.getAnnotations();
            for (Annotation a : annotations) {
                if (validateEndpointPlugin(a, type)) {
                    if (ep.get(((Endpoint) a).urlPattern()) != null) {
                        throw new InvalidPluginException(
                                "Multiple plugins found for endpoint "
                                        + "\""
                                        + ((Endpoint) a).urlPattern()
                                        + "\"");
                    }
                    logger.info("Loading plugin for endpoint {}", ((Endpoint) a).urlPattern());
                    ep.put(((Endpoint) a).urlPattern(), mep);
                }
            }
        }
        return ep;
    }
    private HashMap<String, ModelServerEndpoint> initInferenceEndpoints() {
        return getEndpoints(EndpointTypes.INFERENCE);
    }
    private HashMap<String, ModelServerEndpoint> initManagementEndpoints() {
        return getEndpoints(EndpointTypes.MANAGEMENT);
    }
    public Map<String, ModelServerEndpoint> getInferenceEndpoints() {
        return inferenceEndpoints;
    }
    public Map<String, ModelServerEndpoint> getManagementEndpoints() {
        return managementEndpoints;
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.metrics;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.wlm.ModelManager;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.TimeUnit;
public final class MetricManager {
    private static final MetricManager METRIC_MANAGER = new MetricManager();
    private List<Metric> metrics;
    private MetricManager() {
        metrics = Collections.emptyList();
    }
    public static MetricManager getInstance() {
        return METRIC_MANAGER;
    }
    public static void scheduleMetrics(ConfigManager configManager) {
        MetricCollector metricCollector = new MetricCollector(configManager);
        ModelManager.getInstance()
                .getScheduler()
                .scheduleAtFixedRate(
                        metricCollector,
                        0,
                        configManager.getMetricTimeInterval(),
                        TimeUnit.SECONDS);
    }
    public synchronized List<Metric> getMetrics() {
        return metrics;
    }
    public synchronized void setMetrics(List<Metric> metrics) {
        this.metrics = metrics;
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.metrics;
import com.amazonaws.ml.mms.util.ConfigManager;
import com.amazonaws.ml.mms.wlm.ModelManager;
import com.amazonaws.ml.mms.wlm.WorkerThread;
import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import org.apache.commons.io.IOUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public class MetricCollector implements Runnable {
    static final Logger logger = LoggerFactory.getLogger(MetricCollector.class);
    private static final Logger loggerMetrics =
            LoggerFactory.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
    private ConfigManager configManager;
    public MetricCollector(ConfigManager configManager) {
        this.configManager = configManager;
    }
    @Override
    public void run() {
        try {
            // Collect System level Metrics
            String[] args = new String[2];
            args[0] = configManager.getPythonExecutable();
            args[1] = "mms/metrics/metric_collector.py";
            File workingDir = new File(configManager.getModelServerHome());
            String pythonPath = System.getenv("PYTHONPATH");
            String pythonEnv;
            if ((pythonPath == null || pythonPath.isEmpty())
                    && (!workingDir.getAbsolutePath().contains("site-package"))) {
                pythonEnv = "PYTHONPATH=" + workingDir.getAbsolutePath();
            } else {
                pythonEnv = "PYTHONPATH=" + pythonPath;
                if (!workingDir.getAbsolutePath().contains("site-package")) {
                    pythonEnv += File.pathSeparatorChar + workingDir.getAbsolutePath(); // NOPMD
                }
            }
            // sbin added for macs for python sysctl pythonpath
            StringBuilder path = new StringBuilder();
            path.append("PATH=").append(System.getenv("PATH"));
            String osName = System.getProperty("os.name");
            if (osName.startsWith("Mac OS X")) {
                path.append(File.pathSeparatorChar).append("/sbin/");
            }
            String[] env = {pythonEnv, path.toString()};
            final Process p = Runtime.getRuntime().exec(args, env, workingDir);
            ModelManager modelManager = ModelManager.getInstance();
            Map<Integer, WorkerThread> workerMap = modelManager.getWorkers();
            try (OutputStream os = p.getOutputStream()) {
                writeWorkerPids(workerMap, os);
            }
            new Thread(
                            () -> {
                                try {
                                    String error =
                                            IOUtils.toString(
                                                    p.getErrorStream(), StandardCharsets.UTF_8);
                                    if (!error.isEmpty()) {
                                        logger.error(error);
                                    }
                                } catch (IOException e) {
                                    logger.error("", e);
                                }
                            })
                    .start();
            MetricManager metricManager = MetricManager.getInstance();
            try (BufferedReader reader =
                    new BufferedReader(
                            new InputStreamReader(p.getInputStream(), StandardCharsets.UTF_8))) {
                List<Metric> metricsSystem = new ArrayList<>();
                metricManager.setMetrics(metricsSystem);
                String line;
                while ((line = reader.readLine()) != null) {
                    if (line.isEmpty()) {
                        break;
                    }
                    Metric metric = Metric.parse(line);
                    if (metric == null) {
                        logger.warn("Parse metrics failed: " + line);
                    } else {
                        loggerMetrics.info("{}", metric);
                        metricsSystem.add(metric);
                    }
                }
                // Collect process level metrics
                while ((line = reader.readLine()) != null) {
                    String[] tokens = line.split(":");
                    if (tokens.length != 2) {
                        continue;
                    }
                    try {
                        Integer pid = Integer.valueOf(tokens[0]);
                        WorkerThread worker = workerMap.get(pid);
                        worker.setMemory(Long.parseLong(tokens[1]));
                    } catch (NumberFormatException e) {
                        logger.warn("Failed to parse memory utilization metrics: " + line);
                        continue;
                    }
                }
            }
        } catch (IOException e) {
            logger.error("", e);
        }
    }
    private void writeWorkerPids(Map<Integer, WorkerThread> workerMap, OutputStream os)
            throws IOException {
        boolean first = true;
        for (Integer pid : workerMap.keySet()) {
            if (pid < 0) {
                logger.warn("worker pid is not available yet.");
                continue;
            }
            if (first) {
                first = false;
            } else {
                IOUtils.write(",", os, StandardCharsets.UTF_8);
            }
            IOUtils.write(pid.toString(), os, StandardCharsets.UTF_8);
        }
        os.write('\n');
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.util.messages;
import com.amazonaws.ml.mms.wlm.Model;
public class ModelLoadModelRequest extends BaseModelRequest {
    /**
     * ModelLoadModelRequest is a interface between frontend and backend to notify the backend to
     * load a particular model.
     */
    private String modelPath;
    private String handler;
    private int batchSize;
    private int gpuId;
    private String ioFileDescriptor;
    public ModelLoadModelRequest(Model model, int gpuId, String fd) {
        super(WorkerCommands.LOAD, model.getModelName());
        this.gpuId = gpuId;
        modelPath = model.getModelDir().getAbsolutePath();
        handler = model.getModelArchive().getManifest().getModel().getHandler();
        batchSize = model.getBatchSize();
        ioFileDescriptor = fd;
    }
    public String getIoFileDescriptor() {
        return ioFileDescriptor;
    }
    public void setIoFileDescriptor(String ioFileDescriptor) {
        this.ioFileDescriptor = ioFileDescriptor;
    }
    public String getModelPath() {
        return modelPath;
    }
    public String getHandler() {
        return handler;
    }
    public int getBatchSize() {
        return batchSize;
    }
    public int getGpuId() {
        return gpuId;
    }
}
/*
 * Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 * with the License. A copy of the License is located at
 *
 * http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES
 * OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */
package com.amazonaws.ml.mms.openapi;
import com.amazonaws.ml.mms.archive.Manifest;
import com.amazonaws.ml.mms.util.ConnectorType;
import com.amazonaws.ml.mms.util.JsonUtils;
import com.amazonaws.ml.mms.wlm.Model;
import io.netty.handler.codec.http.HttpHeaderValues;
import java.util.ArrayList;
import java.util.List;
public final class OpenApiUtils {
    private OpenApiUtils() {}
    public static String listApis(ConnectorType type) {
        OpenApi openApi = new OpenApi();
        Info info = new Info();
        info.setTitle("Model Server APIs");
        info.setDescription(
                "Model Server is a flexible and easy to use tool for serving deep learning models");
        info.setVersion("1.0.0");
        openApi.setInfo(info);
        if (ConnectorType.BOTH.equals(type) || ConnectorType.INFERENCE_CONNECTOR.equals(type)) {
            listInferenceApis(openApi);
        }
        if (ConnectorType.BOTH.equals(type) || ConnectorType.MANAGEMENT_CONNECTOR.equals(type)) {
            listManagementApis(openApi);
        }
        return JsonUtils.GSON_PRETTY.toJson(openApi);
    }
    static void listInferenceApis(OpenApi openApi) {
        openApi.addPath("/", getApiDescriptionPath(false));
        openApi.addPath("/{model_name}/predict", getLegacyPredictPath());
        openApi.addPath("/ping", getPingPath());
        openApi.addPath("/predictions/{model_name}", getPredictionsPath());
        openApi.addPath("/api-description", getApiDescriptionPath(true));
        openApi.addPath("/invocations", getInvocationsPath());
        openApi.addPath("/models/{model_name}/invoke", getInvocationsPath());
    }
    static void listManagementApis(OpenApi openApi) {
        openApi.addPath("/", getApiDescriptionPath(false));
        openApi.addPath("/models", getModelsPath());
        openApi.addPath("/models/{model_name}", getModelManagerPath());
    }
    public static String getModelApi(Model model) {
        String modelName = model.getModelName();
        OpenApi openApi = new OpenApi();
        Info info = new Info();
        info.setTitle("RESTful API for: " + modelName);
        info.setVersion("1.0.0");
        openApi.setInfo(info);
        openApi.addPath("/prediction/" + modelName, getModelPath(modelName));
        return JsonUtils.GSON_PRETTY.toJson(openApi);
    }
    private static Path getApiDescriptionPath(boolean legacy) {
        Schema schema = new Schema("object");
        schema.addProperty("openapi", new Schema("string"), true);
        schema.addProperty("info", new Schema("object"), true);
        schema.addProperty("paths", new Schema("object"), true);
        MediaType mediaType = new MediaType(HttpHeaderValues.APPLICATION_JSON.toString(), schema);
        Operation operation = new Operation("apiDescription");
        operation.addResponse(new Response("200", "A openapi 3.0.1 descriptor", mediaType));
        operation.addResponse(new Response("500", "Internal Server Error", getErrorResponse()));
        Path path = new Path();
        if (legacy) {
            operation.setDeprecated(true);
            path.setGet(operation);
        } else {
            path.setOptions(operation);
        }
        return path;
    }
    private static Path getPingPath() {
        Schema schema = new Schema("object");
        schema.addProperty(
                "status", new Schema("string", "Overall status of the Model Server."), true);
        MediaType mediaType = new MediaType(HttpHeaderValues.APPLICATION_JSON.toString(), schema);
        Operation operation = new Operation("ping");
        operation.addResponse(new Response("200", "Model server status", mediaType));
        operation.addResponse(new Response("500", "Internal Server Error", getErrorResponse()));
        Path path = new Path();
        path.setGet(operation);
        return path;
    }
    private static Path getInvocationsPath() {
        Schema schema = new Schema();
        schema.addProperty("model_name", new Schema("string", "Name of model"), false);
        Schema dataProp = new Schema("string", "Inference input data");
        dataProp.setFormat("binary");
        schema.addProperty("data", dataProp, true);
        MediaType multipart =
                new MediaType(HttpHeaderValues.MULTIPART_FORM_DATA.toString(), schema);
        RequestBody requestBody = new RequestBody();
        requestBody.setRequired(true);
        requestBody.addContent(multipart);
        Operation operation =
                new Operation("invocations", "A generic invocation entry point for all models.");
        operation.setRequestBody(requestBody);
        operation.addParameter(new QueryParameter("model_name", "Name of model"));
        MediaType error = getErrorResponse();
        MediaType mediaType = new MediaType("*/*", schema);
        operation.addResponse(new Response("200", "Model specific output data format", mediaType));
        operation.addResponse(new Response("400", "Missing model_name parameter", error));
        operation.addResponse(new Response("404", "Model not found", error));
        operation.addResponse(new Response("500", "Internal Server Error", error));
        operation.addResponse(
                new Response("503", "No worker is available to serve request", error));
        Path path = new Path();
        path.setPost(operation);
        return path;
    }
    private static Path getPredictionsPath() {
        Operation post =
                new Operation(
                        "predictions",
                        "Predictions entry point for each model."
                                + " Use OPTIONS method to get detailed model API input and output description.");
        post.addParameter(new PathParameter("model_name", "Name of model."));
        Schema schema = new Schema("string");
        schema.setFormat("binary");
        MediaType mediaType = new MediaType("*/*", schema);
        RequestBody requestBody = new RequestBody();
        requestBody.setDescription(
                "Input data format is defined by each model. Use OPTIONS method to get details for model input format.");
        requestBody.setRequired(true);
        requestBody.addContent(mediaType);
        post.setRequestBody(requestBody);
        schema = new Schema("string");
        schema.setFormat("binary");
        mediaType = new MediaType("*/*", schema);
        Response resp =
                new Response(
                        "200",
                        "Output data format is defined by each model. Use OPTIONS method to get details for model output and output format.",
                        mediaType);
        post.addResponse(resp);
        MediaType error = getErrorResponse();
        post.addResponse(new Response("404", "Model not found", error));
        post.addResponse(new Response("500", "Internal Server Error", error));
        post.addResponse(new Response("503", "No worker is available to serve request", error));
        Operation options =
                new Operation("predictionsApi", "Display details of per model input and output.");
        options.addParameter(new PathParameter("model_name", "Name of model."));
        mediaType = new MediaType("application/json", new Schema("object"));
        options.addResponse(new Response("200", "OK", mediaType));
        post.addResponse(new Response("500", "Internal Server Error", error));
        Path path = new Path();
        path.setPost(post);
        path.setOptions(options);
        return path;
    }
    private static Path getLegacyPredictPath() {
        Operation operation =
                new Operation("predict", "A legacy predict entry point for each model.");
        operation.addParameter(new PathParameter("model_name", "Name of model to unregister."));
        Schema schema = new Schema("string");
        schema.setFormat("binary");
        MediaType mediaType = new MediaType("*/*", schema);
        RequestBody requestBody = new RequestBody();
        requestBody.setRequired(true);
        requestBody.setDescription("Input data format is defined by each model.");
        requestBody.addContent(mediaType);
        operation.setRequestBody(requestBody);
        schema = new Schema("string");
        schema.setFormat("binary");
        mediaType = new MediaType("*/*", schema);
        MediaType error = getErrorResponse();
        operation.addResponse(new Response("200", "Model specific output data format", mediaType));
        operation.addResponse(new Response("404", "Model not found", error));
        operation.addResponse(new Response("500", "Internal Server Error", error));
        operation.addResponse(
                new Response("503", "No worker is available to serve request", error));
        operation.setDeprecated(true);
        Path path = new Path();
        path.setPost(operation);
        return path;
    }
    private static Path getModelsPath() {
        Path path = new Path();
        path.setGet(getListModelsOperation());
        path.setPost(getRegisterOperation());
        return path;
    }
    private static Path getModelManagerPath() {
        Path path = new Path();
        path.setGet(getDescribeModelOperation());
        path.setPut(getScaleOperation());
        path.setDelete(getUnRegisterOperation());
        return path;
    }
    private static Operation getListModelsOperation() {
        Operation operation =
                new Operation("listModels", "List registered models in Model Server.");
        operation.addParameter(
                new QueryParameter(
                        "limit",
                        "integer",
                        "100",
                        "Use this parameter to specify the maximum number of items to return. When"
                                + " this value is present, Model Server does not return more than the specified"
                                + " number of items, but it might return fewer. This value is optional. If you"
                                + " include a value, it must be between 1 and 1000, inclusive. If you do not"
                                + " include a value, it defaults to 100."));
        operation.addParameter(
                new QueryParameter(
                        "next_page_token",
                        "The token to retrieve the next set of results. Model Server provides the"
                                + " token when the response from a previous call has more results than the"
                                + " maximum page size."));
        operation.addParameter(
                new QueryParameter(
                        "model_name_pattern", "A model name filter to list only matching models."));
        Schema schema = new Schema("object");
        schema.addProperty(
                "nextPageToken",
                new Schema(
                        "string",
                        "Use this parameter in a subsequent request after you receive a response"
                                + " with truncated results. Set it to the value of NextMarker from the"
                                + " truncated response you just received."),
                false);
        Schema modelProp = new Schema("object");
        modelProp.addProperty("modelName", new Schema("string", "Name of the model."), true);
        modelProp.addProperty("modelUrl", new Schema("string", "URL of the model."), true);
        Schema modelsProp = new Schema("array", "A list of registered models.");
        modelsProp.setItems(modelProp);
        schema.addProperty("models", modelsProp, true);
        MediaType json = new MediaType(HttpHeaderValues.APPLICATION_JSON.toString(), schema);
        operation.addResponse(new Response("200", "OK", json));
        operation.addResponse(new Response("500", "Internal Server Error", getErrorResponse()));
        return operation;
    }
    private static Operation getRegisterOperation() {
        Operation operation =
                new Operation("registerModel", "Register a new model in Model Server.");
        operation.addParameter(
                new QueryParameter(
                        "model_url",
                        "string",
                        null,
                        true,
                        "Model archive download url, support local file or HTTP(s) protocol."
                                + " For S3, consider use pre-signed url."));
        operation.addParameter(
                new QueryParameter(
                        "model_name",
                        "Name of model. This value will override modelName in MANIFEST.json if present."));
        operation.addParameter(
                new QueryParameter(
                        "handler",
                        "Inference handler entry-point. This value will override handler in MANIFEST.json if present."));
        Parameter runtime =
                new QueryParameter(
                        "runtime",
                        "Runtime for the model custom service code. This value will override runtime in MANIFEST.json if present.");
        operation.addParameter(runtime);
        operation.addParameter(
                new QueryParameter(
                        "batch_size", "integer", "1", "Inference batch size, default: 1."));
        operation.addParameter(
                new QueryParameter(
                        "max_batch_delay",
                        "integer",
                        "100",
                        "Maximum delay for batch aggregation, default: 100."));
        operation.addParameter(
                new QueryParameter(
                        "response_timeout",
                        "integer",
                        "2",
                        "Maximum time, in seconds, the Model Server waits for a response from the model inference code, default: 120."));
        operation.addParameter(
                new QueryParameter(
                        "initial_workers",
                        "integer",
                        "0",
                        "Number of initial workers, default: 0."));
        operation.addParameter(
                new QueryParameter(
                        "synchronous",
                        "boolean",
                        "false",
                        "Decides whether creation of worker synchronous or not, default: false."));
        operation.addParameter(
                new QueryParameter(
                        "preload_model",
                        "boolean",
                        "false",
                        "Decides if model should be preloaded, default: false."));
        Manifest.RuntimeType[] types = Manifest.RuntimeType.values();
        List<String> runtimeTypes = new ArrayList<>(types.length);
        for (Manifest.RuntimeType type : types) {
            runtimeTypes.add(type.toString());
        }
        runtime.getSchema().setEnumeration(runtimeTypes);
        MediaType status = getStatusResponse();
        MediaType error = getErrorResponse();
        operation.addResponse(new Response("200", "Model registered", status));
        operation.addResponse(new Response("202", "Accepted", status));
        operation.addResponse(new Response("210", "Partial Success", status));
        operation.addResponse(new Response("400", "Bad request", error));
        operation.addResponse(new Response("404", "Model not found", error));
        operation.addResponse(new Response("409", "Model already registered", error));
        operation.addResponse(new Response("500", "Internal Server Error", error));
        return operation;
    }
    private static Operation getUnRegisterOperation() {
        Operation operation =
                new Operation(
                        "unregisterModel",
                        "Unregister a model from Model Server. This is an asynchronous call by default."
                                + " Caller can call listModels to confirm if all the works has be terminated.");
        operation.addParameter(new PathParameter("model_name", "Name of model to unregister."));
        operation.addParameter(
                new QueryParameter(
                        "synchronous",
                        "boolean",
                        "false",
                        "Decides whether the call is synchronous or not, default: false."));
        operation.addParameter(
                new QueryParameter(
                        "timeout",
                        "integer",
                        "-1",
                        "Waiting up to the specified wait time if necessary for"
                                + " a worker to complete all pending requests. Use 0 to terminate backend"
                                + " worker process immediately. Use -1 for wait infinitely."));
        MediaType status = getStatusResponse();
        MediaType error = getErrorResponse();
        operation.addResponse(new Response("200", "Model unregistered", status));
        operation.addResponse(new Response("202", "Accepted", status));
        operation.addResponse(new Response("404", "Model not found", error));
        operation.addResponse(new Response("408", "Request Timeout Error", error));
        operation.addResponse(new Response("500", "Internal Server Error", error));
        return operation;
    }
    private static Operation getDescribeModelOperation() {
        Operation operation =
                new Operation(
                        "describeModel",
                        "Provides detailed information about the specified model.");
        operation.addParameter(new PathParameter("model_name", "Name of model to describe."));
        Schema schema = new Schema("object");
        schema.addProperty("modelName", new Schema("string", "Name of the model."), true);
        schema.addProperty("modelVersion", new Schema("string", "Version of the model."), true);
        schema.addProperty("modelUrl", new Schema("string", "URL of the model."), true);
        schema.addProperty(
                "minWorkers", new Schema("integer", "Configured minimum number of worker."), true);
        schema.addProperty(
                "maxWorkers", new Schema("integer", "Configured maximum number of worker."), true);
        schema.addProperty("batchSize", new Schema("integer", "Configured batch size."), false);
        schema.addProperty(
                "maxBatchDelay",
                new Schema("integer", "Configured maximum batch delay in ms."),
                false);
        schema.addProperty(
                "status", new Schema("string", "Overall health status of the model"), true);
        Schema workers = new Schema("array", "A list of active backend workers.");
        Schema worker = new Schema("object");
        worker.addProperty("id", new Schema("string", "Worker id"), true);
        worker.addProperty("startTime", new Schema("string", "Worker start time"), true);
        worker.addProperty("gpu", new Schema("boolean", "If running on GPU"), false);
        Schema workerStatus = new Schema("string", "Worker status");
        List<String> status = new ArrayList<>();
        status.add("READY");
        status.add("LOADING");
        status.add("UNLOADING");
        workerStatus.setEnumeration(status);
        worker.addProperty("status", workerStatus, true);
        workers.setItems(worker);
        schema.addProperty("workers", workers, true);
        Schema metrics = new Schema("object");
        metrics.addProperty(
                "rejectedRequests",
                new Schema("integer", "Number requests has been rejected in last 10 minutes."),
                true);
        metrics.addProperty(
                "waitingQueueSize",
                new Schema("integer", "Number requests waiting in the queue."),
                true);
        metrics.addProperty(
                "requests",
                new Schema("integer", "Number requests processed in last 10 minutes."),
                true);
        schema.addProperty("metrics", metrics, true);
        MediaType mediaType = new MediaType(HttpHeaderValues.APPLICATION_JSON.toString(), schema);
        operation.addResponse(new Response("200", "OK", mediaType));
        operation.addResponse(new Response("500", "Internal Server Error", getErrorResponse()));
        return operation;
    }
    private static Operation getScaleOperation() {
        Operation operation =
                new Operation(
                        "setAutoScale",
                        "Configure number of workers for a model, This is a asynchronous call by default."
                                + " Caller need to call describeModel check if the model workers has been changed.");
        operation.addParameter(new PathParameter("model_name", "Name of model to describe."));
        operation.addParameter(
                new QueryParameter(
                        "min_worker", "integer", "1", "Minimum number of worker processes."));
        operation.addParameter(
                new QueryParameter(
                        "max_worker", "integer", "1", "Maximum number of worker processes."));
        operation.addParameter(
                new QueryParameter(
                        "number_gpu", "integer", "0", "Number of GPU worker processes to create."));
        operation.addParameter(
                new QueryParameter(
                        "synchronous",
                        "boolean",
                        "false",
                        "Decides whether the call is synchronous or not, default: false."));
        operation.addParameter(
                new QueryParameter(
                        "timeout",
                        "integer",
                        "-1",
                        "Waiting up to the specified wait time if necessary for"
                                + " a worker to complete all pending requests. Use 0 to terminate backend"
                                + " worker process immediately. Use -1 for wait infinitely."));
        MediaType status = getStatusResponse();
        MediaType error = getErrorResponse();
        operation.addResponse(new Response("200", "Model workers updated", status));
        operation.addResponse(new Response("202", "Accepted", status));
        operation.addResponse(new Response("210", "Partial Success", status));
        operation.addResponse(new Response("400", "Bad request", error));
        operation.addResponse(new Response("404", "Model not found", error));
        operation.addResponse(new Response("500", "Internal Server Error", error));
        return operation;
    }
    private static Path getModelPath(String modelName) {
        Operation operation =
                new Operation(modelName, "A predict entry point for model: " + modelName + '.');
        operation.addResponse(new Response("200", "OK"));
        operation.addResponse(new Response("500", "Internal Server Error", getErrorResponse()));
        Path path = new Path();
        path.setPost(operation);
        return path;
    }
    private static MediaType getErrorResponse() {
        Schema schema = new Schema("object");
        schema.addProperty("code", new Schema("integer", "Error code."), true);
        schema.addProperty("type", new Schema("string", "Error type."), true);
        schema.addProperty("message", new Schema("string", "Error message."), true);
        return new MediaType(HttpHeaderValues.APPLICATION_JSON.toString(), schema);
    }
    private static MediaType getStatusResponse() {
        Schema schema = new Schema("object");
        schema.addProperty("status", new Schema("string", "Error type."), true);
        return new MediaType(HttpHeaderValues.APPLICATION_JSON.toString(), schema);
    }
}