package io.cassandrareaper.service;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.CompactionStats;
import io.cassandrareaper.core.DroppedMessages;
import io.cassandrareaper.core.GenericMetric;
import io.cassandrareaper.core.MetricsHistogram;
import io.cassandrareaper.core.Node;
import io.cassandrareaper.core.PercentRepairedMetric;
import io.cassandrareaper.core.RepairSchedule;
import io.cassandrareaper.core.RepairUnit;
import io.cassandrareaper.core.StreamSession;
import io.cassandrareaper.core.ThreadPoolStat;
import io.cassandrareaper.management.ClusterFacade;
import io.cassandrareaper.storage.IDistributedStorage;
import io.cassandrareaper.storage.OpType;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import javax.management.JMException;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.datatype.jdk8.Jdk8Module;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import com.google.common.base.Supplier;
import com.google.common.collect.ImmutableSet;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public final class MetricsService {
  private static final Logger LOG = LoggerFactory.getLogger(MetricsService.class);
  private final AppContext context;
  private final ClusterFacade clusterFacade;
  private final ObjectMapper objectMapper;
  private final String localClusterName;
  private final RepairUnitService repairUnitService;
  private MetricsService(AppContext context, Supplier<ClusterFacade> clusterFacadeSupplier) throws ReaperException {
    this.context = context;
    this.clusterFacade = clusterFacadeSupplier.get();
    if (Boolean.TRUE.equals(context.config.isInSidecarMode())) {
      Node host = Node.builder()
          .withHostname(context.config.getEnforcedLocalNode().orElse("127.0.0.1"))
          .build();
      localClusterName = Cluster.toSymbolicName(clusterFacade.getClusterName(host));
    } else {
      localClusterName = null;
    }
    objectMapper = new ObjectMapper();
    objectMapper.registerModule(new Jdk8Module());
    this.repairUnitService = RepairUnitService.create(context);
  }
  @VisibleForTesting
  static MetricsService create(AppContext context, Supplier<ClusterFacade> supplier) throws ReaperException {
    return new MetricsService(context, supplier);
  }
  public static MetricsService create(AppContext context) throws ReaperException {
    return new MetricsService(context, () -> ClusterFacade.create(context));
  }
  public List<ThreadPoolStat> getTpStats(Node host) throws ReaperException {
    return clusterFacade.getTpStats(host);
  }
  public List<DroppedMessages> getDroppedMessages(Node host) throws ReaperException {
    return clusterFacade.getDroppedMessages(host);
  }
  public List<MetricsHistogram> getClientRequestLatencies(Node host) throws ReaperException {
    return clusterFacade.getClientRequestLatencies(host);
  }
  void grabAndStoreGenericMetrics(Optional<Node> maybeNode) throws ReaperException, InterruptedException, JMException {
    Preconditions.checkState(
        context.config.getDatacenterAvailability().isInCollocatedMode(),
        "grabAndStoreGenericMetrics() can only be called in collocated mode");
    Node node = getNode(maybeNode);
    List<GenericMetric> metrics = ClusterFacade.create(context).collectGenericMetrics(node);
    ((IDistributedStorage) context.storage).storeMetrics(metrics);
    LOG.debug("Grabbing and storing metrics for {}", node.getHostname());
  }
  void grabAndStoreCompactionStats(Optional<Node> maybeNode)
      throws JsonProcessingException, JMException, ReaperException {
    Preconditions.checkState(
        context.config.getDatacenterAvailability().isInCollocatedMode(),
        "grabAndStoreCompactionStats() can only be called in sidecar");
    Node node = getNode(maybeNode);
    CompactionStats compactionStats = ClusterFacade.create(context).listCompactionStatsDirect(node);
    ((IDistributedStorage) context.storage).getOperationsDao()
        .storeOperations(
            node.getClusterName(),
            OpType.OP_COMPACTION,
            node.getHostname(),
            objectMapper.writeValueAsString(compactionStats));
    LOG.debug("Grabbing and storing compaction stats for {}", node.getHostname());
  }
  void grabAndStoreActiveStreams(Optional<Node> maybeNode) throws JsonProcessingException, ReaperException {
    Preconditions.checkState(
        context.config.getDatacenterAvailability().isInCollocatedMode(),
        "grabAndStoreActiveStreams() can only be called in sidecar");
    Node node = getNode(maybeNode);
    List<StreamSession> activeStreams = ClusterFacade.create(context).listStreamsDirect(node);
    ((IDistributedStorage) context.storage).getOperationsDao()
        .storeOperations(
            node.getClusterName(),
            OpType.OP_STREAMING,
            node.getHostname(),
            objectMapper.writeValueAsString(activeStreams));
    LOG.debug("Grabbing and storing streams for {}", node.getHostname());
  }
  private Node getNode(Optional<Node> maybeNode) {
    return maybeNode.orElseGet(() ->
        Node.builder()
            .withHostname(context.getLocalNodeAddress())
            .withCluster(
                Cluster.builder()
                    .withName(localClusterName)
                    .withSeedHosts(ImmutableSet.of(context.getLocalNodeAddress()))
                    .build())
            .build());
  }
  public void grabAndStorePercentRepairedMetrics(Optional<Node> maybeNode, RepairSchedule sched)
      throws ReaperException {
    Node node = getNode(maybeNode);
    RepairUnit repairUnit = context.storage.getRepairUnitDao().getRepairUnit(sched.getRepairUnitId());
    Set<String> tables = this.repairUnitService.getTablesToRepair(node.getCluster().get(), repairUnit);
    List<GenericMetric> metrics = ClusterFacade.create(context).collectPercentRepairedMetrics(node,
            repairUnit.getKeyspaceName());
    LOG.debug("Grabbed the following percent repaired metrics: {}", metrics);
    Optional<GenericMetric> percentRepairedForSchedule = metrics.stream()
        .filter(metric -> tables.contains(metric.getMetricScope()))
        .sorted((metric1, metric2) -> Double.valueOf(metric1.getValue()).compareTo(Double.valueOf(metric2.getValue())))
        .reduce((metric1, metric2) -> metric1);
    if (percentRepairedForSchedule.isPresent()) {
      context.storage.storePercentRepairedMetric(
          PercentRepairedMetric.builder()
              .withCluster(repairUnit.getClusterName())
              .withKeyspaceName(repairUnit.getKeyspaceName())
              .withTableName(percentRepairedForSchedule.get().getMetricScope())
              .withNode(node.getHostname())
              .withRepairScheduleId(sched.getId())
              .withPercentRepaired((int) percentRepairedForSchedule.get().getValue())
              .build());
    }
  }
}
package io.cassandrareaper;
import io.cassandrareaper.management.IManagementConnectionFactory;
import io.cassandrareaper.service.RepairManager;
import io.cassandrareaper.service.SchedulingManager;
import io.cassandrareaper.storage.IStorageDao;
import java.net.InetAddress;
import java.net.UnknownHostException;
import java.util.UUID;
import java.util.concurrent.atomic.AtomicBoolean;
import com.codahale.metrics.MetricRegistry;
import com.datastax.driver.core.utils.UUIDs;
import com.google.common.base.Preconditions;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public final class AppContext {
  public static final String REAPER_INSTANCE_ADDRESS = Private.initialiseInstanceAddress();
  public final UUID reaperInstanceId = UUIDs.timeBased();
  public final AtomicBoolean isRunning = new AtomicBoolean(true);
  public final AtomicBoolean isDistributed = new AtomicBoolean(false);
  public IStorageDao storage;
  public RepairManager repairManager;
  public SchedulingManager schedulingManager;
  public IManagementConnectionFactory managementConnectionFactory;
  public ReaperApplicationConfiguration config;
  public MetricRegistry metricRegistry = new MetricRegistry();
  volatile String localNodeAddress = null;
  public String getLocalNodeAddress() {
    Preconditions.checkState(config.isInSidecarMode());
    return localNodeAddress;
  }
  private static final class Private {
    private static final Logger LOG = LoggerFactory.getLogger(AppContext.class);
    private static final String DEFAULT_INSTANCE_ADDRESS = "127.0.0.1";
    private static String initialiseInstanceAddress() {
      try {
        return InetAddress.getLocalHost().getHostAddress();
      } catch (UnknownHostException e) {
        LOG.warn("Cannot get instance address", e);
      }
      return DEFAULT_INSTANCE_ADDRESS;
    }
  }
}
package io.cassandrareaper.storage;
import io.cassandrareaper.storage.cluster.IClusterDao;
import io.cassandrareaper.storage.events.IEventsDao;
import io.cassandrareaper.storage.metrics.IMetricsDao;
import io.cassandrareaper.storage.repairrun.IRepairRunDao;
import io.cassandrareaper.storage.repairschedule.IRepairScheduleDao;
import io.cassandrareaper.storage.repairsegment.IRepairSegmentDao;
import io.cassandrareaper.storage.repairunit.IRepairUnitDao;
import io.cassandrareaper.storage.snapshot.ISnapshotDao;
import java.util.Set;
import java.util.UUID;
import io.dropwizard.lifecycle.Managed;
public interface IStorageDao extends Managed,
    IMetricsDao {
  boolean lockRunningRepairsForNodes(
      UUID repairId,
      UUID segmentId,
      Set<String> replicas);
  boolean renewRunningRepairsForNodes(
      UUID repairId,
      UUID segmentId,
      Set<String> replicas);
  boolean releaseRunningRepairsForNodes(
      UUID repairId,
      UUID segmentId,
      Set<String> replicas);
  Set<UUID> getLockedSegmentsForRun(UUID runId);
  boolean isStorageConnected();
  IEventsDao getEventsDao();
  ISnapshotDao getSnapshotDao();
  IRepairRunDao getRepairRunDao();
  IRepairSegmentDao getRepairSegmentDao();
  IRepairUnitDao getRepairUnitDao();
  IRepairScheduleDao getRepairScheduleDao();
  IClusterDao getClusterDao();
}
package io.cassandrareaper.core;
import java.util.Objects;
import java.util.Optional;
import com.google.common.base.Preconditions;
public final class Node {
  private final Optional<Cluster> cluster;
  private final String hostname;
  private Node(Builder builder) {
    this.cluster = Optional.ofNullable(builder.cluster);
    this.hostname = builder.hostname;
  }
  public static Builder builder() {
    return new Builder();
  }
  public Builder with() {
    Builder builder = builder().withHostname(hostname);
    if (cluster.isPresent()) {
      builder = builder.withCluster(cluster.get());
    }
    return builder;
  }
  public String getClusterName() {
    return cluster.isPresent() ? cluster.get().getName() : "";
  }
  public String getHostname() {
    return hostname;
  }
  public Optional<Cluster> getCluster() {
    return this.cluster;
  }
  public int getJmxPort() {
    return cluster.isPresent() ? cluster.get().getJmxPort() : Cluster.DEFAULT_JMX_PORT;
  }
  public Optional<JmxCredentials> getJmxCredentials() {
    return cluster.isPresent() ? cluster.get().getJmxCredentials() : Optional.empty();
  }
  public String toString() {
    return hostname + (cluster.isPresent() ? "@" + cluster.get().getName() : "");
  }
  @Override
  public boolean equals(Object obj) {
    if (this == obj) {
      return true;
    }
    if (obj == null || getClass() != obj.getClass()) {
      return false;
    }
    Node node = (Node) obj;
    return Objects.equals(cluster, node.cluster) && Objects.equals(hostname, node.hostname);
  }
  @Override
  public int hashCode() {
    return Objects.hash(cluster, hostname);
  }
  public static final class Builder {
    private Cluster cluster;
    private String hostname;
    private Builder() {}
    public Builder withCluster(Cluster cluster) {
      Preconditions.checkNotNull(cluster);
      this.cluster = cluster;
      return this;
    }
    public Builder withHostname(String hostname) {
      Preconditions.checkNotNull(hostname);
      this.hostname = hostname;
      return this;
    }
    public Node build() {
      Preconditions.checkNotNull(hostname);
      return new Node(this);
    }
  }
}
package io.cassandrareaper;
import io.cassandrareaper.core.JmxCredentials;
import io.cassandrareaper.crypto.CryptographFactory;
import java.time.Duration;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;
import javax.validation.Valid;
import javax.validation.constraints.DecimalMin;
import javax.validation.constraints.Max;
import javax.validation.constraints.NotNull;
import javax.ws.rs.DefaultValue;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.google.common.annotations.VisibleForTesting;
import io.dropwizard.Configuration;
import io.dropwizard.client.HttpClientConfiguration;
import org.apache.cassandra.repair.RepairParallelism;
import org.hibernate.validator.constraints.NotEmpty;
import org.secnod.dropwizard.shiro.ShiroConfiguration;
import systems.composable.dropwizard.cassandra.CassandraFactory;
import systems.composable.dropwizard.cassandra.network.AddressTranslatorFactory;
public final class ReaperApplicationConfiguration extends Configuration {
  public static final int DEFAULT_MGMT_API_METRICS_PORT = 9000;
  private static final int DEFAULT_MGMT_API_PORT = 8080;
  private static final int DEFAULT_SEGMENT_COUNT_PER_NODE = 64;
  private static final Integer DEFAULT_MAX_PENDING_COMPACTIONS = 20;
  @JsonProperty
  private Integer maxPendingCompactions;
  @Deprecated
  @JsonProperty
  private Integer segmentCount;
  @JsonProperty
  private Integer segmentCountPerNode;
  @JsonProperty
  @NotNull
  private RepairParallelism repairParallelism;
  @JsonProperty
  @NotNull
  @DecimalMin(value = "0", inclusive = false)
  @Max(1)
  private Double repairIntensity;
  @JsonProperty
  @NotNull
  @DefaultValue("false")
  private Boolean incrementalRepair;
  @JsonProperty
  @DefaultValue("false")
  private Boolean subrangeIncrementalRepair;
  @JsonProperty
  private Boolean blacklistTwcsTables;
  @DefaultValue("7")
  private Integer scheduleDaysBetween;
  @JsonProperty
  @DefaultValue("false")
  private Boolean useAddressTranslator;
  @Valid
  private Optional<AddressTranslatorFactory> jmxAddressTranslator = Optional.empty();
  @JsonProperty
  @NotNull
  private Integer repairRunThreadCount;
  @JsonProperty
  @Nullable
  private Integer maxParallelRepairs;
  @JsonProperty
  @NotNull
  private Integer hangingRepairTimeoutMins;
  @NotEmpty
  private String storageType;
  private String enableCrossOrigin;
  @JsonProperty
  private Map<String, Integer> jmxPorts;
  @JsonProperty
  private Jmxmp jmxmp = new Jmxmp();
  @JsonProperty
  private Map<String, JmxCredentials> jmxCredentials;
  @JsonProperty
  private JmxCredentials jmxAuth;
  @JsonProperty
  private HttpManagement httpManagement = new HttpManagement();
  @JsonProperty
  private AutoSchedulingConfiguration autoScheduling;
  @JsonProperty
  @DefaultValue("true")
  private Boolean enableDynamicSeedList;
  @JsonProperty
  private Integer repairManagerSchedulingIntervalSeconds;
  @JsonProperty
  @DefaultValue("false")
  private Boolean activateQueryLogger;
  @JsonProperty
  @DefaultValue("5")
  private Integer jmxConnectionTimeoutInSeconds;
  @JsonProperty
  @DefaultValue("7")
  private Integer clusterTimeoutInDays;
  @JsonProperty
  private DatacenterAvailability datacenterAvailability;
  @JsonProperty
  private AccessControlConfiguration accessControl;
  @JsonProperty
  private Integer repairThreadCount;
  @Nullable
  @JsonProperty
  private Integer purgeRecordsAfterInDays;
  @Nullable
  @JsonProperty
  private Integer numberOfRunsToKeepPerUnit;
  private CassandraFactory cassandra = new CassandraFactory();
  @JsonProperty
  private Optional<String> enforcedLocalNode = Optional.empty();
  @JsonProperty
  private Optional<String> enforcedLocalClusterName = Optional.empty();
  @JsonProperty
  private Optional<String> enforcedLocalDatacenter = Optional.empty();
  @JsonProperty
  @DefaultValue("true")
  private Boolean enableConcurrentMigrations;
  @JsonProperty
  private Integer percentRepairedCheckIntervalMinutes;
  private HttpClientConfiguration httpClient = new HttpClientConfiguration();
  @JsonProperty
  @Nullable
  private CryptographFactory cryptograph;
  @JsonProperty
  @Nullable
  private String persistenceStoragePath;
  @JsonProperty
  private Boolean scheduleRetryOnError;
  @JsonProperty
  private Duration scheduleRetryDelay;
  public HttpManagement getHttpManagement() {
    return httpManagement;
  }
  public void setHttpManagement(HttpManagement httpManagement) {
    this.httpManagement = httpManagement;
  }
  public Jmxmp getJmxmp() {
    return jmxmp;
  }
  public void setJmxmp(Jmxmp jmxmp) {
    this.jmxmp = jmxmp;
  }
  public int getSegmentCount() {
    return segmentCount == null ? 0 : segmentCount;
  }
  public void setSegmentCount(int segmentCount) {
    this.segmentCount = segmentCount;
  }
  public int getSegmentCountPerNode() {
    return segmentCountPerNode == null ? DEFAULT_SEGMENT_COUNT_PER_NODE : segmentCountPerNode;
  }
  public void setSegmentCountPerNode(int segmentCountPerNode) {
    this.segmentCountPerNode = segmentCountPerNode;
  }
  public int getMaxPendingCompactions() {
    return maxPendingCompactions == null ? DEFAULT_MAX_PENDING_COMPACTIONS : maxPendingCompactions;
  }
  public void setMaxPendingCompactions(int maxPendingCompactions) {
    this.maxPendingCompactions = maxPendingCompactions;
  }
  public RepairParallelism getRepairParallelism() {
    return repairParallelism;
  }
  public void setRepairParallelism(RepairParallelism repairParallelism) {
    this.repairParallelism = repairParallelism;
  }
  public double getRepairIntensity() {
    return repairIntensity;
  }
  public void setRepairIntensity(double repairIntensity) {
    this.repairIntensity = repairIntensity;
  }
  public boolean getIncrementalRepair() {
    return incrementalRepair != null ? incrementalRepair : false;
  }
  public void setIncrementalRepair(boolean incrementalRepair) {
    this.incrementalRepair = incrementalRepair;
  }
  public boolean getSubrangeIncrementalRepair() {
    return subrangeIncrementalRepair != null ? subrangeIncrementalRepair : false;
  }
  public void setSubrangeIncrementalRepair(boolean subrangeIncrementalRepair) {
    this.subrangeIncrementalRepair = subrangeIncrementalRepair;
  }
  public boolean getBlacklistTwcsTables() {
    return blacklistTwcsTables != null ? blacklistTwcsTables : false;
  }
  public void setBlacklistTwcsTables(boolean blacklistTwcsTables) {
    this.blacklistTwcsTables = blacklistTwcsTables;
  }
  public Integer getScheduleDaysBetween() {
    return scheduleDaysBetween != null ? scheduleDaysBetween : 7;
  }
  public void setScheduleDaysBetween(int scheduleDaysBetween) {
    this.scheduleDaysBetween = scheduleDaysBetween;
  }
  public int getRepairRunThreadCount() {
    return repairRunThreadCount;
  }
  public void setRepairRunThreadCount(int repairRunThreadCount) {
    this.repairRunThreadCount = repairRunThreadCount;
  }
  public int getMaxParallelRepairs() {
    return maxParallelRepairs == null
        ? 2
        : maxParallelRepairs;
  }
  public void setMaxParallelRepairs(int maxParallelRepairs) {
    this.maxParallelRepairs = maxParallelRepairs;
  }
  public String getStorageType() {
    return storageType;
  }
  public void setStorageType(String storageType) {
    this.storageType = storageType;
  }
  public String getEnableCrossOrigin() {
    return this.enableCrossOrigin;
  }
  public boolean isEnableCrossOrigin() {
    return this.enableCrossOrigin != null && "true".equalsIgnoreCase(this.enableCrossOrigin);
  }
  public void setEnableCrossOrigin(String enableCrossOrigin) {
    this.enableCrossOrigin = enableCrossOrigin;
  }
  public int getRepairManagerSchedulingIntervalSeconds() {
    return this.repairManagerSchedulingIntervalSeconds == null ? 30 : this.repairManagerSchedulingIntervalSeconds;
  }
  @JsonProperty
  public void setRepairManagerSchedulingIntervalSeconds(int repairManagerSchedulingIntervalSeconds) {
    this.repairManagerSchedulingIntervalSeconds = repairManagerSchedulingIntervalSeconds;
  }
  public Map<String, Integer> getJmxPorts() {
    return jmxPorts;
  }
  public void setJmxPorts(Map<String, Integer> jmxPorts) {
    this.jmxPorts = jmxPorts;
  }
  public JmxCredentials getJmxAuth() {
    return jmxAuth;
  }
  public void setJmxAuth(JmxCredentials jmxAuth) {
    this.jmxAuth = jmxAuth;
  }
  public Map<String, JmxCredentials> getJmxCredentials() {
    return jmxCredentials;
  }
  public void setJmxCredentials(Map<String, JmxCredentials> jmxCredentials) {
    this.jmxCredentials = jmxCredentials;
  }
  public boolean hasAutoSchedulingEnabled() {
    return autoScheduling != null && autoScheduling.isEnabled();
  }
  public AutoSchedulingConfiguration getAutoScheduling() {
    return autoScheduling;
  }
  public void setAutoScheduling(AutoSchedulingConfiguration autoRepairScheduling) {
    this.autoScheduling = autoRepairScheduling;
  }
  public boolean getEnableDynamicSeedList() {
    return this.enableDynamicSeedList == null ? true : this.enableDynamicSeedList;
  }
  public void setEnableDynamicSeedList(boolean enableDynamicSeedList) {
    this.enableDynamicSeedList = enableDynamicSeedList;
  }
  public boolean getActivateQueryLogger() {
    return this.activateQueryLogger == null ? false : this.activateQueryLogger;
  }
  public void setActivateQueryLogger(boolean activateQueryLogger) {
    this.activateQueryLogger = activateQueryLogger;
  }
  public void setUseAddressTranslator(boolean useAddressTranslator) {
    this.useAddressTranslator = useAddressTranslator;
  }
  public boolean useAddressTranslator() {
    return this.useAddressTranslator != null ? useAddressTranslator : false;
  }
  @JsonProperty("cassandra")
  public CassandraFactory getCassandraFactory() {
    return cassandra;
  }
  @JsonProperty("cassandra")
  public void setCassandraFactory(CassandraFactory cassandra) {
    this.cassandra = cassandra;
  }
  public int getHangingRepairTimeoutMins() {
    return hangingRepairTimeoutMins;
  }
  @JsonProperty
  public void setHangingRepairTimeoutMins(int hangingRepairTimeoutMins) {
    this.hangingRepairTimeoutMins = hangingRepairTimeoutMins;
  }
  public int getJmxConnectionTimeoutInSeconds() {
    return jmxConnectionTimeoutInSeconds != null ? jmxConnectionTimeoutInSeconds : 20;
  }
  @JsonProperty
  public void setJmxConnectionTimeoutInSeconds(int jmxConnectionTimeoutInSeconds) {
    this.jmxConnectionTimeoutInSeconds = jmxConnectionTimeoutInSeconds;
  }
  public int getClusterTimeoutInDays() {
    return clusterTimeoutInDays != null ? clusterTimeoutInDays : 7;
  }
  @JsonProperty
  public void setClusterTimeoutInDays(int clusterTimeoutInDays) {
    this.clusterTimeoutInDays = clusterTimeoutInDays;
  }
  public DatacenterAvailability getDatacenterAvailability() {
    return this.datacenterAvailability != null ? this.datacenterAvailability : DatacenterAvailability.ALL;
  }
  @JsonProperty("datacenterAvailability")
  public void setDatacenterAvailability(DatacenterAvailability datacenterAvailability) {
    this.datacenterAvailability = datacenterAvailability;
  }
  public AccessControlConfiguration getAccessControl() {
    return accessControl;
  }
  public void setAccessControl(AccessControlConfiguration accessControl) {
    this.accessControl = accessControl;
  }
  public boolean isAccessControlEnabled() {
    return getAccessControl() != null;
  }
  public int getRepairThreadCount() {
    return repairThreadCount != null ? repairThreadCount : 1;
  }
  public Integer getPurgeRecordsAfterInDays() {
    return purgeRecordsAfterInDays == null ? 0 : purgeRecordsAfterInDays;
  }
  @JsonProperty("purgeRecordsAfterInDays")
  public void setPurgeRecordsAfterInDays(Integer purgeRecordsAfterInDays) {
    this.purgeRecordsAfterInDays = purgeRecordsAfterInDays;
  }
  public Integer getPercentRepairedCheckIntervalMinutes() {
    return percentRepairedCheckIntervalMinutes == null ? 30 : percentRepairedCheckIntervalMinutes;
  }
  @JsonProperty("percentRepairedCheckIntervalMinutes")
  public void setPercentRepairedCheckIntervalMinutes(Integer percentRepairedCheckIntervalMinutes) {
    this.percentRepairedCheckIntervalMinutes = percentRepairedCheckIntervalMinutes;
  }
  public Boolean isInSidecarMode() {
    return datacenterAvailability == DatacenterAvailability.SIDECAR;
  }
  public Optional<String> getEnforcedLocalNode() {
    return enforcedLocalNode;
  }
  public void setEnforcedLocalNode(Optional<String> enforcedLocalNode) {
    this.enforcedLocalNode = enforcedLocalNode;
  }
  public Optional<String> getEnforcedLocalClusterName() {
    return enforcedLocalClusterName;
  }
  public void setEnforcedLocalClusterName(Optional<String> enforcedLocalClusterName) {
    this.enforcedLocalClusterName = enforcedLocalClusterName;
  }
  public Optional<String> getEnforcedLocalDatacenter() {
    return enforcedLocalDatacenter;
  }
  public void setEnforcedLocalDatacenter(Optional<String> enforcedLocalDatacenter) {
    this.enforcedLocalDatacenter = enforcedLocalDatacenter;
  }
  public boolean getEnableConcurrentMigrations() {
    return this.enableConcurrentMigrations == null ? true : this.enableConcurrentMigrations;
  }
  public void setEnableConcurrentMigrations(boolean enableConcurrentMigrations) {
    this.enableConcurrentMigrations = enableConcurrentMigrations;
  }
  public Integer getNumberOfRunsToKeepPerUnit() {
    return numberOfRunsToKeepPerUnit == null ? 50 : numberOfRunsToKeepPerUnit;
  }
  @JsonProperty("numberOfRunsToKeepPerUnit")
  public void setNumberOfRunsToKeepPerUnit(Integer numberOfRunsToKeepPerUnit) {
    this.numberOfRunsToKeepPerUnit = numberOfRunsToKeepPerUnit;
  }
  public HttpClientConfiguration getHttpClientConfiguration() {
    return httpClient;
  }
  public void setHttpClientConfiguration(HttpClientConfiguration httpClient) {
    this.httpClient = httpClient;
  }
  @JsonProperty
  public Optional<AddressTranslatorFactory> getJmxAddressTranslator() {
    return jmxAddressTranslator;
  }
  @JsonProperty
  public void setJmxAddressTranslator(Optional<AddressTranslatorFactory> jmxAddressTranslator) {
    this.jmxAddressTranslator = jmxAddressTranslator;
  }
  @Nullable
  public CryptographFactory getCryptograph() {
    return cryptograph;
  }
  public void setCryptograph(@Nullable CryptographFactory cryptograph) {
    this.cryptograph = cryptograph;
  }
  public void setPersistenceStoragePath(@Nullable String persistenceStoragePath) {
    this.persistenceStoragePath = persistenceStoragePath;
  }
  @Nullable
  public String getPersistenceStoragePath() {
    return persistenceStoragePath;
  }
  public Boolean isScheduleRetryOnError() {
    return scheduleRetryOnError != null ? scheduleRetryOnError : false;
  }
  public void setScheduleRetryOnError(Boolean scheduleRetryOnError) {
    this.scheduleRetryOnError = scheduleRetryOnError;
  }
  public Duration getScheduleRetryDelay() {
    return scheduleRetryDelay != null ? scheduleRetryDelay : Duration.ofMinutes(60);
  }
  public void setScheduleRetryDelay(Duration scheduleRetryDelay) {
    this.scheduleRetryDelay = scheduleRetryDelay;
  }
  public enum DatacenterAvailability {
    ALL,
    LOCAL,
    EACH,
    SIDECAR;
    public boolean isInCollocatedMode() {
      switch (this) {
        case LOCAL:
        case SIDECAR:
        case EACH:
          return true;
        default:
          return false;
      }
    }
  }
  public static final class AutoSchedulingConfiguration {
    @JsonProperty
    private Boolean enabled;
    @JsonProperty
    private Duration initialDelayPeriod;
    @JsonProperty
    private Duration periodBetweenPolls;
    @JsonProperty
    private Duration timeBeforeFirstSchedule;
    @JsonProperty
    private Duration scheduleSpreadPeriod;
    @JsonProperty
    private List<String> excludedKeyspaces = Collections.emptyList();
    @JsonProperty
    private List<String> excludedClusters = Collections.emptyList();
    @JsonProperty
    private Boolean adaptive;
    @JsonProperty
    private Boolean incremental;
    @JsonProperty
    private Boolean subrangeIncrementalRepair;
    @JsonProperty
    private Integer percentUnrepairedThreshold;
    public Boolean isEnabled() {
      return enabled;
    }
    public void setEnabled(Boolean enable) {
      this.enabled = enable;
    }
    public Duration getInitialDelayPeriod() {
      return initialDelayPeriod;
    }
    public void setInitialDelayPeriod(Duration initialDelayPeriod) {
      this.initialDelayPeriod = initialDelayPeriod;
    }
    public Duration getPeriodBetweenPolls() {
      return periodBetweenPolls;
    }
    public void setPeriodBetweenPolls(Duration periodBetweenPolls) {
      this.periodBetweenPolls = periodBetweenPolls;
    }
    public Duration getTimeBeforeFirstSchedule() {
      return timeBeforeFirstSchedule;
    }
    public void setTimeBeforeFirstSchedule(Duration timeBeforeFirstSchedule) {
      this.timeBeforeFirstSchedule = timeBeforeFirstSchedule;
    }
    public Duration getScheduleSpreadPeriod() {
      return scheduleSpreadPeriod;
    }
    public void setScheduleSpreadPeriod(Duration scheduleSpreadPeriod) {
      this.scheduleSpreadPeriod = scheduleSpreadPeriod;
    }
    public boolean hasScheduleSpreadPeriod() {
      return scheduleSpreadPeriod != null;
    }
    public List<String> getExcludedKeyspaces() {
      return excludedKeyspaces;
    }
    public void setExcludedKeyspaces(List<String> excludedKeyspaces) {
      this.excludedKeyspaces = null != excludedKeyspaces ? excludedKeyspaces : Collections.emptyList();
    }
    public List<String> getExcludedClusters() {
      return excludedClusters;
    }
    public void setExcludedClusters(List<String> excludedClusters) {
      this.excludedClusters = null != excludedClusters ? excludedClusters : Collections.emptyList();
    }
    public Boolean isAdaptive() {
      return adaptive == null ? false : adaptive;
    }
    public void setAdaptive(Boolean adaptive) {
      this.adaptive = adaptive;
    }
    public Boolean incremental() {
      return incremental == null ? false : incremental;
    }
    public void setIncremental(Boolean incremental) {
      this.incremental = incremental;
    }
    public Boolean subrangeIncrementalRepair() {
      return subrangeIncrementalRepair == null ? false : subrangeIncrementalRepair;
    }
    public void setSubrangeIncrementalRepair(Boolean subrangeIncrementalRepair) {
      this.subrangeIncrementalRepair = subrangeIncrementalRepair;
    }
    public Integer getPercentUnrepairedThreshold() {
      return percentUnrepairedThreshold == null ? -1 : percentUnrepairedThreshold;
    }
    public void setPercentUnrepairedThreshold(Integer percentUnrepairedThreshold) {
      this.percentUnrepairedThreshold = percentUnrepairedThreshold;
    }
    @Override
    public String toString() {
      return "AutoSchedulingConfiguration{"
          + "enabled="
          + enabled
          + ", initialDelayPeriod="
          + initialDelayPeriod
          + ", periodBetweenPolls="
          + periodBetweenPolls
          + ", timeBeforeFirstSchedule="
          + timeBeforeFirstSchedule
          + ", scheduleSpreadPeriod="
          + scheduleSpreadPeriod
          + ", adaptive="
          + adaptive
          + '}';
    }
  }
  public static final class AccessControlConfiguration {
    @JsonProperty
    private ShiroConfiguration shiro;
    @JsonProperty
    private Duration sessionTimeout;
    public ShiroConfiguration getShiroConfiguration() {
      return shiro;
    }
    public Duration getSessionTimeout() {
      return sessionTimeout != null ? sessionTimeout : Duration.ofMinutes(10);
    }
  }
  public static final class Jmxmp {
    @JsonProperty
    private Boolean ssl = false;
    @JsonProperty
    private Boolean enabled = false;
    public Boolean useSsl() {
      return ssl;
    }
    public Boolean isEnabled() {
      return enabled;
    }
  }
  public static final class HttpManagement {
    @JsonProperty
    private Boolean enabled = false;
    @JsonProperty
    private String keystore;
    @JsonProperty
    private String truststore;
    @JsonProperty
    private String truststoresDir;
    @JsonProperty
    private Integer mgmtApiMetricsPort;
    @JsonProperty
    private Integer managementApiPort;
    public Boolean isEnabled() {
      return enabled;
    }
    public String getKeystore() {
      return keystore;
    }
    public String getTruststore() {
      return truststore;
    }
    public String getTruststoresDir() {
      return truststoresDir;
    }
    @VisibleForTesting
    public void setEnabled(Boolean enabled) {
      this.enabled = enabled;
    }
    @VisibleForTesting
    public void setKeystore(String keystore) {
      this.keystore = keystore;
    }
    @VisibleForTesting
    public void setTruststore(String truststore) {
      this.truststore = truststore;
    }
    @VisibleForTesting
    public void setTruststoresDir(String truststoresDir) {
      this.truststoresDir = truststoresDir;
    }
    public int getMgmtApiMetricsPort() {
      return mgmtApiMetricsPort == null ? DEFAULT_MGMT_API_METRICS_PORT : mgmtApiMetricsPort;
    }
    public void setManagementApiPort(Integer managementApiPort) {
      this.managementApiPort = managementApiPort;
    }
    public Integer getManagementApiPort() {
      return managementApiPort == null ? DEFAULT_MGMT_API_PORT : managementApiPort;
    }
    @JsonProperty("mgmtApiMetricsPort")
    public void setMgmtApiMetricsPort(int mgmtApiMetricsPort) {
      this.mgmtApiMetricsPort = mgmtApiMetricsPort;
    }
  }
}
package io.cassandrareaper.core;
import java.time.LocalDate;
import java.util.Objects;
import java.util.Optional;
import java.util.Set;
import com.google.common.base.Preconditions;
public final class Cluster implements Comparable<Cluster> {
  public enum State {
    UNKNOWN,
    ACTIVE,
    UNREACHABLE
  }
  public static final int DEFAULT_JMX_PORT = 7199;
  private final String name;
  private final Optional<String> partitioner; 
  private final Set<String> seedHosts;
  private final State state;
  private final LocalDate lastContact;
  private final ClusterProperties properties;
  private Cluster(
      String name,
      Optional<String> partitioner,
      Set<String> seedHosts,
      State state,
      LocalDate lastContact,
      ClusterProperties properties) {
    this.name = toSymbolicName(name);
    this.partitioner = partitioner;
    this.seedHosts = seedHosts;
    this.state = state;
    this.lastContact = lastContact;
    this.properties = properties;
  }
  public static Builder builder() {
    return new Builder();
  }
  public Builder with() {
    Builder builder = new Builder()
        .withName(name)
        .withSeedHosts(seedHosts)
        .withState(state)
        .withLastContact(lastContact)
        .withJmxPort(getJmxPort());
    Optional<JmxCredentials> jmxCredentials = getJmxCredentials();
    if (jmxCredentials.isPresent()) {
      builder = builder.withJmxCredentials(jmxCredentials.get());
    }
    if (partitioner.isPresent()) {
      builder = builder.withPartitioner(partitioner.get());
    }
    return builder;
  }
  public static String toSymbolicName(String name) {
    Preconditions.checkNotNull(name, "cannot turn null into symbolic name");
    return name.toLowerCase().replaceAll("[^a-z0-9_\\-\\.]", "");
  }
  public String getName() {
    return name;
  }
  public Optional<String> getPartitioner() {
    return partitioner;
  }
  public Set<String> getSeedHosts() {
    return seedHosts;
  }
  public int getJmxPort() {
    return properties.getJmxPort();
  }
  public Optional<JmxCredentials> getJmxCredentials() {
    return Optional.ofNullable(properties.getJmxCredentials());
  }
  public State getState() {
    return state;
  }
  public LocalDate getLastContact() {
    return lastContact;
  }
  public ClusterProperties getProperties() {
    return properties;
  }
  @Override
  public int compareTo(Cluster other) {
    return this.getState() == other.getState()
        ? this.getName().compareTo(other.getName())
        : this.getState().compareTo(other.getState());
  }
  @Override
  public boolean equals(Object obj) {
    if (this == obj) {
      return true;
    }
    if (obj == null || getClass() != obj.getClass()) {
      return false;
    }
    Cluster cluster = (Cluster) obj;
    return Objects.equals(name, cluster.name);
  }
  @Override
  public int hashCode() {
    return Objects.hash(name);
  }
  public static final class Builder {
    private String name;
    private String partitioner;
    private Set<String> seedHosts;
    private State state = State.UNKNOWN;
    private LocalDate lastContact = LocalDate.MIN;
    private final ClusterProperties.Builder properties = ClusterProperties.builder().withJmxPort(DEFAULT_JMX_PORT);
    private Builder() {
    }
    public Builder withName(String name) {
      Preconditions.checkState(null == this.name);
      this.name = name;
      return this;
    }
    public Builder withPartitioner(String partitioner) {
      Preconditions.checkState(null == this.partitioner);
      this.partitioner = partitioner;
      return this;
    }
    public Builder withSeedHosts(Set<String> seedHosts) {
      Preconditions.checkArgument(!seedHosts.isEmpty());
      this.seedHosts = seedHosts;
      return this;
    }
    public Builder withState(State state) {
      Preconditions.checkNotNull(state);
      this.state = state;
      return this;
    }
    public Builder withLastContact(LocalDate lastContact) {
      Preconditions.checkNotNull(lastContact);
      this.lastContact = lastContact;
      return this;
    }
    public Builder withJmxPort(int jmxPort) {
      this.properties.withJmxPort(jmxPort);
      return this;
    }
    public Builder withJmxCredentials(JmxCredentials jmxCredentials) {
      Preconditions.checkNotNull(jmxCredentials);
      Preconditions.checkNotNull(jmxCredentials.getUsername());
      Preconditions.checkNotNull(jmxCredentials.getPassword());
      this.properties.withJmxCredentials(jmxCredentials);
      return this;
    }
    public Cluster build() {
      Preconditions.checkNotNull(name);
      Preconditions.checkNotNull(seedHosts);
      return new Cluster(name, Optional.ofNullable(partitioner), seedHosts, state, lastContact, properties.build());
    }
  }
}
package io.cassandrareaper.service;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.Node;
import io.cassandrareaper.core.RepairSchedule;
import io.cassandrareaper.core.RepairUnit;
import io.cassandrareaper.core.Table;
import io.cassandrareaper.management.ClusterFacade;
import io.cassandrareaper.management.ICassandraManagementProxy;
import java.util.Collection;
import java.util.Collections;
import java.util.Objects;
import java.util.Optional;
import java.util.Set;
import java.util.stream.Collectors;
import com.datastax.driver.core.VersionNumber;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import com.google.common.base.Supplier;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Sets;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public final class RepairUnitService {
  private static final Logger LOG = LoggerFactory.getLogger(RepairUnitService.class);
  private static final Set<String> BLACKLISTED_STRATEGEIS
      = ImmutableSet.of("TimeWindowCompactionStrategy", "DateTieredCompactionStrategy");
  private final AppContext context;
  private final ClusterFacade clusterFacade;
  private RepairUnitService(AppContext context, Supplier<ClusterFacade> clusterFacadeSupplier) {
    this.context = context;
    this.clusterFacade = clusterFacadeSupplier.get();
  }
  @VisibleForTesting
  static RepairUnitService create(AppContext context, Supplier<ClusterFacade> supplier) throws ReaperException {
    return new RepairUnitService(context, supplier);
  }
  public static RepairUnitService create(AppContext context) {
    return new RepairUnitService(context, () -> ClusterFacade.create(context));
  }
  private static Integer versionCompare(String str1, String str2) {
    VersionNumber version1 = VersionNumber.parse(str1);
    VersionNumber version2 = VersionNumber.parse(str2);
    return version1.compareTo(version2);
  }
  private static boolean isBlackListedCompactionStrategy(Table table) {
    return BLACKLISTED_STRATEGEIS.stream()
        .anyMatch(s -> table.getCompactionStrategy().toLowerCase().contains(s.toLowerCase()));
  }
  private static Set<String> listRepairTables(RepairUnit.Builder builder, Set<String> allTables) {
    Set<String> tables = Sets.newHashSet(builder.columnFamilies.isEmpty() ? allTables : builder.columnFamilies);
    tables.removeAll(builder.blacklistedTables);
    return tables;
  }
  public Optional<RepairUnit> getOrCreateRepairUnit(Cluster cluster, RepairUnit.Builder params) {
    return getOrCreateRepairUnit(cluster, params, false);
  }
  public Optional<RepairUnit> getOrCreateRepairUnit(Cluster cluster, RepairUnit.Builder params, boolean force) {
    if (params.incrementalRepair) {
      try {
        String version = clusterFacade.getCassandraVersion(cluster);
        if (null != version && ICassandraManagementProxy.versionCompare(version, "2.1.0") < 0) {
          throw new IllegalArgumentException("Incremental repair does not work with Cassandra versions before 2.1");
        }
      } catch (ReaperException e) {
        LOG.warn("unknown version to cluster {}, maybe enabling incremental on 2.0...", cluster.getName(), e);
      }
    }
    if (params.subrangeIncrementalRepair) {
      try {
        String version = clusterFacade.getCassandraVersion(cluster);
        if (null != version && ICassandraManagementProxy.versionCompare(version, "4.0.0") < 0) {
          throw new IllegalArgumentException(
              "Subrange incremental repair does not work with Cassandra versions before 4.0.0");
        }
      } catch (ReaperException e) {
        LOG.warn("unknown version to cluster {}, maybe enabling subrange incremental before 4.0...",
            cluster.getName(), e);
      }
    }
    Optional<RepairUnit> repairUnit = context.storage.getRepairUnitDao().getRepairUnit(params);
    if (repairUnit.isPresent()) {
      return repairUnit;
    }
    try {
      return Optional.of(createRepairUnit(cluster, params, force));
    } catch (IllegalArgumentException e) {
      return Optional.empty();
    }
  }
  Set<String> getTablesToRepair(Cluster cluster, RepairUnit repairUnit) throws ReaperException {
    String keyspace = repairUnit.getKeyspaceName();
    Set<String> result;
    if (repairUnit.getColumnFamilies().isEmpty()) {
      Set<Table> tables = clusterFacade.getTablesForKeyspace(cluster, keyspace);
      Set<String> twcsBlacklisted = findBlacklistedCompactionStrategyTables(cluster, tables);
      result = tables.stream()
          .map(Table::getName)
          .filter(tableName -> !repairUnit.getBlacklistedTables().contains(tableName))
          .filter(tableName -> !twcsBlacklisted.contains(tableName))
          .collect(Collectors.toSet());
    } else {
      result = repairUnit.getColumnFamilies().stream()
          .filter(tableName -> !repairUnit.getBlacklistedTables().contains(tableName))
          .collect(Collectors.toSet());
    }
    Preconditions.checkState(
        repairUnit.getBlacklistedTables().isEmpty() || !result.isEmpty(),
        "Invalid blacklist definition. It filtered out all tables in the keyspace.");
    return result;
  }
  public Set<String> findBlacklistedCompactionStrategyTables(Cluster clstr, Set<Table> tables) throws ReaperException {
    if (context.config.getBlacklistTwcsTables()
        && versionCompare(clusterFacade.getCassandraVersion(clstr), "2.1") >= 0) {
      return tables
          .stream()
          .filter(RepairUnitService::isBlackListedCompactionStrategy)
          .map(Table::getName)
          .collect(Collectors.toSet());
    }
    return Collections.emptySet();
  }
  private RepairUnit createRepairUnit(Cluster cluster, RepairUnit.Builder builder, boolean force) {
    Preconditions.checkArgument(
        force || !unitConflicts(cluster, builder),
        "unit conflicts with existing in " + builder.clusterName + ":" + builder.keyspaceName);
    return context.storage.getRepairUnitDao().addRepairUnit(builder);
  }
  @VisibleForTesting
  boolean unitConflicts(Cluster cluster, RepairUnit.Builder builder) {
    Collection<RepairSchedule> repairSchedules = context.storage.getRepairScheduleDao()
        .getRepairSchedulesForClusterAndKeyspace(builder.clusterName, builder.keyspaceName);
    for (RepairSchedule sched : repairSchedules) {
      RepairUnit repairUnitForSched = context.storage.getRepairUnitDao().getRepairUnit(sched.getRepairUnitId());
      Preconditions.checkState(repairUnitForSched.getClusterName().equals(builder.clusterName));
      Preconditions.checkState(repairUnitForSched.getKeyspaceName().equals(builder.keyspaceName));
      if (conflictingUnits(cluster, repairUnitForSched, builder)) {
        return true;
      }
    }
    return false;
  }
  boolean conflictingUnits(Cluster cluster, RepairUnit unit, RepairUnit.Builder builder) {
    if (unit.with().equals(builder)) {
      return true;
    }
    Preconditions.checkState(unit.getKeyspaceName().equals(builder.keyspaceName));
    Set<String> tables = unit.getColumnFamilies().isEmpty() || builder.columnFamilies.isEmpty()
        ? getTableNamesForKeyspace(cluster, unit.getKeyspaceName())
        : Collections.emptySet();
    return !Sets.intersection(listRepairTables(unit.with(), tables), listRepairTables(builder, tables)).isEmpty();
  }
  boolean identicalUnits(Cluster cluster, RepairUnit unit, RepairUnit.Builder builder) {
    if (unit.with().equals(builder)) {
      return true;
    }
    if (unit.getIncrementalRepair() != builder.incrementalRepair.booleanValue()) {
      return false;
    }
    if (unit.getSubrangeIncrementalRepair() != builder.subrangeIncrementalRepair.booleanValue()) {
      return false;
    }
    Preconditions.checkState(unit.getKeyspaceName().equals(builder.keyspaceName));
    Set<String> tables = unit.getColumnFamilies().isEmpty() || builder.columnFamilies.isEmpty()
        ? getTableNamesForKeyspace(cluster, unit.getKeyspaceName())
        : Collections.emptySet();
    if (!Objects.equals(listRepairTables(unit.with(), tables), listRepairTables(builder, tables))) {
      return false;
    }
    Set<String> unitNodes = getRepairUnitNodes(cluster, unit.with());
    Set<String> builderNodes = getRepairUnitNodes(cluster, builder);
    if (!Objects.equals(unitNodes, builderNodes)) {
      return false;
    }
    Set<String> unitDatacenters = getRepairUnitDatacenters(cluster, unit.with(), unitNodes);
    Set<String> builderDatacenters = getRepairUnitDatacenters(cluster, builder, builderNodes);
    if (!Objects.equals(unitDatacenters, builderDatacenters)) {
      return false;
    }
    return true;
  }
  public Set<String> getTableNamesForKeyspace(Cluster cluster, String keyspace) {
    try {
      return clusterFacade
          .getTablesForKeyspace(cluster, keyspace)
          .stream()
          .map(Table::getName)
          .collect(Collectors.toSet());
    } catch (ReaperException e) {
      LOG.warn("unknown table list to cluster {} keyspace", cluster.getName(), keyspace, e);
      return Collections.emptySet();
    }
  }
  private Set<String> getRepairUnitNodes(Cluster cluster, RepairUnit.Builder builder) {
    if (!builder.nodes.isEmpty()) {
      return builder.nodes;
    }
    try {
      return clusterFacade
          .getLiveNodes(cluster)
          .stream()
          .collect(Collectors.toSet());
    } catch (ReaperException e) {
      LOG.warn("Unable to get list of live nodes for cluster {}", cluster.getName());
      return Collections.emptySet();
    }
  }
  private Set<String> getRepairUnitDatacenters(Cluster cluster, RepairUnit.Builder builder, Set<String> nodes) {
    if (!builder.datacenters.isEmpty()) {
      return builder.datacenters;
    }
    Set<String> datacenters = Sets.newHashSet();
    try {
      for (String node : nodes) {
        datacenters.add(clusterFacade.getDatacenter(Node.builder().withHostname(node).build()));
      }
    } catch (ReaperException | InterruptedException e) {
      LOG.warn("Unable to get the list of datacenters for cluster {}", cluster.getName(), e);
    }
    return datacenters;
  }
}
package io.cassandrareaper.core;
import java.util.Collection;
import java.util.List;
import java.util.UUID;
import java.util.stream.Collectors;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import org.apache.cassandra.repair.RepairParallelism;
import org.joda.time.DateTime;
public final class RepairSchedule extends EditableRepairSchedule {
  private final UUID id;
  private final UUID repairUnitId;
  private final State state;
  private final DateTime nextActivation;
  private final ImmutableList<UUID> runHistory;
  private final DateTime creationTime;
  private final DateTime pauseTime;
  private final UUID lastRun;
  private RepairSchedule(Builder builder, UUID id) {
    this.id = id;
    this.repairUnitId = builder.repairUnitId;
    this.state = builder.state;
    this.daysBetween = builder.daysBetween;
    this.nextActivation = builder.nextActivation;
    this.runHistory = builder.runHistory;
    this.repairParallelism = builder.repairParallelism;
    this.intensity = builder.intensity;
    this.creationTime = builder.creationTime;
    this.owner = builder.owner;
    this.pauseTime = builder.pauseTime;
    this.segmentCountPerNode = builder.segmentCountPerNode;
    this.adaptive = builder.adaptive;
    this.percentUnrepairedThreshold = builder.percentUnrepairedThreshold;
    this.lastRun = builder.lastRun;
  }
  public static Builder builder(UUID repairUnitId) {
    return new Builder(repairUnitId);
  }
  public UUID getId() {
    return id;
  }
  public UUID getRepairUnitId() {
    return repairUnitId;
  }
  public State getState() {
    return state;
  }
  public DateTime getFollowingActivation() {
    return getNextActivation().plusDays(getDaysBetween());
  }
  public DateTime getNextActivation() {
    return nextActivation;
  }
  public ImmutableList<UUID> getRunHistory() {
    return runHistory;
  }
  public UUID getLastRun() {
    return lastRun;
  }
  public LongCollectionSqlType getRunHistorySql() {
    List<Long> list = runHistory.stream().map(UUID::getMostSignificantBits).collect(Collectors.toList());
    return new LongCollectionSqlType(list);
  }
  public DateTime getCreationTime() {
    return creationTime;
  }
  public DateTime getPauseTime() {
    return pauseTime;
  }
  public Builder with() {
    return new Builder(this);
  }
  @Override
  public String toString() {
    return String.format("%s[%s]", getClass().getSimpleName(), id.toString());
  }
  public enum State {
    ACTIVE,
    PAUSED,
    DELETED
  }
  public static final class Builder {
    public final UUID repairUnitId;
    private State state = RepairSchedule.State.ACTIVE;
    private Integer daysBetween;
    private DateTime nextActivation;
    private ImmutableList<UUID> runHistory = ImmutableList.<UUID>of();
    private RepairParallelism repairParallelism;
    private Double intensity;
    private DateTime creationTime = DateTime.now();
    private String owner = "";
    private DateTime pauseTime;
    private Integer segmentCountPerNode;
    private boolean adaptive = false;
    private Integer percentUnrepairedThreshold;
    private UUID lastRun;
    private Builder(UUID repairUnitId) {
      this.repairUnitId = repairUnitId;
    }
    private Builder(RepairSchedule original) {
      repairUnitId = original.repairUnitId;
      state = original.state;
      daysBetween = original.daysBetween;
      nextActivation = original.nextActivation;
      runHistory = original.runHistory;
      repairParallelism = original.repairParallelism;
      intensity = original.intensity;
      creationTime = original.creationTime;
      owner = original.owner;
      pauseTime = original.pauseTime;
      segmentCountPerNode = original.segmentCountPerNode;
      adaptive = original.adaptive;
      percentUnrepairedThreshold = original.percentUnrepairedThreshold;
      lastRun = original.lastRun;
    }
    public Builder state(State state) {
      this.state = state;
      return this;
    }
    public Builder daysBetween(int daysBetween) {
      this.daysBetween = daysBetween;
      return this;
    }
    public Builder nextActivation(DateTime nextActivation) {
      this.nextActivation = nextActivation;
      return this;
    }
    public Builder runHistory(ImmutableList<UUID> runHistory) {
      this.runHistory = runHistory;
      return this;
    }
    public Builder repairParallelism(RepairParallelism repairParallelism) {
      this.repairParallelism = repairParallelism;
      return this;
    }
    public Builder intensity(double intensity) {
      this.intensity = intensity;
      return this;
    }
    public Builder creationTime(DateTime creationTime) {
      this.creationTime = creationTime;
      return this;
    }
    public Builder owner(String owner) {
      this.owner = owner;
      return this;
    }
    public Builder pauseTime(DateTime pauseTime) {
      this.pauseTime = pauseTime;
      return this;
    }
    public Builder segmentCountPerNode(int segmentCountPerNode) {
      this.segmentCountPerNode = segmentCountPerNode;
      return this;
    }
    public Builder adaptive(boolean adaptive) {
      this.adaptive = adaptive;
      return this;
    }
    public Builder percentUnrepairedThreshold(Integer percentUnrepairedThreshold) {
      this.percentUnrepairedThreshold = percentUnrepairedThreshold;
      return this;
    }
    public Builder lastRun(UUID lastRun) {
      this.lastRun = lastRun;
      return this;
    }
    public RepairSchedule build(UUID id) {
      Preconditions.checkState(null != daysBetween, "daysBetween(..) must be called before build(..)");
      Preconditions.checkState(null != nextActivation, "nextActivation(..) must be called before build(..)");
      Preconditions.checkState(null != repairParallelism, "repairParallelism(..) must be called before build(..)");
      Preconditions.checkState(null != intensity, "intensity(..) must be called before build(..)");
      Preconditions.checkState(null != segmentCountPerNode, "segmentCountPerNode(..) must be called before build(..)");
      return new RepairSchedule(this, id);
    }
  }
  public static final class LongCollectionSqlType {
    private final Collection<Long> collection;
    public LongCollectionSqlType(Collection<Long> collection) {
      this.collection = collection;
    }
    public Collection<Long> getValue() {
      return null != collection ? collection : Lists.newArrayList();
    }
  }
}
package io.cassandrareaper.management;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.ReaperApplicationConfiguration.DatacenterAvailability;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.Compaction;
import io.cassandrareaper.core.CompactionStats;
import io.cassandrareaper.core.DroppedMessages;
import io.cassandrareaper.core.GenericMetric;
import io.cassandrareaper.core.MetricsHistogram;
import io.cassandrareaper.core.Node;
import io.cassandrareaper.core.Segment;
import io.cassandrareaper.core.Snapshot;
import io.cassandrareaper.core.StreamSession;
import io.cassandrareaper.core.Table;
import io.cassandrareaper.core.ThreadPoolStat;
import io.cassandrareaper.resources.view.NodesStatus;
import io.cassandrareaper.service.RingRange;
import io.cassandrareaper.storage.IDistributedStorage;
import io.cassandrareaper.storage.OpType;
import java.io.IOError;
import java.io.IOException;
import java.math.BigInteger;
import java.time.LocalDate;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Optional;
import java.util.Set;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;
import javax.management.JMException;
import javax.management.MalformedObjectNameException;
import javax.management.ReflectionException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.datatype.jdk8.Jdk8Module;
import com.fasterxml.jackson.datatype.joda.JodaModule;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import com.google.common.cache.Cache;
import com.google.common.cache.CacheBuilder;
import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.google.common.util.concurrent.ExecutionError;
import org.apache.commons.lang3.tuple.Pair;
import org.joda.time.DateTime;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public final class ClusterFacade {
  private static final int METRICS_PARTITIONING_TIME_MINS = 10;
  private static final Logger LOG = LoggerFactory.getLogger(ClusterFacade.class);
  private static final long CLUSTER_VERSIONS_TTL_SECONDS
      = Long.getLong(ClusterFacade.class.getPackage().getName() + ".cluster_versions_ttl_seconds", 60);
  private static final long TABLES_IN_KEYSPACE_TTL_SECONDS
      = Long.getLong(ClusterFacade.class.getPackage().getName() + ".tables_in_keyspace_ttl_seconds", 60);
  private static final long TOKEN_RANGES_IN_KEYSPACE_TTL_SECONDS
      = Long.getLong(ClusterFacade.class.getPackage().getName() + ".token_ranges_in_keyspace_ttl_seconds", 60);
  private static final Cache<Pair<Cluster, String>, String> CLUSTER_VERSIONS
      = CacheBuilder.newBuilder().expireAfterWrite(CLUSTER_VERSIONS_TTL_SECONDS, TimeUnit.SECONDS).build();
  private static final Cache<Pair<Cluster, String>, Set<Table>> TABLES_IN_KEYSPACE
      = CacheBuilder.newBuilder().expireAfterWrite(TABLES_IN_KEYSPACE_TTL_SECONDS, TimeUnit.SECONDS).build();
  private static final Cache<Pair<Cluster, String>, Map<List<String>, List<String>>> TOKEN_RANGES_IN_KEYSPACE
      = CacheBuilder.newBuilder().expireAfterWrite(TOKEN_RANGES_IN_KEYSPACE_TTL_SECONDS, TimeUnit.SECONDS).build();
  private static final String LOCALHOST = "127.0.0.1";
  private final AppContext context;
  private ClusterFacade(AppContext context) {
    this.context = context;
  }
  public static ClusterFacade create(AppContext context) {
    return new ClusterFacade(context);
  }
  protected static Map<List<String>, List<String>>
      maybeCleanupEndpointFromScylla(Map<List<String>, List<String>> endpointMap) {
    Map<List<String>, List<String>> resultEndpointMap = Maps.newHashMap();
    String firstToken = "";
    String lastToken = "";
    List<String> lastNode = Lists.newArrayList();
    for (Entry<List<String>, List<String>> entry : endpointMap.entrySet()) {
      String nodeStartToken = entry.getKey().get(0);
      String nodeEndToken = entry.getKey().get(1);
      if ("".equals(nodeStartToken)) {
        lastToken = nodeEndToken;
        lastNode = entry.getValue();
      } else if ("".equals(nodeEndToken)) {
        firstToken = nodeStartToken;
      } else {
        resultEndpointMap.put(entry.getKey(), entry.getValue());
      }
    }
    if (!"".equals(firstToken) && !"".equals(lastToken)) {
      resultEndpointMap.put(Lists.newArrayList(firstToken, lastToken), lastNode);
    }
    return resultEndpointMap;
  }
  public static List<StreamSession> parseStreamSessionJson(String json) throws IOException {
    return parseJson(json, new TypeReference<List<StreamSession>>() {
    });
  }
  public static CompactionStats parseCompactionStats(String json) throws IOException {
    if (json.isEmpty()) {
      return CompactionStats.builder()
          .withPendingCompactions(Optional.empty())
          .withActiveCompactions(Collections.emptyList())
          .build();
    }
    try {
      return parseJson(json, new TypeReference<CompactionStats>() {
      });
    } catch (IOException e) {
      List<Compaction> compactions = parseJson(json, new TypeReference<List<Compaction>>() {
      });
      return CompactionStats.builder()
          .withPendingCompactions(Optional.empty())
          .withActiveCompactions(compactions)
          .build();
    }
  }
  private static <T> T parseJson(String json, TypeReference<T> ref) throws IOException {
    try {
      ObjectMapper mapper = new ObjectMapper();
      mapper.registerModule(new Jdk8Module()).registerModule(new JodaModule());
      return mapper.readValue(json, ref);
    } catch (IOException e) {
      LOG.error("Error parsing json", e);
      throw e;
    }
  }
  public ICassandraManagementProxy connectToManagementMechanism(Cluster cluster,
                                                                Collection<String> endpoints) throws ReaperException {
    Preconditions.checkArgument(!context.config.isInSidecarMode());
    return connectImpl(cluster, endpoints);
  }
  public String getClusterName(Cluster cluster, Collection<String> endpoints) throws ReaperException {
    return connect(cluster, endpoints).getClusterName();
  }
  public String getClusterName(Node node) throws ReaperException {
    return connect(node).getClusterName();
  }
  public String getPartitioner(Cluster cluster, Collection<String> endpoints) throws ReaperException {
    return connect(cluster, endpoints).getPartitioner();
  }
  public List<String> getLiveNodes(Cluster cluster) throws ReaperException {
    return getLiveNodes(cluster, cluster.getSeedHosts());
  }
  public List<String> getLiveNodes(Cluster cluster, Collection<String> endpoints) throws ReaperException {
    return connect(cluster, endpoints).getLiveNodes();
  }
  public NodesStatus getNodesStatus(Cluster cluster) throws ReaperException {
    ICassandraManagementProxy cassandraManagementProxy = connect(cluster);
    return cassandraManagementProxy.getNodesStatus();
  }
  public String getCassandraVersion(Cluster cluster) throws ReaperException {
    return getCassandraVersion(cluster, cluster.getSeedHosts());
  }
  public String getCassandraVersion(Cluster cluster, Collection<String> endpoints) throws ReaperException {
    for (String endpoint : endpoints) {
      String version = CLUSTER_VERSIONS.getIfPresent(Pair.of(cluster, endpoint));
      if (null != version) {
        return version;
      }
    }
    ICassandraManagementProxy cassandraManagementProxy = connect(cluster, endpoints);
    String version = cassandraManagementProxy.getCassandraVersion();
    CLUSTER_VERSIONS.put(Pair.of(cluster, cassandraManagementProxy.getHost()), version);
    return version;
  }
  public List<BigInteger> getTokens(Cluster cluster) throws ReaperException {
    return connect(cluster).getTokens();
  }
  public Map<List<String>, List<String>> getRangeToEndpointMap(
      Cluster cluster,
      String keyspace) throws ReaperException {
    try {
      return TOKEN_RANGES_IN_KEYSPACE.get(
          Pair.of(cluster, keyspace),
          () -> getRangeToEndpointMapImpl(cluster, keyspace));
    } catch (ExecutionException ex) {
      throw new ReaperException(ex);
    } catch (ExecutionError ex) {
      if ((ex.getCause() instanceof AssertionError)) {
        throw new ReaperException(String.valueOf(ex));
      } else {
        throw ex;
      }
    }
  }
  public Set<Table> getTablesForKeyspace(Cluster cluster, String keyspaceName) throws ReaperException {
    try {
      return TABLES_IN_KEYSPACE.get(
          Pair.of(cluster, keyspaceName),
          () -> getTablesForKeyspaceImpl(cluster, keyspaceName));
    } catch (ExecutionException ex) {
      throw new ReaperException(ex);
    }
  }
  public Map<String, List<String>> listTablesByKeyspace(Cluster cluster) throws ReaperException {
    return connect(cluster).listTablesByKeyspace();
  }
  public List<String> getKeyspaces(Cluster cluster) throws ReaperException {
    return connect(cluster).getKeyspaces();
  }
  public Map<String, String> getEndpointToHostId(Cluster cluster) throws ReaperException {
    return connect(cluster).getEndpointToHostId();
  }
  public List<String> tokenRangeToEndpoint(Cluster cluster, String keyspace, Segment segment) {
    Set<Map.Entry<List<String>, List<String>>> entries;
    try {
      entries = getRangeToEndpointMap(cluster, keyspace).entrySet();
    } catch (ReaperException e) {
      LOG.error("[tokenRangeToEndpoint] no replicas found for token range {}", segment, e);
      return Lists.newArrayList();
    }
    for (Map.Entry<List<String>, List<String>> entry : entries) {
      BigInteger rangeStart = new BigInteger(entry.getKey().get(0));
      BigInteger rangeEnd = new BigInteger(entry.getKey().get(1));
      if (new RingRange(rangeStart, rangeEnd).encloses(segment.getTokenRanges().get(0))) {
        return entry.getValue();
      }
    }
    LOG.error("[tokenRangeToEndpoint] no replicas found for token range {}", segment);
    LOG.debug("[tokenRangeToEndpoint] checked token ranges were {}", entries);
    return Lists.newArrayList();
  }
  public List<RingRange> getRangesForLocalEndpoint(Cluster cluster, String keyspace) throws ReaperException {
    Preconditions.checkArgument(context.config.isInSidecarMode(), "This method is only allowed in sidecar mode");
    List<RingRange> localRanges = Lists.newArrayList();
    Map<List<String>, List<String>> ranges = getRangeToEndpointMap(cluster, keyspace);
    String localEndpoint = connect(cluster, Arrays.asList(LOCALHOST)).getLocalEndpoint();
    ranges
        .entrySet()
        .stream()
        .forEach(entry -> {
          if (entry.getValue().contains(localEndpoint)) {
            localRanges.add(
                new RingRange(new BigInteger(entry.getKey().get(0)), new BigInteger(entry.getKey().get(1))));
          }
        });
    return localRanges;
  }
  public String getDatacenter(Cluster cluster, String endpoint) throws ReaperException {
    return EndpointSnitchInfoProxy.create(connect(cluster)).getDataCenter(endpoint);
  }
  public String getDatacenter(Node node) throws ReaperException, InterruptedException {
    return EndpointSnitchInfoProxy.create(connect(node)).getDataCenter();
  }
  public String getLocalEndpoint(Node node) throws ReaperException, InterruptedException {
    return connect(node).getLocalEndpoint();
  }
  public Map<String, List<String>> getTokensByNode(Cluster cluster) throws ReaperException {
    return StorageServiceProxy.create(connect(cluster)).getTokensByNode();
  }
  public CompactionStats listActiveCompactions(Node node)
      throws MalformedObjectNameException, ReflectionException, ReaperException, InterruptedException, IOException {
    LOG.debug("Listing active compactions for node {}", node);
    String nodeDc = getDatacenter(node.getCluster().get(), node.getHostname());
    if (nodeIsDirectlyAccessible(nodeDc, node.getHostname())) {
      LOG.debug("Yay!! Node {} in DC {} is accessible for management proxy", node.getHostname(), nodeDc);
      return listCompactionStatsDirect(node);
    } else {
      LOG.debug("Node {} in DC {} is not accessible through jmx/http", node.getHostname(), nodeDc);
      String compactionsJson = ((IDistributedStorage) context.storage).getOperationsDao()
          .listOperations(node.getClusterName(), OpType.OP_COMPACTION, node.getHostname());
      return parseCompactionStats(compactionsJson);
    }
  }
  public CompactionStats listCompactionStatsDirect(Node node)
      throws ReaperException, MalformedObjectNameException, ReflectionException {
    CompactionProxy compactionProxy = CompactionProxy.create(connect(node), context.metricRegistry);
    return CompactionStats.builder()
        .withPendingCompactions(Optional.of(compactionProxy.getPendingCompactions()))
        .withActiveCompactions(compactionProxy.listActiveCompactions())
        .build();
  }
  public boolean nodeIsDirectlyAccessible(String nodeDc, String node) {
    return DatacenterAvailability.ALL == context.config.getDatacenterAvailability()
        || (Arrays.asList(DatacenterAvailability.EACH, DatacenterAvailability.LOCAL)
        .contains(context.config.getDatacenterAvailability())
        && context.managementConnectionFactory.getAccessibleDatacenters().contains(nodeDc))
        || (DatacenterAvailability.SIDECAR == context.config.getDatacenterAvailability()
        && node.equals(context.getLocalNodeAddress()));
  }
  public List<GenericMetric> collectGenericMetrics(Node node) throws ReaperException {
    try {
      return MetricsProxy.create(connect(node), node).collectGenericMetrics();
    } catch (JMException | IOException e) {
      LOG.error("Failed collecting metrics for host {}", node, e);
      throw new ReaperException(e);
    }
  }
  public List<GenericMetric> collectPercentRepairedMetrics(Node node, String keyspaceName) throws ReaperException {
    try {
      return MetricsProxy.create(connect(node), node).collectPercentRepairedMetrics(keyspaceName);
    } catch (JMException | IOException e) {
      LOG.error("Failed collecting metrics for host {}", node, e);
      throw new ReaperException(e);
    }
  }
  public List<MetricsHistogram> getClientRequestLatencies(Node node) throws ReaperException {
    try {
      String nodeDc = getDatacenter(node.getCluster().get(), node.getHostname());
      if (nodeIsDirectlyAccessible(nodeDc, node.getHostname())) {
        MetricsProxy metricsProxy = MetricsProxy.create(connect(node), node);
        return convertToMetricsHistogram(metricsProxy.collectLatencyMetrics());
      } else {
        return convertToMetricsHistogram(((IDistributedStorage) context.storage)
            .getMetrics(
                node.getClusterName(),
                Optional.of(node.getHostname()),
                "org.apache.cassandra.metrics",
                "ClientRequest",
                DateTime.now().minusMinutes(METRICS_PARTITIONING_TIME_MINS + 1).getMillis()));
      }
    } catch (JMException | IOException e) {
      LOG.error("Failed collecting tpstats for host {}", node, e);
      throw new ReaperException(e);
    }
  }
  public List<DroppedMessages> getDroppedMessages(Node node) throws ReaperException {
    try {
      String nodeDc = getDatacenter(node.getCluster().get(), node.getHostname());
      if (nodeIsDirectlyAccessible(nodeDc, node.getHostname())) {
        MetricsProxy proxy = MetricsProxy.create(connect(node), node);
        return convertToDroppedMessages(proxy.collectDroppedMessages());
      } else {
        return convertToDroppedMessages(((IDistributedStorage) context.storage)
            .getMetrics(
                node.getClusterName(),
                Optional.of(node.getHostname()),
                "org.apache.cassandra.metrics",
                "DroppedMessage",
                DateTime.now().minusMinutes(1).getMillis()));
      }
    } catch (JMException | IOException e) {
      LOG.error("Failed collecting tpstats for host {}", node, e);
      throw new ReaperException(e);
    }
  }
  @VisibleForTesting
  public List<DroppedMessages> convertToDroppedMessages(List<GenericMetric> metrics) {
    List<DroppedMessages> droppedMessages = Lists.newArrayList();
    Map<String, List<GenericMetric>> metricsByScope
        = metrics.stream().collect(Collectors.groupingBy(GenericMetric::getMetricScope));
    for (Entry<String, List<GenericMetric>> pool : metricsByScope.entrySet()) {
      DroppedMessages.Builder builder = DroppedMessages.builder().withName(pool.getKey());
      for (GenericMetric stat : pool.getValue()) {
        builder = MetricsProxy.updateGenericMetricAttribute(stat, builder);
      }
      droppedMessages.add(builder.build());
    }
    return droppedMessages;
  }
  public List<ThreadPoolStat> getTpStats(Node node) throws ReaperException {
    try {
      String nodeDc = getDatacenter(node.getCluster().get(), node.getHostname());
      if (nodeIsDirectlyAccessible(nodeDc, node.getHostname())) {
        MetricsProxy proxy = MetricsProxy.create(connect(node), node);
        return convertToThreadPoolStats(proxy.collectTpStats());
      } else {
        Preconditions.checkState(context.storage instanceof IDistributedStorage,
            "Storage must be IDistributedStorage to collect tpstats from storage");
        return convertToThreadPoolStats(((IDistributedStorage) context.storage)
            .getMetrics(
                node.getClusterName(),
                Optional.of(node.getHostname()),
                "org.apache.cassandra.metrics",
                "ThreadPools",
                DateTime.now().minusMinutes(1).getMillis()));
      }
    } catch (JMException | IOException e) {
      LOG.error("Failed collecting tpstats for host {}", node, e);
      throw new ReaperException(e);
    }
  }
  @VisibleForTesting
  public List<ThreadPoolStat> convertToThreadPoolStats(List<GenericMetric> metrics) {
    List<ThreadPoolStat> tpstats = Lists.newArrayList();
    Map<String, List<GenericMetric>> metricsByScope
        = metrics.stream().collect(Collectors.groupingBy(GenericMetric::getMetricScope));
    for (Entry<String, List<GenericMetric>> pool : metricsByScope.entrySet()) {
      ThreadPoolStat.Builder builder = ThreadPoolStat.builder().withName(pool.getKey());
      for (GenericMetric stat : pool.getValue()) {
        builder = MetricsProxy.updateGenericMetricAttribute(stat, builder);
      }
      tpstats.add(builder.build());
    }
    return tpstats;
  }
  @VisibleForTesting
  public List<MetricsHistogram> convertToMetricsHistogram(List<GenericMetric> metrics) {
    List<MetricsHistogram> histograms = Lists.newArrayList();
    Map<String, List<GenericMetric>> metricsByScope
        = metrics.stream().collect(Collectors.groupingBy(GenericMetric::getMetricScope));
    for (Entry<String, List<GenericMetric>> metricByScope : metricsByScope.entrySet()) {
      Map<String, List<GenericMetric>> metricsByName
          = metricByScope
          .getValue()
          .stream()
          .collect(Collectors.groupingBy(GenericMetric::getMetricName));
      for (Entry<String, List<GenericMetric>> metricByName : metricsByName.entrySet()) {
        MetricsHistogram.Builder builder
            = MetricsHistogram.builder()
            .withName(metricByScope.getKey())
            .withType(metricByName.getKey());
        for (GenericMetric stat : metricByName.getValue()) {
          builder = MetricsProxy.updateGenericMetricAttribute(stat, builder);
        }
        histograms.add(builder.build());
      }
    }
    return histograms;
  }
  public Pair<Node, String> takeSnapshot(String snapshotName, Node host, String... keyspaces) throws ReaperException {
    Preconditions.checkArgument(!context.config.isInSidecarMode(), "Snapshots aren't yet supported in sidecar mode");
    LOG.debug("Taking snapshot for node {} and keyspace {}", host, keyspaces);
    return Pair.of(host, SnapshotProxy.create(connect(host)).takeSnapshot(snapshotName, keyspaces));
  }
  public List<Snapshot> listSnapshots(Node host) throws ReaperException {
    try {
      if (context.managementConnectionFactory.getHostConnectionCounters().getSuccessfulConnections(
          host.getHostname()) >= 0) {
        return SnapshotProxy.create(connect(host)).listSnapshots();
      }
    } catch (UnsupportedOperationException unsupported) {
      LOG.debug("Listing snapshot is unsupported with Cassandra 2.0 and prior");
      throw unsupported;
    }
    return Collections.emptyList();
  }
  public void clearSnapshot(String snapshotName, Node host) throws ReaperException {
    try {
      SnapshotProxy.create(connect(host)).clearSnapshot(snapshotName);
    } catch (IOError e) {
      LOG.info("already cleared snapshot " + snapshotName, e);
    }
  }
  public List<StreamSession> listActiveStreams(Node node)
      throws ReaperException, InterruptedException, IOException {
    String nodeDc = getDatacenter(node.getCluster().get(), node.getHostname());
    if (nodeIsDirectlyAccessible(nodeDc, node.getHostname())) {
      return listStreamsDirect(node);
    } else {
      LOG.debug("Node {} in DC {} is not accessible through jmx/http", node.getHostname(), nodeDc);
      String streamsJson = ((IDistributedStorage) context.storage).getOperationsDao()
          .listOperations(node.getClusterName(), OpType.OP_STREAMING, node.getHostname());
      if (streamsJson.length() > 0) {
        return parseStreamSessionJson(streamsJson);
      }
      return Collections.emptyList();
    }
  }
  public List<StreamSession> listStreamsDirect(Node node) throws ReaperException {
    return StreamsProxy.create(connect(node)).listStreams(node);
  }
  private Set<Table> getTablesForKeyspaceImpl(Cluster cluster, String keyspaceName) throws ReaperException {
    return connect(cluster).getTablesForKeyspace(keyspaceName);
  }
  private Map<List<String>, List<String>> getRangeToEndpointMapImpl(
      Cluster cluster,
      String keyspace) throws ReaperException {
    ICassandraManagementProxy managementConnection = connect(cluster);
    Map<List<String>, List<String>> endpointMap = managementConnection.getRangeToEndpointMap(keyspace);
    return maybeCleanupEndpointFromScylla(endpointMap);
  }
  public ICassandraManagementProxy connect(Cluster cluster) throws ReaperException {
    return connectImpl(cluster, enforceLocalNodeForSidecar(cluster.getSeedHosts()));
  }
  public ICassandraManagementProxy connect(Cluster cluster, Collection<String> endpoints) throws ReaperException {
    return connectImpl(cluster, enforceLocalNodeForSidecar(endpoints));
  }
  public ICassandraManagementProxy connect(Node node) throws ReaperException {
    return connectImpl(node, enforceLocalNodeForSidecar(Collections.singletonList(node.getHostname())));
  }
  public ICassandraManagementProxy connect(Node node, Collection<String> endpoints) throws ReaperException {
    return connectImpl(node, enforceLocalNodeForSidecar(endpoints));
  }
  private ICassandraManagementProxy connectImpl(Cluster cluster, Collection<String> endpoints)
          throws ReaperException {
    try {
      ICassandraManagementProxy proxy = context.managementConnectionFactory.connectAny(endpoints.stream()
          .map(host -> Node.builder().withCluster(cluster).withHostname(host).build())
          .collect(Collectors.toList())
      );
      Async.markClusterActive(cluster, context);
      return proxy;
    } catch (ReaperException ex) {
      Async.markClusterUnreachable(cluster, context);
      throw ex;
    }
  }
  private ICassandraManagementProxy connectImpl(Node node, Collection<String> endpoints) throws ReaperException {
    return context.managementConnectionFactory.connectAny(
        endpoints
            .stream()
            .map(host -> node.with().withHostname(host).build())
            .collect(Collectors.toList()));
  }
  private Collection<String> enforceLocalNodeForSidecar(Collection<String> endpoints) {
    return context.config.isInSidecarMode()
        ? Arrays.asList(context.config.getEnforcedLocalNode().orElse(LOCALHOST))
        : endpoints;
  }
  private static final class Async {
    private static final ExecutorService ASYNC = Executors.newSingleThreadExecutor();
    private static boolean markClusterActive(Cluster cluster, AppContext context) {
      if (Cluster.State.UNKNOWN != cluster.getState() && !LocalDate.now().equals(cluster.getLastContact())) {
        Cluster.Builder builder = cluster.with().withState(Cluster.State.ACTIVE).withLastContact(LocalDate.now());
        ASYNC.submit(() -> context.storage.getClusterDao().updateCluster(builder.build()));
        return true;
      }
      return false;
    }
    private static boolean markClusterUnreachable(Cluster cluster, AppContext context) {
      if (Cluster.State.ACTIVE == cluster.getState()
          && LocalDate.now().minusDays(context.config.getClusterTimeoutInDays()).isAfter(cluster.getLastContact())) {
        ASYNC.submit(() -> context.storage.getClusterDao()
            .updateCluster(cluster.with().withState(Cluster.State.UNREACHABLE).build()));
        return true;
      }
      return false;
    }
  }
}
package io.cassandrareaper.service;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.ReaperApplicationConfiguration.DatacenterAvailability;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.CompactionStats;
import io.cassandrareaper.core.Node;
import io.cassandrareaper.core.RepairRun;
import io.cassandrareaper.core.RepairSchedule;
import io.cassandrareaper.core.RepairSegment;
import io.cassandrareaper.core.RepairUnit;
import io.cassandrareaper.core.Segment;
import io.cassandrareaper.management.ClusterFacade;
import io.cassandrareaper.management.EndpointSnitchInfoProxy;
import io.cassandrareaper.management.ICassandraManagementProxy;
import io.cassandrareaper.metrics.PrometheusMetricsFilter;
import io.cassandrareaper.storage.IDistributedStorage;
import io.cassandrareaper.storage.repairrun.IRepairRunDao;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.NoSuchElementException;
import java.util.Optional;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.ThreadLocalRandom;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.stream.Collectors;
import com.codahale.metrics.Gauge;
import com.codahale.metrics.MetricRegistry;
import com.datastax.driver.core.utils.UUIDs;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import com.google.common.util.concurrent.MoreExecutors;
import org.apache.cassandra.repair.RepairParallelism;
import org.apache.commons.lang3.StringUtils;
import org.apache.commons.lang3.tuple.Pair;
import org.joda.time.DateTime;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import static io.cassandrareaper.metrics.MetricNameUtils.cleanId;
import static io.cassandrareaper.metrics.MetricNameUtils.cleanName;
final class RepairRunner implements Runnable {
  private static final Logger LOG = LoggerFactory.getLogger(RepairRunner.class);
  private static final ExecutorService METRICS_GRABBER_EXECUTOR = Executors.newFixedThreadPool(10);
  private static final int PERCENT_EXTENDED_THRESHOLD = 20;
  private static final int MIN_SEGMENTS_PER_NODE_REDUCTION = 16;
  private static final int SEGMENT_DURATION_FOR_REDUCTION_THRESHOLD = 5;
  private final AppContext context;
  private final ClusterFacade clusterFacade;
  private final RepairRunService repairRunService;
  private final UUID repairRunId;
  private final String clusterName;
  private final String metricNameForMillisSinceLastRepairPerKeyspace;
  private final String metricNameForMillisSinceLastRepair;
  private final Cluster cluster;
  private float repairProgress;
  private float segmentsDone;
  private float segmentsTotal;
  private final List<RingRange> localEndpointRanges;
  private final RepairUnit repairUnit;
  private AtomicBoolean isRunning = new AtomicBoolean(false);
  private final IRepairRunDao repairRunDao;
  private RepairRunner(
      AppContext context,
      UUID repairRunId,
      ClusterFacade clusterFacade,
      IRepairRunDao repairRunDao) throws ReaperException {
    LOG.debug("Creating RepairRunner for run with ID {}", repairRunId);
    this.context = context;
    this.clusterFacade = clusterFacade;
    this.repairRunService = RepairRunService.create(context, repairRunDao);
    this.repairRunId = repairRunId;
    Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
    assert repairRun.isPresent() : "No RepairRun with ID " + repairRunId + " found from storage";
    this.isRunning.set(repairRun.get().getRunState() == RepairRun.RunState.RUNNING);
    this.cluster = context.storage.getClusterDao().getCluster(repairRun.get().getClusterName());
    repairUnit = context.storage.getRepairUnitDao().getRepairUnit(repairRun.get().getRepairUnitId());
    this.clusterName = cluster.getName();
    this.repairRunDao = repairRunDao;
    localEndpointRanges = context.config.isInSidecarMode()
        ? clusterFacade.getRangesForLocalEndpoint(cluster, repairUnit.getKeyspaceName())
        : Collections.emptyList();
    String repairUnitClusterName = repairUnit.getClusterName();
    String repairUnitKeyspaceName = repairUnit.getKeyspaceName();
    String metricNameForRepairProgressPerKeyspace
        = metricName(
        "repairProgress",
        repairUnitClusterName,
        repairUnitKeyspaceName,
        repairRun.get().getRepairUnitId());
    String metricNameForRepairProgress
        = metricName("repairProgress", repairUnitClusterName, repairRun.get().getRepairUnitId());
    registerMetric(metricNameForRepairProgressPerKeyspace, (Gauge<Float>) () -> repairProgress);
    registerMetric(metricNameForRepairProgress, (Gauge<Float>) () -> repairProgress);
    PrometheusMetricsFilter.ignoreMetric(metricNameForRepairProgress);
    metricNameForMillisSinceLastRepairPerKeyspace
        = metricName(
        "millisSinceLastRepair",
        repairUnitClusterName,
        repairUnitKeyspaceName,
        repairRun.get().getRepairUnitId());
    metricNameForMillisSinceLastRepair
        = metricName(
        "millisSinceLastRepair", repairUnitClusterName, repairRun.get().getRepairUnitId());
    String metricNameForDoneSegmentsPerKeyspace
        = metricName("segmentsDone", repairUnitClusterName, repairUnitKeyspaceName, repairRun.get().getRepairUnitId());
    String metricNameForDoneSegments
        = metricName("segmentsDone", repairUnitClusterName, repairRun.get().getRepairUnitId());
    registerMetric(metricNameForDoneSegmentsPerKeyspace, (Gauge<Float>) () -> segmentsDone);
    registerMetric(metricNameForDoneSegments, (Gauge<Integer>) () -> (int) segmentsDone);
    PrometheusMetricsFilter.ignoreMetric(metricNameForDoneSegments);
    String metricNameForTotalSegmentsPerKeyspace
        = metricName("segmentsTotal", repairUnitClusterName, repairUnitKeyspaceName, repairRun.get().getRepairUnitId());
    String metricNameForTotalSegments
        = metricName("segmentsTotal", repairUnitClusterName, repairRun.get().getRepairUnitId());
    registerMetric(metricNameForTotalSegmentsPerKeyspace, (Gauge<Integer>) () -> (int) segmentsTotal);
    registerMetric(metricNameForTotalSegments, (Gauge<Float>) () -> segmentsTotal);
    PrometheusMetricsFilter.ignoreMetric(metricNameForTotalSegments);
  }
  public static RepairRunner create(
      AppContext context,
      UUID repairRunId,
      ClusterFacade clusterFacade,
      IRepairRunDao repairRunDao) throws ReaperException {
    return new RepairRunner(context, repairRunId, clusterFacade, repairRunDao);
  }
  static boolean okToRepairSegment(
      boolean allHostsChecked,
      boolean allLocalDcHostsChecked,
      DatacenterAvailability dcAvailability) {
    return allHostsChecked || (allLocalDcHostsChecked && DatacenterAvailability.LOCAL == dcAvailability);
  }
  @VisibleForTesting
  String getClusterName() {
    return clusterName;
  }
  private void registerMetric(String metricName, Gauge<?> gauge) {
    if (context.metricRegistry.getMetrics().containsKey(metricName)) {
      context.metricRegistry.remove(metricName);
    }
    context.metricRegistry.register(metricName, gauge);
  }
  boolean isRunning() {
    return isRunning.get();
  }
  UUID getRepairRunId() {
    return repairRunId;
  }
  @Override
  public void run() {
    Thread.currentThread().setName(clusterName + ":" + repairRunId);
    Map<UUID, RepairRunner> currentRunners = context.repairManager.repairRunners;
    List<UUID> repairRunIds = RepairRunner.getRunningRepairRunIds(currentRunners, clusterName);
    try {
      Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
      if ((!repairRun.isPresent() || repairRun.get().getRunState().isTerminated())) {
        LOG.warn("RepairRun \"{}\" does not exist. Killing RepairRunner for this run instance.", repairRunId);
        killAndCleanupRunner();
        return;
      }
      RepairRun.RunState state = repairRun.get().getRunState();
      this.isRunning.set(repairRun.get().getRunState() == RepairRun.RunState.RUNNING);
      LOG.debug("run() called for repair run #{} with run state {}", repairRunId, state);
      switch (state) {
        case NOT_STARTED:
          start();
          break;
        case RUNNING:
          if (isAllowedToRun(repairRunIds, repairRunId)) {
            startNextSegment();
            updateClusterNodeList();
          } else {
            LOG.info("Maximum number of concurrent repairs reached. Repair {} will resume later.", repairRunId);
            LOG.info("Current active repair runners: {}",
                repairRunIds.stream()
                    .map(runId -> Pair.of(runId, UUIDs.unixTimestamp(runId)))
                    .collect(Collectors.toList()));
            context.repairManager.scheduleRetry(this);
          }
          break;
        case PAUSED:
          context.repairManager.scheduleRetry(this);
          break;
        default:
          throw new IllegalStateException("un-known/implemented state " + state);
      }
    } catch (RuntimeException | ReaperException | InterruptedException e) {
      LOG.error("RepairRun FAILURE, scheduling retry", e);
      context.repairManager.scheduleRetry(this);
    }
    LOG.debug("run() exiting for repair run #{}", repairRunId);
  }
  @VisibleForTesting
  static List<UUID> getRunningRepairRunIds(Map<UUID, RepairRunner> currentRunners, String currentClusterName) {
    return new ArrayList<UUID>(currentRunners.entrySet().stream()
          .filter(entry -> entry.getValue().isRunning())
          .filter(entry -> entry.getValue().getClusterName().equals(currentClusterName))
          .map(Entry::getKey)
          .collect(Collectors.toList()));
  }
  @VisibleForTesting
  boolean isAllowedToRun(List<UUID> runningRepairRunIds, UUID currentId) {
    runningRepairRunIds.sort((id1, id2) -> Long.valueOf(UUIDs.unixTimestamp(id1)).compareTo(UUIDs.unixTimestamp(id2)));
    for (int i = 0; i < context.config.getMaxParallelRepairs(); i++) {
      LOG.debug("Repair run #{} is in the list of running repair runs in position {}", runningRepairRunIds.get(i), i);
      if (runningRepairRunIds.get(i).equals(currentId)) {
        LOG.debug("Repair run #{} is allowed to run", currentId);
        return true;
      }
    }
    LOG.debug("Repair run #{} is not allowed to run", currentId);
    return false;
  }
  private void start() throws ReaperException, InterruptedException {
    LOG.info("Repairs for repair run #{} starting", repairRunId);
    synchronized (this) {
      RepairRun repairRun = repairRunDao.getRepairRun(repairRunId).get();
      repairRunDao.updateRepairRun(
          repairRun.with().runState(RepairRun.RunState.RUNNING).startTime(DateTime.now()).build(repairRun.getId()));
    }
    startNextSegment();
  }
  private void updateClusterNodeList() throws ReaperException {
    Set<String> liveNodes = ImmutableSet.copyOf(clusterFacade.getLiveNodes(cluster));
    Cluster cluster = context.storage.getClusterDao().getCluster(clusterName);
    if (context.config.getEnableDynamicSeedList() && !cluster.getSeedHosts().equals(liveNodes)
        && !liveNodes.isEmpty()) {
      LOG.info("Updating the seed list for cluster {} as topology changed since the last repair.", clusterName);
      context.storage.getClusterDao().updateCluster(cluster.with().withSeedHosts(liveNodes).build());
    }
  }
  private void endRepairRun() {
    LOG.info("Repairs for repair run #{} done", repairRunId);
    synchronized (this) {
      Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
      if (repairRun.isPresent()) {
        try {
          DateTime repairRunCompleted = DateTime.now();
          repairRunDao.updateRepairRun(
              repairRun.get()
                  .with()
                  .runState(RepairRun.RunState.DONE)
                  .endTime(repairRunCompleted)
                  .lastEvent("All done")
                  .build(repairRun.get().getId()));
          context.metricRegistry.remove(metricNameForMillisSinceLastRepairPerKeyspace);
          context.metricRegistry.remove(metricNameForMillisSinceLastRepair);
          PrometheusMetricsFilter.removeIgnoredMetric(metricNameForMillisSinceLastRepair);
          context.metricRegistry.register(
              metricNameForMillisSinceLastRepairPerKeyspace,
              (Gauge<Long>) () -> DateTime.now().getMillis() - repairRunCompleted.toInstant().getMillis());
          context.metricRegistry.register(
              metricNameForMillisSinceLastRepair,
              (Gauge<Long>) () -> DateTime.now().getMillis() - repairRunCompleted.toInstant().getMillis());
          PrometheusMetricsFilter.ignoreMetric(metricNameForMillisSinceLastRepair);
          context.metricRegistry.counter(
              MetricRegistry.name(RepairManager.class, "repairDone", RepairRun.RunState.DONE.toString())).inc();
          maybeAdaptRepairSchedule();
          context.schedulingManager.maybeRegisterRepairRunCompleted(repairRun.get());
        } finally {
          killAndCleanupRunner();
        }
      }
    }
  }
  private void maybeScheduleRetryOnError() {
    if (context.config.isScheduleRetryOnError()) {
      Collection<RepairSchedule> schedulesForKeyspace
              = context.storage.getRepairScheduleDao()
              .getRepairSchedulesForClusterAndKeyspace(clusterName, repairUnit.getKeyspaceName());
      List<RepairSchedule> repairSchedules = schedulesForKeyspace.stream()
              .filter(schedule -> schedule.getRepairUnitId().equals(repairUnit.getId()))
              .collect(Collectors.toList());
      if (!repairSchedules.isEmpty()) {
        Preconditions.checkArgument(repairSchedules.size() == 1,
                String.format("Update for repair run %s and unit %s "
                                + "should impact a single schedule. %d were found",
                        repairRunId,
                        repairUnit.getId(),
                        repairSchedules.size())
        );
        RepairSchedule scheduleToTune = repairSchedules.get(0);
        int minuteRetryDelay = (int) context.config.getScheduleRetryDelay().toMinutes();
        DateTime nextRepairRun = DateTime.now().plusMinutes(minuteRetryDelay);
        if (nextRepairRun.isBefore(scheduleToTune.getNextActivation())) {
          LOG.debug("Scheduling next repair run at {} for repair schedule {}", nextRepairRun,
                  scheduleToTune.getId());
          RepairSchedule newSchedule
                  = scheduleToTune.with().nextActivation(nextRepairRun).build(scheduleToTune.getId());
          context.storage.getRepairScheduleDao().updateRepairSchedule(newSchedule);
        }
      }
    }
  }
  @VisibleForTesting
  public void maybeAdaptRepairSchedule() {
    Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
    if (repairRun.isPresent() && Boolean.TRUE.equals(repairRun.get().getAdaptiveSchedule())) {
      Collection<RepairSegment> segments
          = context.storage.getRepairSegmentDao().getSegmentsWithState(repairRunId, RepairSegment.State.DONE);
      int maxSegmentDurationInMins = segments.stream()
          .mapToInt(repairSegment
              -> (int) (repairSegment.getEndTime().getMillis() - repairSegment.getStartTime().getMillis()) / 60_000)
          .max()
          .orElseThrow(NoSuchElementException::new);
      int extendedSegments = (int) segments.stream().filter(segment -> segment.getFailCount() > 0).count();
      double percentExtendedSegments
          = ((float) extendedSegments / (float) repairRun.get().getSegmentCount()) * 100.0;
      LOG.info("extendedSegments = {}, total segments = {}, percent extended = {}",
          extendedSegments,
          repairRun.get().getSegmentCount(),
          percentExtendedSegments);
      tuneAdaptiveRepair(percentExtendedSegments, maxSegmentDurationInMins);
    }
  }
  @VisibleForTesting
  void tuneAdaptiveRepair(double percentExtendedSegments, int maxSegmentDuration) {
    LOG.info("Percent extended segments: {}", percentExtendedSegments);
    if (percentExtendedSegments > PERCENT_EXTENDED_THRESHOLD) {
      addSegmentsPerNodeToScheduleForUnit();
    } else if ((int) percentExtendedSegments <= PERCENT_EXTENDED_THRESHOLD && percentExtendedSegments >= 1) {
      raiseTimeoutOfUnit();
    } else if (percentExtendedSegments == 0 && maxSegmentDuration < SEGMENT_DURATION_FOR_REDUCTION_THRESHOLD) {
      reduceSegmentsPerNodeToScheduleForUnit();
    }
  }
  @VisibleForTesting
  void reduceSegmentsPerNodeToScheduleForUnit() {
    LOG.debug("Reducing segments per node for adaptive schedule on repair unit {}", repairUnit.getId());
    RepairSchedule scheduleToTune = getScheduleForRun();
    int newSegmentCountPerNode
        = (int) Math.max(MIN_SEGMENTS_PER_NODE_REDUCTION, scheduleToTune.getSegmentCountPerNode() / 1.1);
    RepairSchedule newSchedule
        = scheduleToTune.with().segmentCountPerNode(newSegmentCountPerNode).build(scheduleToTune.getId());
    context.storage.getRepairScheduleDao().updateRepairSchedule(newSchedule);
  }
  @VisibleForTesting
  void raiseTimeoutOfUnit() {
    RepairUnit updatedUnit = repairUnit.with().timeout(repairUnit.getTimeout() * 2).build(repairUnit.getId());
    context.storage.getRepairUnitDao().updateRepairUnit(updatedUnit);
  }
  @VisibleForTesting
  void addSegmentsPerNodeToScheduleForUnit() {
    LOG.debug("Adding segments per node for adaptive schedule on repair unit {}", repairUnit.getId());
    RepairSchedule scheduleToTune = getScheduleForRun();
    int newSegmentCountPerNode
        = (int) Math.max(MIN_SEGMENTS_PER_NODE_REDUCTION, scheduleToTune.getSegmentCountPerNode() * 1.2);
    RepairSchedule newSchedule
        = scheduleToTune.with().segmentCountPerNode(newSegmentCountPerNode).build(scheduleToTune.getId());
    context.storage.getRepairScheduleDao().updateRepairSchedule(newSchedule);
  }
  private RepairSchedule getScheduleForRun() {
    Collection<RepairSchedule> schedulesForKeyspace
        = context.storage.getRepairScheduleDao()
        .getRepairSchedulesForClusterAndKeyspace(clusterName, repairUnit.getKeyspaceName());
    List<RepairSchedule> schedulesToTune = schedulesForKeyspace.stream()
        .filter(schedule -> schedule.getRepairUnitId().equals(repairUnit.getId()))
        .collect(Collectors.toList());
    Preconditions.checkArgument(schedulesToTune.size() == 1, String.format("Update for repair run %s and unit %s "
        + "should impact a single schedule. %d were found", repairRunId, repairUnit.getId(), schedulesToTune.size()));
    return schedulesToTune.get(0);
  }
  private void startNextSegment() throws ReaperException, InterruptedException {
    boolean scheduleRetry = true;
    boolean repairStarted = false;
    LOG.info("Attempting to run new segment...");
    List<RepairSegment> nextRepairSegments
        = context.config.isInSidecarMode()
        ? ((IDistributedStorage) context.storage)
        .getNextFreeSegmentsForRanges(
            repairRunId, localEndpointRanges)
        : context.storage.getRepairSegmentDao().getNextFreeSegments(
        repairRunId);
    Optional<RepairSegment> nextRepairSegment = Optional.empty();
    final Collection<String> potentialReplicas = new HashSet<>();
    for (RepairSegment segment : nextRepairSegments) {
      Map<String, String> potentialReplicaMap = this.repairRunService.getDCsByNodeForRepairSegment(
          cluster, segment.getTokenRange(), repairUnit.getKeyspaceName(), repairUnit);
      if (repairUnit.getIncrementalRepair() && !repairUnit.getSubrangeIncrementalRepair()) {
        Map<String, String> endpointHostIdMap = clusterFacade.getEndpointToHostId(cluster);
        if (segment.getHostID() == null) {
          throw new ReaperException(
              String.format("No host ID for repair segment %s", segment.getId().toString())
          );
        }
        endpointHostIdMap.entrySet().stream()
            .filter(entry -> entry.getValue().equals(segment.getHostID().toString()))
            .forEach(entry -> potentialReplicas.add(entry.getKey()));
      } else {
        potentialReplicas.addAll(potentialReplicaMap.keySet());
      }
      if (potentialReplicas.isEmpty()) {
        failRepairDueToOutdatedSegment(segment.getId(), segment.getTokenRange());
      }
      LOG.debug("Potential replicas for segment {}: {}", segment.getId(), potentialReplicas);
      ICassandraManagementProxy coordinator = clusterFacade.connect(cluster, potentialReplicas);
      if (nodesReadyForNewRepair(coordinator, segment, potentialReplicaMap, repairRunId)) {
        nextRepairSegment = Optional.of(segment);
        break;
      }
    }
    if (!nextRepairSegment.isPresent()) {
      String msg = "All nodes are busy or have too many pending compactions for the remaining candidate segments.";
      LOG.info(msg);
      updateLastEvent(msg);
    } else {
      LOG.info("Next segment to run : {}", nextRepairSegment.get().getId());
      scheduleRetry = repairSegment(
          nextRepairSegment.get().getId(),
          nextRepairSegment.get().getTokenRange(),
          potentialReplicas);
      if (scheduleRetry) {
        segmentsTotal = context.storage.getRepairSegmentDao().getSegmentAmountForRepairRun(repairRunId);
        repairStarted = true;
      }
    }
    if (!repairStarted) {
      segmentsDone = context.storage.getRepairSegmentDao().getSegmentAmountForRepairRunWithState(repairRunId,
          RepairSegment.State.DONE);
      segmentsTotal = context.storage.getRepairSegmentDao().getSegmentAmountForRepairRun(repairRunId);
      LOG.info("Repair amount done {}", segmentsDone);
      repairProgress = segmentsDone / segmentsTotal;
      if (segmentsDone == segmentsTotal) {
        endRepairRun();
        scheduleRetry = false;
      }
    } else {
      segmentsDone = context.storage.getRepairSegmentDao().getSegmentAmountForRepairRunWithState(repairRunId,
          RepairSegment.State.DONE);
    }
    if (scheduleRetry) {
      context.repairManager.scheduleRetry(this);
    }
  }
  Pair<String, Callable<Optional<CompactionStats>>> getNodeMetrics(String node, String localDc, String nodeDc) {
    return Pair.of(node, () -> {
      LOG.debug("getMetricsForHost {} / {} / {}", node, localDc, nodeDc);
      CompactionStats activeCompactions = clusterFacade.listActiveCompactions(
          Node.builder()
              .withCluster(cluster)
              .withHostname(node)
              .build());
      return Optional.ofNullable(activeCompactions);
    });
  }
  private boolean nodesReadyForNewRepair(
      ICassandraManagementProxy coordinator,
      RepairSegment segment,
      Map<String, String> dcByNode,
      UUID segmentId) throws ReaperException {
    Collection<String> nodes = getNodesInvolvedInSegment(dcByNode);
    LOG.debug("Nodes involved in segment {}: {}", segmentId, nodes);
    String dc = EndpointSnitchInfoProxy.create(coordinator).getDataCenter();
    boolean requireAllHostMetrics = DatacenterAvailability.LOCAL != context.config.getDatacenterAvailability();
    boolean allLocalDcHostsChecked = true;
    boolean allHostsChecked = true;
    Set<String> unreachableNodes = Sets.newHashSet();
    List<Pair<String, Future<Optional<CompactionStats>>>> nodeMetricsTasks = nodes.stream()
        .map(node -> getNodeMetrics(node, dc != null ? dc : "", dcByNode.get(node) != null ? dcByNode.get(node) : ""))
        .map(pair -> Pair.of(pair.getLeft(), METRICS_GRABBER_EXECUTOR.submit(pair.getRight())))
        .collect(Collectors.toList());
    for (Pair<String, Future<Optional<CompactionStats>>> pair : nodeMetricsTasks) {
      try {
        Optional<CompactionStats> result = pair.getRight().get();
        if (result.isPresent()) {
          CompactionStats metrics = result.get();
          Optional<Integer> pendingCompactions = metrics.getPendingCompactions();
          if (pendingCompactions.isPresent() && pendingCompactions.get() > context.config.getMaxPendingCompactions()) {
            String msg = String.format(
                "postponed repair segment %s because of too many pending compactions (%s > %s) on host %s",
                segmentId, pendingCompactions, context.config.getMaxPendingCompactions(), pair.getLeft());
            updateLastEvent(msg);
            return false;
          }
          continue;
        }
      } catch (InterruptedException | ExecutionException e) {
        LOG.info("Failed grabbing metrics from {}", pair.getLeft(), e);
      }
      allHostsChecked = false;
      if (dcByNode.get(pair.getLeft()).equals(dc)) {
        allLocalDcHostsChecked = false;
      }
      if (requireAllHostMetrics || dcByNode.get(pair.getLeft()).equals(dc)) {
        unreachableNodes.add(pair.getLeft());
      }
    }
    if (okToRepairSegment(allHostsChecked, allLocalDcHostsChecked, context.config.getDatacenterAvailability())) {
      LOG.debug("Ok to repair segment '{}' on repair run with id '{}'", segment.getId(), segment.getRunId());
      return true;
    } else {
      LOG.debug("Couldn't get metrics for hosts {}, will retry later", unreachableNodes);
      String msg = String.format(
          "Postponed repair segment %s on repair run with id %s because we couldn't get %shosts metrics on %s",
          segment.getId(),
          segment.getRunId(),
          (requireAllHostMetrics ? "" : "datacenter "),
          StringUtils.join(unreachableNodes, ' '));
      updateLastEvent(msg);
      return false;
    }
  }
  private Collection<String> getNodesInvolvedInSegment(Map<String, String> dcByNode) {
    Set<String> datacenters = repairUnit.getDatacenters();
    return dcByNode.keySet().stream()
        .filter(node -> datacenters.isEmpty() || datacenters.contains(dcByNode.get(node)))
        .collect(Collectors.toList());
  }
  private boolean repairSegment(final UUID segmentId, Segment segment, Collection<String> segmentReplicas)
      throws InterruptedException, ReaperException {
    RepairRun repairRun;
    final UUID unitId;
    final double intensity;
    final RepairParallelism validationParallelism;
    {
      repairRun = repairRunDao.getRepairRun(repairRunId).get();
      unitId = repairRun.getRepairUnitId();
      intensity = repairRun.getIntensity();
      validationParallelism = repairRun.getRepairParallelism();
      int amountDone = context.storage.getRepairSegmentDao().getSegmentAmountForRepairRunWithState(repairRunId,
          RepairSegment.State.DONE);
      repairProgress = (float) amountDone / repairRun.getSegmentCount();
    }
    RepairUnit repairUnit = context.storage.getRepairUnitDao().getRepairUnit(unitId);
    repairRun = fixMissingRepairRunTables(repairRun, repairUnit);
    String keyspace = repairUnit.getKeyspaceName();
    LOG.debug("preparing to repair segment {} on run with id {}", segmentId, repairRunId);
    List<String> potentialCoordinators = Lists.newArrayList();
    if (!repairUnit.getIncrementalRepair() || repairUnit.getSubrangeIncrementalRepair()) {
      try {
        potentialCoordinators = filterPotentialCoordinatorsByDatacenters(
            repairUnit.getDatacenters(),
            clusterFacade.tokenRangeToEndpoint(cluster, keyspace, segment));
      } catch (RuntimeException e) {
        LOG.warn("Couldn't get token ranges from coordinator", e);
        return true;
      }
      if (potentialCoordinators.isEmpty()) {
        failRepairDueToOutdatedSegment(segmentId, segment);
        return false;
      }
    } else {
      Thread.sleep(ThreadLocalRandom.current().nextInt(10, 100) * 100);
      Optional<RepairSegment> rs = context.storage.getRepairSegmentDao().getRepairSegment(repairRunId, segmentId);
      if (rs.isPresent()) {
        potentialCoordinators.addAll(segmentReplicas);
      } else {
        return false;
      }
    }
    try {
      SegmentRunner segmentRunner = SegmentRunner.create(
          context,
          clusterFacade,
          segmentId,
          potentialCoordinators,
          TimeUnit.MINUTES.toMillis(repairUnit.getTimeout()),
          intensity,
          validationParallelism,
          clusterName,
          repairUnit,
          repairRun.getTables(),
          this);
      ListenableFuture<?> segmentResult = context.repairManager.submitSegment(segmentRunner);
      Futures.addCallback(
          segmentResult,
          new FutureCallback<Object>() {
            @Override
            public void onSuccess(Object ignored) {
              handleResult(segmentId);
            }
            @Override
            public void onFailure(Throwable throwable) {
              LOG.error("Executing SegmentRunner failed", throwable);
            }
          },
          MoreExecutors.directExecutor());
    } catch (ReaperException ex) {
      LOG.error("Executing SegmentRunner failed", ex);
    }
    return true;
  }
  private synchronized void failRepairDueToOutdatedSegment(UUID segmentId, Segment segment) {
    LOG.warn("Segment #{} is faulty, no potential coordinators for range: {}", segmentId,
            segment.toString());
    Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
    if (repairRun.isPresent()) {
      try {
        repairRunDao.updateRepairRun(
                repairRunDao.getRepairRun(repairRunId).get()
                        .with()
                        .runState(RepairRun.RunState.ERROR)
                        .lastEvent(String.format("No coordinators for range %s", segment))
                        .endTime(DateTime.now())
                        .build(repairRunId));
        context.metricRegistry.counter(
                MetricRegistry.name(RepairManager.class, "repairDone",
                        RepairRun.RunState.ERROR.toString())).inc();
        maybeScheduleRetryOnError();
      } finally {
        killAndCleanupRunner();
      }
    }
  }
  private List<String> filterPotentialCoordinatorsByDatacenters(
      Collection<String> datacenters,
      List<String> potentialCoordinators) throws ReaperException {
    List<Pair<String, String>> coordinatorsWithDc = Lists.newArrayList();
    for (String coordinator : potentialCoordinators) {
      coordinatorsWithDc.add(getNodeDatacenterPair(coordinator));
    }
    List<String> coordinators = coordinatorsWithDc
        .stream()
        .filter(node -> datacenters.contains(node.getRight()) || datacenters.isEmpty())
        .map(nodeTuple -> nodeTuple.getLeft())
        .collect(Collectors.toList());
    LOG.debug(
        "[filterPotentialCoordinatorsByDatacenters] coordinators filtered by dc {}. Before : {} / After : {}",
        datacenters,
        potentialCoordinators,
        coordinators);
    return coordinators;
  }
  private Pair<String, String> getNodeDatacenterPair(String node) throws ReaperException {
    Pair<String, String> result = Pair.of(node, clusterFacade.getDatacenter(cluster, node));
    LOG.debug("[getNodeDatacenterPair] node/datacenter association {}", result);
    return result;
  }
  private void handleResult(UUID segmentId) {
    Optional<RepairSegment> segment = context.storage.getRepairSegmentDao().getRepairSegment(repairRunId, segmentId);
    if (segment.isPresent()) {
      RepairSegment.State state = segment.get().getState();
      LOG.debug("In repair run #{}, triggerRepair on segment {} ended with state {}", repairRunId, segmentId, state);
      switch (state) {
        case NOT_STARTED:
          break;
        case DONE:
          break;
        default:
          String msg = "handleResult called with a segment state ("
              + state
              + ") that it "
              + "should not have after segmentRunner has tried a repair";
          LOG.error(msg);
          throw new AssertionError(msg);
      }
    } else {
      LOG.warn("In repair run #{}, triggerRepair on segment {} ended, but run is missing", repairRunId, segmentId);
    }
  }
  void updateLastEvent(String newEvent) {
    synchronized (this) {
      Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
      if (!repairRun.isPresent() || repairRun.get().getRunState().isTerminated()) {
        LOG.warn(
            "Will not update lastEvent of run that has already terminated. The message was: " + "\"{}\"",
            newEvent);
      } else {
        repairRunDao.updateRepairRun(
            repairRun.get().with().lastEvent(newEvent).build(repairRunId),
            Optional.of(false));
        LOG.info(newEvent);
      }
    }
  }
  void killAndCleanupRunner() {
    context.repairManager.removeRunner(this);
    Thread.currentThread().interrupt();
  }
  private String metricName(String metric, String clusterName, String keyspaceName, UUID repairRunId) {
    return MetricRegistry.name(RepairRunner.class, metric, cleanName(clusterName), cleanName(keyspaceName),
        cleanId(repairRunId));
  }
  private String metricName(String metric, String clusterName, UUID repairRunId) {
    return MetricRegistry.name(RepairRunner.class, metric, cleanName(clusterName), cleanId(repairRunId));
  }
  private RepairRun fixMissingRepairRunTables(RepairRun repairRun, RepairUnit repairUnit) throws ReaperException {
    if (repairRun.getTables().isEmpty()) {
      RepairRun newRepairRun = repairRun
          .with()
          .tables(RepairUnitService.create(context).getTablesToRepair(cluster, repairUnit))
          .build(repairRun.getId());
      repairRunDao.updateRepairRun(newRepairRun, Optional.of(false));
      return newRepairRun;
    }
    return repairRun;
  }
  public Cluster getCluster() {
    return this.cluster;
  }
}
package io.cassandrareaper.management;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.RepairType;
import io.cassandrareaper.core.Snapshot;
import io.cassandrareaper.core.Table;
import io.cassandrareaper.resources.view.NodesStatus;
import io.cassandrareaper.service.RingRange;
import java.io.IOException;
import java.math.BigInteger;
import java.net.UnknownHostException;
import java.time.Duration;
import java.util.Collection;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.ExecutionException;
import javax.management.JMException;
import javax.management.openmbean.CompositeData;
import javax.validation.constraints.NotNull;
import com.datastax.driver.core.VersionNumber;
import org.apache.cassandra.repair.RepairParallelism;
public interface ICassandraManagementProxy {
  Duration DEFAULT_JMX_CONNECTION_TIMEOUT = Duration.ofSeconds(5);
  long KB_FACTOR = 1000;
  long KIB_FACTOR = 1024;
  long MB_FACTOR = 1000 * KB_FACTOR;
  long GB_FACTOR = 1000 * MB_FACTOR;
  long MIB_FACTOR = 1024 * KIB_FACTOR;
  long GIB_FACTOR = 1024 * MIB_FACTOR;
  void cancelAllRepairs();
  String getCassandraVersion();
  String getClusterName();
  @NotNull
  Map<String, String> getEndpointToHostId();
  String getLocalEndpoint() throws ReaperException;
  String getHost();
  List<String> getKeyspaces();
  List<String> getLiveNodes() throws ReaperException;
  String getPartitioner() throws ReaperException;
  int getPendingCompactions() throws ReaperException;
  Map<List<String>, List<String>> getRangeToEndpointMap(String keyspace) throws ReaperException;
  Set<Table> getTablesForKeyspace(String keyspace) throws ReaperException;
  List<BigInteger> getTokens();
  boolean isRepairRunning() throws JMException;
  Map<String, List<String>> listTablesByKeyspace() throws ReaperException;
  int triggerRepair(
      String keyspace,
      RepairParallelism repairParallelism,
      Collection<String> columnFamilies,
      RepairType repairType,
      Collection<String> datacenters,
      RepairStatusHandler repairStatusHandler,
      List<RingRange> associatedTokens,
      int repairThreadCount)
      throws ReaperException;
  void removeRepairStatusHandler(int repairNo);
  void clearSnapshot(String var1, String... var2) throws IOException;
  List<Snapshot> listSnapshots() throws UnsupportedOperationException;
  void takeSnapshot(String var1, String... var2) throws IOException;
  void takeColumnFamilySnapshot(String var1, String var2, String var3) throws IOException;
  Map<String, String> getTokenToEndpointMap();
  void forceKeyspaceCompaction(boolean splitOutput, String keyspaceName, String... columnFamilies) throws IOException,
      ExecutionException,
      InterruptedException;
  List<Map<String, String>> getCompactions();
  NodesStatus getNodesStatus() throws ReaperException;
  String getDatacenter(String var1) throws UnknownHostException;
  Set<CompositeData> getCurrentStreams();
  static Integer versionCompare(String str1, String str2) {
    VersionNumber version1 = VersionNumber.parse(str1);
    VersionNumber version2 = VersionNumber.parse(str2);
    return version1.compareTo(version2);
  }
  String getUntranslatedHost() throws ReaperException;
  static double parseHumanReadableSize(String readableSize) {
    int spaceNdx = readableSize.indexOf(" ");
    double ret = readableSize.contains(".")
        ? Double.parseDouble(readableSize.substring(0, spaceNdx))
        : Double.parseDouble(readableSize.substring(0, spaceNdx).replace(",", "."));
    switch (readableSize.substring(spaceNdx + 1)) {
      case "GB":
        return ret * GB_FACTOR;
      case "GiB":
        return ret * GIB_FACTOR;
      case "MB":
        return ret * MB_FACTOR;
      case "MiB":
        return ret * MIB_FACTOR;
      case "KB":
        return ret * KB_FACTOR;
      case "KiB":
        return ret * KIB_FACTOR;
      case "bytes":
        return ret;
      default:
        return 0;
    }
  }
}
package io.cassandrareaper.service;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.RepairRun;
import io.cassandrareaper.core.RepairSegment;
import io.cassandrareaper.core.RepairUnit;
import io.cassandrareaper.core.Segment;
import io.cassandrareaper.core.Table;
import io.cassandrareaper.management.ClusterFacade;
import io.cassandrareaper.management.EndpointSnitchInfoProxy;
import io.cassandrareaper.management.ICassandraManagementProxy;
import io.cassandrareaper.storage.repairrun.IRepairRunDao;
import java.math.BigInteger;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Optional;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.ConcurrentHashMap;
import java.util.stream.Collectors;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.CharMatcher;
import com.google.common.base.Preconditions;
import com.google.common.base.Splitter;
import com.google.common.base.Supplier;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import org.apache.cassandra.repair.RepairParallelism;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import static com.google.common.base.Preconditions.checkNotNull;
public final class RepairRunService {
  public static final Splitter COMMA_SEPARATED_LIST_SPLITTER
      = Splitter.on(',').trimResults(CharMatcher.anyOf(" ()[]\"'")).omitEmptyStrings();
  public static final int DEFAULT_SEGMENT_COUNT_PER_NODE = 64;
  private static final Logger LOG = LoggerFactory.getLogger(RepairRunService.class);
  private final AppContext context;
  private final RepairUnitService repairUnitService;
  private final ClusterFacade clusterFacade;
  private final IRepairRunDao repairRunDao;
  private RepairRunService(AppContext context,
                           Supplier<ClusterFacade> clusterFacadeSupplier,
                           IRepairRunDao repairRunDao) {
    this.context = context;
    this.repairUnitService = RepairUnitService.create(context);
    this.clusterFacade = clusterFacadeSupplier.get();
    this.repairRunDao = repairRunDao;
  }
  @VisibleForTesting
  static RepairRunService create(AppContext context, Supplier<ClusterFacade> supplier,
                                 IRepairRunDao repairRunDao) throws ReaperException {
    return new RepairRunService(context, supplier, repairRunDao);
  }
  public static RepairRunService create(AppContext context, IRepairRunDao repairRunDao) {
    return new RepairRunService(context, () -> ClusterFacade.create(context), repairRunDao);
  }
  public static void sortByRunState(List<RepairRun> repairRunCollection) {
    Comparator<RepairRun> comparator = new Comparator<RepairRun>() {
      @Override
      public int compare(RepairRun o1, RepairRun o2) {
        if (!o1.getRunState().isTerminated() && o2.getRunState().isTerminated()) {
          return -1; 
        } else if (o1.getRunState().isTerminated() && !o2.getRunState().isTerminated()) {
          return 1; 
        } else { 
          return o1.getId().compareTo(o2.getId());
        }
      }
    };
    Collections.sort(repairRunCollection, comparator);
  }
  static int computeGlobalSegmentCount(
      int segmentCountPerNode,
      Map<String, List<RingRange>> endpointToRange) {
    Preconditions.checkArgument(1 <= endpointToRange.keySet().size());
    return endpointToRange.keySet().size()
        * (segmentCountPerNode != 0 ? segmentCountPerNode : DEFAULT_SEGMENT_COUNT_PER_NODE);
  }
  static List<Segment> filterSegmentsByNodes(
      List<Segment> segments,
      RepairUnit repairUnit,
      Map<String, List<RingRange>> endpointToRange) {
    if (repairUnit.getNodes().isEmpty()) {
      return segments;
    } else {
      return segments
          .stream()
          .filter(
              segment -> {
                RingRange firstRange = segment.getBaseRange();
                for (Entry<String, List<RingRange>> entry : endpointToRange.entrySet()) {
                  if (repairUnit.getNodes().contains(entry.getKey())) {
                    for (RingRange range : entry.getValue()) {
                      if (range.encloses(firstRange)) {
                        return true;
                      }
                    }
                  }
                }
                return false;
              })
          .collect(Collectors.toList());
    }
  }
  @VisibleForTesting
  static Map<String, List<RingRange>> buildEndpointToRangeMap(Map<List<String>, List<String>> rangeToEndpoint) {
    Map<String, List<RingRange>> endpointToRange = new ConcurrentHashMap<>();
    for (Entry<List<String>, List<String>> entry : rangeToEndpoint.entrySet()) {
      RingRange range = new RingRange(entry.getKey().toArray(new String[entry.getKey().size()]));
      for (String endpoint : entry.getValue()) {
        List<RingRange> ranges = endpointToRange.getOrDefault(endpoint, Lists.newArrayList());
        ranges.add(range);
        endpointToRange.put(endpoint, ranges);
      }
    }
    return endpointToRange;
  }
  @VisibleForTesting
  static Map<List<String>, List<RingRange>> buildReplicasToRangeMap(
      Map<List<String>, List<String>> rangeToEndpoint) {
    Map<List<String>, List<RingRange>> replicasToRange = new ConcurrentHashMap<>();
    for (Entry<List<String>, List<String>> entry : rangeToEndpoint.entrySet()) {
      RingRange range = new RingRange(entry.getKey().toArray(new String[entry.getKey().size()]));
      List<String> sortedReplicas = entry.getValue().stream().sorted().collect(Collectors.toList());
      List<RingRange> ranges = replicasToRange.getOrDefault(sortedReplicas, Lists.newArrayList());
      ranges.add(range);
      replicasToRange.put(sortedReplicas, ranges);
    }
    return replicasToRange;
  }
  private static List<RepairSegment.Builder> createRepairSegments(
      List<Segment> tokenSegments,
      RepairUnit repairUnit) {
    List<RepairSegment.Builder> repairSegmentBuilders = Lists.newArrayList();
    tokenSegments.forEach(
        range -> repairSegmentBuilders.add(RepairSegment.builder(range, repairUnit.getId())));
    return repairSegmentBuilders;
  }
  @VisibleForTesting
  static List<RepairSegment.Builder> createRepairSegmentsForIncrementalRepair(
      Map<String, RingRange> nodes,
      RepairUnit repairUnit,
      Cluster cluster,
      ClusterFacade clusterFacade) throws ReaperException {
    Map<String, String> endpointHostIdMap = clusterFacade.getEndpointToHostId(cluster);
    List<RepairSegment.Builder> repairSegmentBuilders = Lists.newArrayList();
    nodes
        .entrySet()
        .forEach(range -> {
          RepairSegment.Builder segment = RepairSegment.builder(
                  Segment.builder()
                      .withTokenRanges(Arrays.asList(range.getValue()))
                      .build(),
                  repairUnit.getId())
              .withReplicas(Collections.emptyMap())
              .withCoordinatorHost(range.getKey())
              .withHostID(UUID.fromString(endpointHostIdMap.get(range.getKey())));
          repairSegmentBuilders.add(segment);
        });
    return repairSegmentBuilders;
  }
  public static Set<String> getDatacentersToRepairBasedOnParam(Optional<String> datacenters) {
    Set<String> datacentersToRepair = Collections.emptySet();
    if (datacenters.isPresent() && !datacenters.get().isEmpty()) {
      datacentersToRepair = Sets.newHashSet(COMMA_SEPARATED_LIST_SPLITTER.split(datacenters.get()));
    }
    return datacentersToRepair;
  }
  public RepairRun registerRepairRun(
      Cluster cluster,
      RepairUnit repairUnit,
      Optional<String> cause,
      String owner,
      Integer segmentsPerNode,
      RepairParallelism repairParallelism,
      Double intensity,
      Boolean adaptiveSchedule)
      throws ReaperException {
    List<Segment> tokenSegments = repairUnit.getIncrementalRepair() && !repairUnit.getSubrangeIncrementalRepair()
        ? Lists.newArrayList()
        : generateSegments(cluster, segmentsPerNode, repairUnit);
    checkNotNull(tokenSegments, "failed generating repair segments");
    Map<String, RingRange> nodes = getClusterNodes(cluster, repairUnit);
    int segments = repairUnit.getIncrementalRepair() && !repairUnit.getSubrangeIncrementalRepair()
        ? nodes.keySet().size()
        : tokenSegments.size();
    RepairRun.Builder runBuilder = RepairRun.builder(cluster.getName(), repairUnit.getId())
        .intensity(intensity)
        .segmentCount(segments)
        .repairParallelism(repairParallelism)
        .cause(cause.orElse("no cause specified"))
        .owner(owner)
        .tables(repairUnitService.getTablesToRepair(cluster, repairUnit))
        .adaptiveSchedule(adaptiveSchedule);
    List<RepairSegment.Builder> segmentBuilders
        = repairUnit.getIncrementalRepair() && !repairUnit.getSubrangeIncrementalRepair()
        ? createRepairSegmentsForIncrementalRepair(nodes, repairUnit, cluster, clusterFacade)
        : createRepairSegments(tokenSegments, repairUnit);
    RepairRun repairRun = repairRunDao.addRepairRun(runBuilder, segmentBuilders);
    if (null == repairRun) {
      String errMsg = String.format(
          "failed storing repair run for cluster \"%s\", keyspace \"%s\", and column families: %s",
          cluster.getName(), repairUnit.getKeyspaceName(), repairUnit.getColumnFamilies());
      LOG.error(errMsg);
      throw new ReaperException(errMsg);
    }
    return repairRun;
  }
  @VisibleForTesting
  List<Segment> generateSegments(
      Cluster targetCluster, int segmentCountPerNode, RepairUnit repairUnit)
      throws ReaperException {
    List<Segment> segments = Lists.newArrayList();
    Preconditions.checkState(
        targetCluster.getPartitioner().isPresent(),
        "no partitioner for cluster: " + targetCluster.getName());
    SegmentGenerator sg = new SegmentGenerator(targetCluster.getPartitioner().get());
    try {
      List<BigInteger> tokens = clusterFacade.getTokens(targetCluster);
      Map<List<String>, List<String>> rangeToEndpoint
          = clusterFacade.getRangeToEndpointMap(targetCluster, repairUnit.getKeyspaceName());
      Map<String, List<RingRange>> endpointToRange = buildEndpointToRangeMap(rangeToEndpoint);
      Map<List<String>, List<RingRange>> replicasToRange = buildReplicasToRangeMap(rangeToEndpoint);
      String cassandraVersion = clusterFacade.getCassandraVersion(targetCluster);
      int globalSegmentCount = computeGlobalSegmentCount(segmentCountPerNode, endpointToRange);
      segments = filterSegmentsByNodes(
          sg.generateSegments(
              globalSegmentCount,
              tokens,
              repairUnit.getIncrementalRepair() && !repairUnit.getSubrangeIncrementalRepair(),
              replicasToRange,
              cassandraVersion),
          repairUnit,
          endpointToRange);
    } catch (ReaperException e) {
      LOG.warn("couldn't connect to any host: {}, life sucks...", targetCluster.getSeedHosts(), e);
    } catch (IllegalArgumentException e) {
      LOG.error("Couldn't get endpoints for tokens");
      throw new ReaperException("Couldn't get endpoints for tokens", e);
    }
    if (segments.isEmpty() && (!repairUnit.getIncrementalRepair() || repairUnit.getSubrangeIncrementalRepair())) {
      String errMsg = String.format("failed to generate repair segments for cluster \"%s\"", targetCluster.getName());
      LOG.error(errMsg);
      throw new ReaperException(errMsg);
    }
    List<Segment> segmentsWithReplicas = Lists.newArrayList();
    for (Segment segment : segments) {
      segmentsWithReplicas.add(
          Segment.builder()
              .withBaseRange(segment.getBaseRange())
              .withTokenRanges(segment.getTokenRanges())
              .withReplicas(getDCsByNodeForRepairSegment(
                  targetCluster, segment, repairUnit.getKeyspaceName(), repairUnit))
              .build());
    }
    return segmentsWithReplicas;
  }
  Map<String, String> getDCsByNodeForRepairSegment(
      Cluster cluster,
      Segment segment,
      String keyspace,
      RepairUnit repairUnit) throws ReaperException {
    final int maxAttempts = 2;
    for (int attempt = 0; attempt < maxAttempts; attempt++) {
      try {
        ICassandraManagementProxy jmxConnection = clusterFacade.connect(cluster);
        Collection<String> nodes = clusterFacade.tokenRangeToEndpoint(cluster, keyspace, segment);
        Map<String, String> dcByNode = new ConcurrentHashMap<>();
        nodes.forEach(node -> dcByNode.put(node, EndpointSnitchInfoProxy.create(jmxConnection).getDataCenter(node)));
        if (repairUnit.getDatacenters().isEmpty()) {
          return dcByNode;
        } else {
          return dcByNode.entrySet().stream()
              .filter(entry -> repairUnit.getDatacenters().contains(entry.getValue()))
              .collect(Collectors.toMap(entry -> entry.getKey(), entry -> entry.getValue()));
        }
      } catch (RuntimeException e) {
        if (attempt < maxAttempts - 1) {
          LOG.warn("Failed getting replicas for token range {}. Attempt {} of {}",
              segment.getBaseRange(), attempt + 1, maxAttempts, e);
        }
      }
    }
    throw new ReaperException(String.format("Failed getting replicas for token range (%s, %s)",
        segment.getBaseRange().getStart(), segment.getBaseRange().getEnd()));
  }
  @VisibleForTesting
  Map<String, RingRange> getClusterNodes(Cluster targetCluster, RepairUnit repairUnit) throws ReaperException {
    ConcurrentHashMap<String, RingRange> nodesWithRanges = new ConcurrentHashMap<>();
    Map<List<String>, List<String>> rangeToEndpoint = new ConcurrentHashMap<>();
    try {
      rangeToEndpoint
          = clusterFacade
          .getRangeToEndpointMap(targetCluster, repairUnit.getKeyspaceName());
    } catch (ReaperException e) {
      LOG.error("couldn't connect to any host: {}, will try next one", e);
      throw new ReaperException(e);
    }
    for (Entry<List<String>, List<String>> tokenRangeToEndpoint : rangeToEndpoint.entrySet()) {
      String node = tokenRangeToEndpoint.getValue().get(0);
      RingRange range = new RingRange(tokenRangeToEndpoint.getKey().get(0), tokenRangeToEndpoint.getKey().get(1));
      nodesWithRanges.putIfAbsent(node, range);
    }
    return nodesWithRanges;
  }
  public Set<String> getTableNamesBasedOnParam(
      Cluster cluster,
      String keyspace,
      Optional<String> tableNamesParam) throws ReaperException {
    Set<String> knownTables;
    knownTables
        = clusterFacade
        .getTablesForKeyspace(cluster, keyspace)
        .stream()
        .map(Table::getName)
        .collect(Collectors.toSet());
    if (knownTables.isEmpty()) {
      LOG.debug("no known tables for keyspace {} in cluster {}", keyspace, cluster.getName());
      throw new IllegalArgumentException("no column families found for keyspace");
    }
    Set<String> tableNames = Collections.emptySet();
    if (tableNamesParam.isPresent() && !tableNamesParam.get().isEmpty()) {
      tableNames = Sets.newHashSet(COMMA_SEPARATED_LIST_SPLITTER.split(tableNamesParam.get()));
      for (String name : tableNames) {
        if (!knownTables.contains(name)) {
          throw new IllegalArgumentException("keyspace doesn't contain a table named \"" + name + "\"");
        }
      }
    }
    return tableNames;
  }
  public Set<String> getNodesToRepairBasedOnParam(
      Cluster cluster,
      Optional<String> nodesToRepairParam) throws ReaperException {
    Set<String> nodesInCluster;
    nodesInCluster = clusterFacade.getEndpointToHostId(cluster).keySet();
    if (nodesInCluster.isEmpty()) {
      LOG.debug("no nodes found in cluster {}", cluster.getName());
      throw new IllegalArgumentException("no nodes found in cluster");
    }
    Set<String> nodesToRepair = Collections.emptySet();
    if (nodesToRepairParam.isPresent() && !nodesToRepairParam.get().isEmpty()) {
      nodesToRepair = Sets.newHashSet(COMMA_SEPARATED_LIST_SPLITTER.split(nodesToRepairParam.get()));
      for (String node : nodesToRepair) {
        if (!nodesInCluster.contains(node)) {
          throw new IllegalArgumentException(
              "cluster \"" + cluster.getName() + "\" doesn't contain a node named \"" + node + "\"");
        }
      }
    }
    return nodesToRepair;
  }
}
package io.cassandrareaper.service;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.RepairSchedule;
import io.cassandrareaper.core.RepairUnit;
import io.cassandrareaper.management.ClusterFacade;
import io.cassandrareaper.storage.repairrun.IRepairRunDao;
import java.util.Collection;
import java.util.List;
import java.util.Set;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Sets;
import org.joda.time.DateTime;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import static java.lang.String.format;
public final class ClusterRepairScheduler {
  private static final Logger LOG = LoggerFactory.getLogger(ClusterRepairScheduler.class);
  private static final String REPAIR_OWNER = "auto-scheduling";
  private static final String SYSTEM_KEYSPACE_PREFIX = "system";
  private final AppContext context;
  private final RepairUnitService repairUnitService;
  private final RepairScheduleService repairScheduleService;
  public ClusterRepairScheduler(AppContext context, IRepairRunDao repairRunDao) {
    this.context = context;
    this.repairUnitService = RepairUnitService.create(context);
    this.repairScheduleService = RepairScheduleService.create(context, repairRunDao);
  }
  public void scheduleRepairs(Cluster cluster) throws ReaperException {
    List<String> excludedClusters = context
        .config
        .getAutoScheduling()
        .getExcludedClusters()
        .stream()
        .map(Cluster::toSymbolicName)
        .collect(Collectors.toList());
    if (excludedClusters.contains(cluster.getName())) {
      LOG.debug("Not creating schedules for excluded cluster {}.", cluster.getName());
      return;
    }
    AtomicInteger scheduleIndex = new AtomicInteger();
    ScheduledRepairDiffView schedulesDiff = ScheduledRepairDiffView.compareWithExistingSchedules(context, cluster);
    schedulesDiff.keyspacesDeleted().forEach(keyspace -> deleteRepairSchedule(cluster, keyspace));
    schedulesDiff
        .keyspacesWithoutSchedules()
        .stream()
        .filter(keyspace -> keyspaceCandidateForRepair(cluster, keyspace))
        .forEach(
            keyspace
                -> createRepairSchedule(cluster, keyspace, nextActivationStartDate(scheduleIndex.getAndIncrement())));
  }
  private DateTime nextActivationStartDate(int scheduleIndex) {
    DateTime timeBeforeFirstSchedule
        = DateTime.now().plus(context.config.getAutoScheduling().getTimeBeforeFirstSchedule().toMillis());
    if (context.config.getAutoScheduling().hasScheduleSpreadPeriod()) {
      return timeBeforeFirstSchedule.plus(
          scheduleIndex * context.config.getAutoScheduling().getScheduleSpreadPeriod().toMillis());
    }
    return timeBeforeFirstSchedule;
  }
  private void deleteRepairSchedule(Cluster cluster, String keyspace) {
    Collection<RepairSchedule> scheduleCollection
        = context.storage.getRepairScheduleDao().getRepairSchedulesForClusterAndKeyspace(cluster.getName(), keyspace);
    scheduleCollection.forEach(
        repairSchedule -> {
          repairScheduleService.deleteRepairSchedule(repairSchedule.getId());
          LOG.info("Scheduled repair deleted: {}", repairSchedule);
        });
  }
  private boolean keyspaceCandidateForRepair(Cluster cluster, String keyspace) {
    if (keyspace.toLowerCase().startsWith(ClusterRepairScheduler.SYSTEM_KEYSPACE_PREFIX)
        || context.config.getAutoScheduling().getExcludedKeyspaces().contains(keyspace)) {
      LOG.debug("Scheduled repair skipped for system keyspace {} in cluster {}.", keyspace, cluster.getName());
      return false;
    }
    if (repairUnitService.getTableNamesForKeyspace(cluster, keyspace).isEmpty()) {
      LOG.warn(
          "No tables found for keyspace {} in cluster {}. No repair will be scheduled for this keyspace.",
          keyspace,
          cluster.getName());
      return false;
    }
    return true;
  }
  private void createRepairSchedule(Cluster cluster, String keyspace, DateTime nextActivationTime) {
    boolean incrementalRepair = context.config.getAutoScheduling().incremental();
    boolean subrangeIncrementalRepair = context.config.getAutoScheduling().subrangeIncrementalRepair();
    RepairUnit.Builder builder = RepairUnit.builder()
        .clusterName(cluster.getName())
        .keyspaceName(keyspace)
        .incrementalRepair(incrementalRepair)
        .subrangeIncrementalRepair(subrangeIncrementalRepair)
        .repairThreadCount(context.config.getRepairThreadCount())
        .timeout(context.config.getHangingRepairTimeoutMins());
    RepairSchedule repairSchedule = repairScheduleService.storeNewRepairSchedule(
        cluster,
        repairUnitService.getOrCreateRepairUnit(cluster, builder).get(),
        context.config.getScheduleDaysBetween(),
        nextActivationTime,
        REPAIR_OWNER,
        context.config.getSegmentCountPerNode(),
        context.config.getRepairParallelism(),
        context.config.getRepairIntensity(),
        false,
        context.config.getAutoScheduling().isAdaptive(),
        context.config.getAutoScheduling().getPercentUnrepairedThreshold());
    LOG.info("Scheduled repair created: {}", repairSchedule);
  }
  private static class ScheduledRepairDiffView {
    private final ImmutableSet<String> keyspacesThatRequireSchedules;
    private final ImmutableSet<String> keyspacesDeleted;
    ScheduledRepairDiffView(AppContext context, Cluster cluster) throws ReaperException {
      Set<String> allKeyspacesInCluster = keyspacesInCluster(context, cluster);
      Set<String> keyspacesThatHaveSchedules = keyspacesThatHaveSchedules(context, cluster);
      keyspacesThatRequireSchedules
          = Sets.difference(allKeyspacesInCluster, keyspacesThatHaveSchedules).immutableCopy();
      keyspacesDeleted = Sets.difference(keyspacesThatHaveSchedules, allKeyspacesInCluster).immutableCopy();
    }
    static ScheduledRepairDiffView compareWithExistingSchedules(AppContext context, Cluster cluster)
        throws ReaperException {
      return new ScheduledRepairDiffView(context, cluster);
    }
    Set<String> keyspacesWithoutSchedules() {
      return keyspacesThatRequireSchedules;
    }
    Set<String> keyspacesDeleted() {
      return keyspacesDeleted;
    }
    private Set<String> keyspacesThatHaveSchedules(AppContext context, Cluster cluster) {
      Collection<RepairSchedule> currentSchedules = context.storage.getRepairScheduleDao()
          .getRepairSchedulesForCluster(cluster.getName());
      return currentSchedules
          .stream()
          .map(repairSchedule -> context.storage.getRepairUnitDao()
              .getRepairUnit(
                  repairSchedule.getRepairUnitId())
              .getKeyspaceName())
          .collect(Collectors.toSet());
    }
    private Set<String> keyspacesInCluster(AppContext context, Cluster cluster) throws ReaperException {
      List<String> keyspaces = ClusterFacade.create(context).getKeyspaces(cluster);
      if (keyspaces.isEmpty()) {
        String message = format("No keyspace found in cluster %s", cluster.getName());
        LOG.debug(message);
        throw new IllegalArgumentException(message);
      }
      return Sets.newHashSet(keyspaces);
    }
  }
}
package io.cassandrareaper.service;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.RepairRun;
import io.cassandrareaper.core.RepairSchedule;
import io.cassandrareaper.core.RepairUnit;
import io.cassandrareaper.storage.repairrun.IRepairRunDao;
import java.util.Collection;
import java.util.Objects;
import java.util.Optional;
import java.util.UUID;
import com.codahale.metrics.Gauge;
import com.codahale.metrics.MetricRegistry;
import com.google.common.base.Preconditions;
import org.apache.cassandra.repair.RepairParallelism;
import org.joda.time.DateTime;
import static io.cassandrareaper.metrics.MetricNameUtils.cleanId;
import static io.cassandrareaper.metrics.MetricNameUtils.cleanName;
public final class RepairScheduleService {
  public static final String MILLIS_SINCE_LAST_REPAIR_METRIC_NAME = "millisSinceLastRepairForSchedule";
  private final AppContext context;
  private final RepairUnitService repairUnitService;
  private final IRepairRunDao repairRunDao;
  private RepairScheduleService(AppContext context, IRepairRunDao repairRunDao) {
    this.context = context;
    this.repairUnitService = RepairUnitService.create(context);
    registerRepairScheduleMetrics(context.storage.getRepairScheduleDao().getAllRepairSchedules());
    this.repairRunDao = repairRunDao;
  }
  public static RepairScheduleService create(AppContext context, IRepairRunDao repairRunDao) {
    return new RepairScheduleService(context, repairRunDao);
  }
  public Optional<RepairSchedule> conflictingRepairSchedule(Cluster cluster, RepairUnit.Builder repairUnit) {
    Collection<RepairSchedule> repairSchedules = context.storage.getRepairScheduleDao()
        .getRepairSchedulesForClusterAndKeyspace(repairUnit.clusterName, repairUnit.keyspaceName);
    for (RepairSchedule sched : repairSchedules) {
      RepairUnit repairUnitForSched = context.storage.getRepairUnitDao().getRepairUnit(sched.getRepairUnitId());
      Preconditions.checkState(repairUnitForSched.getClusterName().equals(repairUnit.clusterName));
      Preconditions.checkState(repairUnitForSched.getKeyspaceName().equals(repairUnit.keyspaceName));
      if (repairUnitService.conflictingUnits(cluster, repairUnitForSched, repairUnit)) {
        return Optional.of(sched);
      }
    }
    return Optional.empty();
  }
  public Optional<RepairSchedule> identicalRepairUnit(Cluster cluster, RepairUnit.Builder repairUnit) {
    Collection<RepairSchedule> repairSchedules = context.storage.getRepairScheduleDao()
        .getRepairSchedulesForClusterAndKeyspace(repairUnit.clusterName, repairUnit.keyspaceName);
    for (RepairSchedule sched : repairSchedules) {
      RepairUnit repairUnitForSched = context.storage.getRepairUnitDao().getRepairUnit(sched.getRepairUnitId());
      Preconditions.checkState(repairUnitForSched.getClusterName().equals(repairUnit.clusterName));
      Preconditions.checkState(repairUnitForSched.getKeyspaceName().equals(repairUnit.keyspaceName));
      if (repairUnitService.identicalUnits(cluster, repairUnitForSched, repairUnit)) {
        return Optional.of(sched);
      }
    }
    return Optional.empty();
  }
  public RepairSchedule storeNewRepairSchedule(
      Cluster cluster,
      RepairUnit repairUnit,
      int daysBetween,
      DateTime nextActivation,
      String owner,
      int segmentCountPerNode,
      RepairParallelism repairParallelism,
      Double intensity,
      boolean force,
      boolean adaptive,
      int percentUnrepairedThreshold) {
    Preconditions.checkArgument(
        force || !conflictingRepairSchedule(cluster, repairUnit.with()).isPresent(),
        "A repair schedule already exists for cluster \"%s\", keyspace \"%s\", and column families: %s",
        cluster.getName(),
        repairUnit.getKeyspaceName(),
        repairUnit.getColumnFamilies());
    RepairSchedule.Builder scheduleBuilder = RepairSchedule.builder(repairUnit.getId())
        .daysBetween(daysBetween)
        .nextActivation(nextActivation)
        .repairParallelism(repairParallelism)
        .intensity(intensity)
        .segmentCountPerNode(segmentCountPerNode)
        .owner(owner)
        .adaptive(adaptive)
        .percentUnrepairedThreshold(percentUnrepairedThreshold);
    RepairSchedule repairSchedule = context.storage.getRepairScheduleDao().addRepairSchedule(scheduleBuilder);
    registerScheduleMetrics(repairSchedule.getId());
    return repairSchedule;
  }
  public void deleteRepairSchedule(UUID repairScheduleId) {
    unregisterScheduleMetrics(repairScheduleId);
    context.storage.getRepairScheduleDao().deleteRepairSchedule(repairScheduleId);
  }
  private void registerRepairScheduleMetrics(Collection<RepairSchedule> allRepairSchedules) {
    allRepairSchedules.forEach(schedule -> registerScheduleMetrics(schedule.getId()));
  }
  private void registerScheduleMetrics(UUID repairScheduleId) {
    RepairSchedule schedule = context.storage.getRepairScheduleDao().getRepairSchedule(repairScheduleId).get();
    RepairUnit repairUnit = context.storage.getRepairUnitDao().getRepairUnit(schedule.getRepairUnitId());
    String metricName = metricName(MILLIS_SINCE_LAST_REPAIR_METRIC_NAME,
        repairUnit.getClusterName(),
        repairUnit.getKeyspaceName(),
        schedule.getId());
    if (!context.metricRegistry.getMetrics().containsKey(metricName)) {
      context.metricRegistry.register(metricName, getMillisSinceLastRepairForSchedule(schedule.getId()));
    }
  }
  private void unregisterScheduleMetrics(UUID repairScheduleId) {
    Optional<RepairSchedule> schedule = context.storage.getRepairScheduleDao().getRepairSchedule(repairScheduleId);
    schedule.ifPresent(sched -> {
      RepairUnit repairUnit = context.storage.getRepairUnitDao().getRepairUnit(sched.getRepairUnitId());
      String metricName = metricName(MILLIS_SINCE_LAST_REPAIR_METRIC_NAME,
          repairUnit.getClusterName(),
          repairUnit.getKeyspaceName(),
          sched.getId());
      if (context.metricRegistry.getMetrics().containsKey(metricName)) {
        context.metricRegistry.remove(metricName);
      }
    });
  }
  private Gauge<Long> getMillisSinceLastRepairForSchedule(UUID repairSchedule) {
    return () -> {
      Optional<RepairSchedule> schedule = context.storage.getRepairScheduleDao().getRepairSchedule(repairSchedule);
      Optional<UUID> latestRepairUuid = Optional.ofNullable(schedule.orElseThrow(() ->
              new IllegalArgumentException("Repair schedule not found"))
          .getLastRun());
      Long millisSinceLastRepair = latestRepairUuid.map(uuid -> repairRunDao.getRepairRun(uuid))
          .filter(Optional::isPresent)
          .map(Optional::get)
          .map(RepairRun::getEndTime)
          .filter(Objects::nonNull)
          .map(dateTime -> DateTime.now().getMillis() - dateTime.getMillis())
          .orElse(DateTime.now().getMillis()); 
      return millisSinceLastRepair;
    };
  }
  private String metricName(String metric, String clusterName, String keyspaceName, UUID scheduleId) {
    return MetricRegistry.name(RepairScheduleService.class,
        metric, cleanName(clusterName), cleanName(keyspaceName), cleanId(scheduleId));
  }
}
package io.cassandrareaper.resources;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.RepairRun;
import io.cassandrareaper.core.RepairRun.RunState;
import io.cassandrareaper.core.RepairSegment;
import io.cassandrareaper.core.RepairUnit;
import io.cassandrareaper.management.ClusterFacade;
import io.cassandrareaper.management.ICassandraManagementProxy;
import io.cassandrareaper.resources.view.RepairRunStatus;
import io.cassandrareaper.service.PurgeService;
import io.cassandrareaper.service.RepairRunService;
import io.cassandrareaper.service.RepairUnitService;
import io.cassandrareaper.storage.repairrun.IRepairRunDao;
import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.UUID;
import java.util.stream.Collectors;
import javax.annotation.Nullable;
import javax.validation.ValidationException;
import javax.ws.rs.DELETE;
import javax.ws.rs.GET;
import javax.ws.rs.POST;
import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.PathParam;
import javax.ws.rs.Produces;
import javax.ws.rs.QueryParam;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;
import javax.ws.rs.core.Response.Status;
import javax.ws.rs.core.UriInfo;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import org.apache.cassandra.repair.RepairParallelism;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import static java.lang.Math.min;
@Path("/repair_run")
@Produces(MediaType.APPLICATION_JSON)
public final class RepairRunResource {
  private static final Logger LOG = LoggerFactory.getLogger(RepairRunResource.class);
  private final AppContext context;
  private final RepairUnitService repairUnitService;
  private final RepairRunService repairRunService;
  private final IRepairRunDao repairRunDao;
  public RepairRunResource(AppContext context, IRepairRunDao repairRunDao) {
    this.context = context;
    this.repairUnitService = RepairUnitService.create(context);
    this.repairRunService = RepairRunService.create(context, repairRunDao);
    this.repairRunDao = repairRunDao;
  }
  @Nullable
  static Response checkRequestForAddRepair(
      AppContext context,
      Optional<String> clusterName,
      Optional<String> keyspace,
      Optional<String> tableNamesParam,
      Optional<String> owner,
      Optional<Integer> segmentCountPerNode,
      Optional<String> repairParallelism,
      Optional<String> intensityStr,
      Optional<String> incrementalRepairStr,
      Optional<String> subrangeIncrementalStr,
      Optional<String> nodesStr,
      Optional<String> datacentersStr,
      Optional<String> blacklistedTableNamesParam,
      Optional<Integer> repairThreadCountStr,
      Optional<String> forceParam,
      Optional<Integer> timeoutParam) throws ReaperException {
    if (!clusterName.isPresent()) {
      return createMissingArgumentResponse("clusterName");
    }
    if (!keyspace.isPresent()) {
      return createMissingArgumentResponse("keyspace");
    }
    if (!owner.isPresent()) {
      return createMissingArgumentResponse("owner");
    }
    if (segmentCountPerNode.isPresent()
        && (segmentCountPerNode.get() < 0 || segmentCountPerNode.get() > 1000)) {
      return Response.status(Response.Status.BAD_REQUEST)
          .entity("invalid query parameter \"segmentCountPerNode\", maximum value is 100000")
          .build();
    }
    if (repairParallelism.isPresent()) {
      try {
        checkRepairParallelismString(repairParallelism.get());
      } catch (ReaperException ex) {
        LOG.error(ex.getMessage(), ex);
        return Response.status(Response.Status.BAD_REQUEST).entity(ex.getMessage()).build();
      }
    }
    if (intensityStr.isPresent()) {
      try {
        parseIntensity(intensityStr.get());
      } catch (ValidationException ex) {
        return Response.status(Status.BAD_REQUEST).entity(ex.getMessage()).build();
      }
    }
    if (incrementalRepairStr.isPresent()
        && (!incrementalRepairStr.get().toUpperCase().contentEquals("TRUE")
        && !incrementalRepairStr.get().toUpperCase().contentEquals("FALSE"))) {
      return Response.status(Response.Status.BAD_REQUEST)
          .entity("invalid query parameter \"incrementalRepair\", expecting [True,False]")
          .build();
    }
    if (subrangeIncrementalStr.isPresent()
        && (!subrangeIncrementalStr.get().toUpperCase().contentEquals("TRUE")
        && !subrangeIncrementalStr.get().toUpperCase().contentEquals("FALSE"))) {
      return Response.status(Response.Status.BAD_REQUEST)
          .entity("invalid query parameter \"subrangeIncrementalStr\", expecting [True,False]")
          .build();
    }
    try {
      Cluster cluster = context.storage.getClusterDao().getCluster(Cluster.toSymbolicName(clusterName.get()));
      if (!datacentersStr.orElse("").isEmpty() && !nodesStr.orElse("").isEmpty()) {
        return Response.status(Response.Status.BAD_REQUEST)
            .entity("Parameters \"datacenters\" and \"nodes\" are mutually exclusive.")
            .build();
      }
      if (subrangeIncrementalStr.isPresent() && "true".equalsIgnoreCase(subrangeIncrementalStr.get())) {
        try {
          String version = ClusterFacade.create(context).getCassandraVersion(cluster);
          if (null != version && ICassandraManagementProxy.versionCompare(version, "4.0.0") < 0) {
            String msg = "Subrange incremental repair does not work with Cassandra versions before 4.0.0";
            return Response.status(Response.Status.BAD_REQUEST).entity(msg).build();
          }
        } catch (ReaperException e) {
          String msg = String.format("find version of cluster %s failed", cluster.getName());
          LOG.error(msg, e);
          return Response.serverError().entity(msg).build();
        }
      } else if (incrementalRepairStr.isPresent() && "true".equalsIgnoreCase(incrementalRepairStr.get())) {
        try {
          String version = ClusterFacade.create(context).getCassandraVersion(cluster);
          if (null != version && ICassandraManagementProxy.versionCompare(version, "2.1.0") < 0) {
            String msg = "Incremental repair does not work with Cassandra versions before 2.1";
            return Response.status(Response.Status.BAD_REQUEST).entity(msg).build();
          }
        } catch (ReaperException e) {
          String msg = String.format("find version of cluster %s failed", cluster.getName());
          LOG.error(msg, e);
          return Response.serverError().entity(msg).build();
        }
      }
    } catch (IllegalArgumentException ex) {
      return Response.status(Response.Status.NOT_FOUND)
          .entity("No cluster found with name \"" + clusterName.get() + "\"")
          .build();
    }
    if (tableNamesParam.isPresent() && blacklistedTableNamesParam.isPresent()) {
      return Response.status(Response.Status.BAD_REQUEST)
          .entity("invalid to specify a table list and a blacklist")
          .build();
    }
    if (forceParam.isPresent()
        && (!forceParam.get().toUpperCase().contentEquals("TRUE")
        && !forceParam.get().toUpperCase().contentEquals("FALSE"))) {
      return Response.status(Response.Status.BAD_REQUEST)
          .entity("invalid query parameter \"force\", expecting [True,False]")
          .build();
    }
    if (timeoutParam.isPresent()
        && timeoutParam.get() == 0) {
      return Response.status(Response.Status.BAD_REQUEST)
          .entity("invalid query parameter \"timeout\", should be higher than 0")
          .build();
    }
    return null;
  }
  private static boolean isStarting(RepairRun.RunState oldState, RepairRun.RunState newState) {
    return oldState == RepairRun.RunState.NOT_STARTED && newState == RepairRun.RunState.RUNNING;
  }
  private static boolean isPausing(RepairRun.RunState oldState, RepairRun.RunState newState) {
    return oldState == RepairRun.RunState.RUNNING && newState == RepairRun.RunState.PAUSED;
  }
  private static boolean isResuming(RepairRun.RunState oldState, RepairRun.RunState newState) {
    return oldState == RepairRun.RunState.PAUSED && newState == RepairRun.RunState.RUNNING;
  }
  private static boolean isRetrying(RepairRun.RunState oldState, RepairRun.RunState newState) {
    return oldState == RepairRun.RunState.ERROR && newState == RepairRun.RunState.RUNNING;
  }
  private static boolean isAborting(RepairRun.RunState oldState, RepairRun.RunState newState) {
    return oldState != RepairRun.RunState.ERROR && newState == RepairRun.RunState.ABORTED;
  }
  private static URI buildRepairRunUri(UriInfo uriInfo, RepairRun repairRun) {
    return uriInfo.getBaseUriBuilder().path("repair_run").path(repairRun.getId().toString()).build();
  }
  static Set<String> splitStateParam(Optional<String> state) {
    if (state.isPresent()) {
      final Iterable<String> chunks = RepairRunService.COMMA_SEPARATED_LIST_SPLITTER.split(state.get());
      for (final String chunk : chunks) {
        try {
          RepairRun.RunState.valueOf(chunk.toUpperCase());
        } catch (IllegalArgumentException e) {
          LOG.warn("Listing repair runs called with erroneous states: {}", state.get(), e);
          return null;
        }
      }
      return Sets.newHashSet(chunks);
    } else {
      return Sets.newHashSet();
    }
  }
  private static void checkRepairParallelismString(String repairParallelism) throws ReaperException {
    try {
      RepairParallelism.valueOf(repairParallelism.toUpperCase());
    } catch (IllegalArgumentException ex) {
      throw new ReaperException(
          "invalid repair parallelism given \""
              + repairParallelism
              + "\", must be one of: "
              + Arrays.toString(RepairParallelism.values()),
          ex);
    }
  }
  private static Response createMissingArgumentResponse(String argumentName) {
    return Response.status(Status.BAD_REQUEST).entity(argumentName + " argument missing").build();
  }
  private static RunState parseRunState(String input) throws ValidationException {
    try {
      return RunState.valueOf(input.toUpperCase());
    } catch (IllegalArgumentException ex) {
      throw new ValidationException("invalid \"state\" argument: " + input, ex);
    }
  }
  private static double parseIntensity(String input) throws ValidationException {
    try {
      double intensity = Double.parseDouble(input);
      if (intensity <= 0.0 || intensity > 1.0) {
        throw new ValidationException("query parameter \"intensity\" must be in range (0.0, 1.0]: " + input);
      }
      return intensity;
    } catch (NumberFormatException ex) {
      throw new ValidationException("invalid value for query parameter \"intensity\": " + input, ex);
    }
  }
  @POST
  public Response addRepairRun(
      @Context UriInfo uriInfo,
      @QueryParam("clusterName") Optional<String> clusterName,
      @QueryParam("keyspace") Optional<String> keyspace,
      @QueryParam("tables") Optional<String> tableNamesParam,
      @QueryParam("owner") Optional<String> owner,
      @QueryParam("cause") Optional<String> cause,
      @QueryParam("segmentCountPerNode") Optional<Integer> segmentCountPerNode,
      @QueryParam("repairParallelism") Optional<String> repairParallelism,
      @QueryParam("intensity") Optional<String> intensityStr,
      @QueryParam("incrementalRepair") Optional<String> incrementalRepairStr,
      @QueryParam("subrangeIncrementalRepair") Optional<String> subrangeIncrementalRepairStr,
      @QueryParam("nodes") Optional<String> nodesToRepairParam,
      @QueryParam("datacenters") Optional<String> datacentersToRepairParam,
      @QueryParam("blacklistedTables") Optional<String> blacklistedTableNamesParam,
      @QueryParam("repairThreadCount") Optional<Integer> repairThreadCountParam,
      @QueryParam("force") Optional<String> forceParam,
      @QueryParam("timeout") Optional<Integer> timeoutParam) {
    try {
      final Response possibleFailedResponse
          = RepairRunResource.checkRequestForAddRepair(
          context,
          clusterName,
          keyspace,
          tableNamesParam,
          owner,
          segmentCountPerNode,
          repairParallelism,
          intensityStr,
          incrementalRepairStr,
          subrangeIncrementalRepairStr,
          nodesToRepairParam,
          datacentersToRepairParam,
          blacklistedTableNamesParam,
          repairThreadCountParam,
          forceParam,
          timeoutParam);
      if (null != possibleFailedResponse) {
        return possibleFailedResponse;
      }
      final Cluster cluster = context.storage.getClusterDao().getCluster(Cluster.toSymbolicName(clusterName.get()));
      Set<String> tableNames;
      try {
        tableNames = repairRunService.getTableNamesBasedOnParam(cluster, keyspace.get(), tableNamesParam);
      } catch (IllegalArgumentException ex) {
        LOG.error(ex.getMessage(), ex);
        return Response.status(Response.Status.NOT_FOUND).entity(ex.getMessage()).build();
      }
      Set<String> blacklistedTableNames;
      try {
        blacklistedTableNames
            = repairRunService.getTableNamesBasedOnParam(cluster, keyspace.get(), blacklistedTableNamesParam);
      } catch (IllegalArgumentException ex) {
        LOG.error(ex.getMessage(), ex);
        return Response.status(Response.Status.NOT_FOUND).entity(ex.getMessage()).build();
      }
      final Set<String> nodesToRepair;
      try {
        nodesToRepair = repairRunService.getNodesToRepairBasedOnParam(cluster, nodesToRepairParam);
      } catch (IllegalArgumentException ex) {
        LOG.error(ex.getMessage(), ex);
        return Response.status(Response.Status.NOT_FOUND).entity(ex.getMessage()).build();
      }
      final Set<String> datacentersToRepair;
      try {
        datacentersToRepair = RepairRunService
            .getDatacentersToRepairBasedOnParam(datacentersToRepairParam);
      } catch (IllegalArgumentException ex) {
        LOG.error(ex.getMessage(), ex);
        return Response.status(Response.Status.NOT_FOUND).entity(ex.getMessage()).build();
      }
      int timeout = timeoutParam.orElse(context.config.getHangingRepairTimeoutMins());
      boolean force = (forceParam.isPresent() ? Boolean.parseBoolean(forceParam.get()) : false);
      boolean subrangeIncrementalRepair = getSubrangeIncrementalRepair(subrangeIncrementalRepairStr);
      boolean incrementalRepair = getIncrementalRepair(incrementalRepairStr);
      RepairUnit.Builder builder = RepairUnit.builder()
          .clusterName(cluster.getName())
          .keyspaceName(keyspace.get())
          .columnFamilies(tableNames)
          .incrementalRepair(incrementalRepair)
          .subrangeIncrementalRepair(subrangeIncrementalRepair)
          .nodes(nodesToRepair)
          .datacenters(datacentersToRepair)
          .blacklistedTables(blacklistedTableNames)
          .repairThreadCount(repairThreadCountParam.orElse(context.config.getRepairThreadCount()))
          .timeout(timeout);
      final Optional<RepairUnit> maybeTheRepairUnit = repairUnitService.getOrCreateRepairUnit(cluster, builder, force);
      if (maybeTheRepairUnit.isPresent()) {
        RepairUnit theRepairUnit = maybeTheRepairUnit.get();
        if (theRepairUnit.getIncrementalRepair() != incrementalRepair
            && theRepairUnit.getSubrangeIncrementalRepair() != subrangeIncrementalRepair) {
          String msg = String.format(
              "A repair unit %s already exist for the same cluster/keyspace/tables"
                  + " but with a different incremental repair value. Requested value %s | Existing value: %s",
              theRepairUnit.getId(),
              incrementalRepair,
              theRepairUnit.getIncrementalRepair());
          return Response.status(Response.Status.CONFLICT).entity(msg).build();
        }
        RepairParallelism parallelism = context.config.getRepairParallelism();
        if (repairParallelism.isPresent()) {
          LOG.debug(
              "using given repair parallelism {} instead of configured value {}",
              repairParallelism.get(),
              context.config.getRepairParallelism());
          parallelism = RepairParallelism.valueOf(repairParallelism.get().toUpperCase());
        }
        if (incrementalRepair || subrangeIncrementalRepair) {
          parallelism = RepairParallelism.PARALLEL;
        }
        Double intensity = getIntensity(intensityStr);
        int segments = getSegments(segmentCountPerNode, subrangeIncrementalRepair, incrementalRepair);
        final RepairRun newRepairRun = repairRunService.registerRepairRun(
            cluster,
            theRepairUnit,
            cause,
            owner.get(),
            segments,
            parallelism,
            intensity,
            false);
        return Response.created(buildRepairRunUri(uriInfo, newRepairRun))
            .entity(new RepairRunStatus(newRepairRun, theRepairUnit, 0))
            .build();
      } else {
        return Response.status(Response.Status.CONFLICT)
            .entity("An existing repair unit conflicts with your repair run.")
            .build();
      }
    } catch (ReaperException e) {
      LOG.error(e.getMessage(), e);
      return Response.serverError().entity(e.getMessage()).build();
    }
  }
  @VisibleForTesting
  public int getSegments(Optional<Integer> segmentCountPerNode, boolean subrangeIncrementalRepair,
      boolean incrementalRepair) {
    int segments = context.config.getSegmentCountPerNode();
    if (!incrementalRepair || subrangeIncrementalRepair) {
      if (segmentCountPerNode.isPresent()) {
        LOG.debug(
            "using given segment count {} instead of configured value {}",
            segmentCountPerNode.get(),
            context.config.getSegmentCount());
        segments = segmentCountPerNode.get();
      }
    } else {
      segments = -1;
    }
    return segments;
  }
  @VisibleForTesting
  public boolean getIncrementalRepair(Optional<String> incrementalRepairStr) {
    boolean incrementalRepair;
    if (incrementalRepairStr.isPresent()) {
      incrementalRepair = Boolean.parseBoolean(incrementalRepairStr.get());
    } else {
      incrementalRepair = context.config.getIncrementalRepair();
      LOG.debug("no incremental repair given, so using default value: {}", incrementalRepair);
    }
    return incrementalRepair;
  }
  @VisibleForTesting
  public boolean getSubrangeIncrementalRepair(Optional<String> subrangeIncrementalRepairStr) {
    boolean subrangeIncrementalRepair;
    if (subrangeIncrementalRepairStr.isPresent()) {
      subrangeIncrementalRepair = Boolean.parseBoolean(subrangeIncrementalRepairStr.get());
    } else {
      subrangeIncrementalRepair = context.config.getSubrangeIncrementalRepair();
      LOG.debug("no subrange incremental repair given, so using default value: {}", subrangeIncrementalRepair);
    }
    return subrangeIncrementalRepair;
  }
  @VisibleForTesting
  public Double getIntensity(Optional<String> intensityStr) {
    Double intensity;
    if (intensityStr.isPresent()) {
      intensity = Double.parseDouble(intensityStr.get());
    } else {
      intensity = context.config.getRepairIntensity();
      LOG.debug("no intensity given, so using default value: {}", intensity);
    }
    return intensity;
  }
  @PUT
  @Path("/{id}/state/{state}")
  public Response modifyRunState(
      @Context UriInfo uriInfo,
      @PathParam("id") UUID repairRunId,
      @PathParam("state") Optional<String> stateStr)
      throws ReaperException {
    LOG.info("modify repair run state called with: id = {}, state = {}", repairRunId, stateStr);
    try {
      if (!stateStr.isPresent()) {
        return createMissingArgumentResponse("state");
      }
      Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
      if (!repairRun.isPresent()) {
        return Response.status(Status.NOT_FOUND).entity("repair run " + repairRunId + " doesn't exist").build();
      }
      final RepairRun.RunState newState = parseRunState(stateStr.get());
      final RunState oldState = repairRun.get().getRunState();
      if (oldState == newState) {
        String msg = "given \"state\" " + stateStr + " is same as the current run state";
        return Response.noContent().entity(msg).location(buildRepairRunUri(uriInfo, repairRun.get())).build();
      }
      if ((isStarting(oldState, newState) || isResuming(oldState, newState) || isRetrying(oldState, newState))
          && isUnitAlreadyRepairing(repairRun.get())) {
        String errMsg = "repair unit already has run " + repairRun.get().getRepairUnitId() + " in RUNNING state";
        LOG.error(errMsg);
        return Response.status(Status.CONFLICT).entity(errMsg).build();
      }
      if (isStarting(oldState, newState)) {
        return startRun(uriInfo, repairRun.get());
      } else if (isPausing(oldState, newState)) {
        return pauseRun(uriInfo, repairRun.get());
      } else if (isResuming(oldState, newState) || isRetrying(oldState, newState)) {
        return resumeRun(uriInfo, repairRun.get());
      } else if (isAborting(oldState, newState)) {
        return abortRun(uriInfo, repairRun.get());
      } else {
        String errMsg = String.format("Transition %s->%s not supported.", oldState.toString(), newState.toString());
        LOG.error(errMsg);
        return Response.status(Status.CONFLICT).entity(errMsg).build();
      }
    } catch (ValidationException ex) {
      return Response.status(Status.BAD_REQUEST).entity(ex.getMessage()).build();
    }
  }
  @PUT
  @Path("/{id}/intensity/{intensity}")
  public Response modifyRunIntensity(
      @Context UriInfo uriInfo,
      @PathParam("id") UUID repairRunId,
      @PathParam("intensity") Optional<String> intensityStr)
      throws ReaperException {
    LOG.info("modify repair run intensity called with: id = {}, state = {}", repairRunId, intensityStr);
    try {
      if (!intensityStr.isPresent()) {
        return createMissingArgumentResponse("intensity");
      }
      final double intensity = parseIntensity(intensityStr.get());
      Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
      if (!repairRun.isPresent()) {
        return Response.status(Status.NOT_FOUND).entity("repair run " + repairRunId + " doesn't exist").build();
      }
      if (RunState.PAUSED != repairRun.get().getRunState() && RunState.NOT_STARTED != repairRun.get().getRunState()) {
        return Response.status(Status.CONFLICT).entity("repair run must first be paused").build();
      }
      return updateRunIntensity(uriInfo, repairRun.get(), intensity);
    } catch (ValidationException ex) {
      return Response.status(Response.Status.BAD_REQUEST).entity(ex.getMessage()).build();
    }
  }
  private boolean isUnitAlreadyRepairing(RepairRun repairRun) {
    return repairRunDao.getRepairRunsForUnit(repairRun.getRepairUnitId()).stream()
        .anyMatch((run) -> (!run.getId().equals(repairRun.getId()) && run.getRunState().equals(RunState.RUNNING)));
  }
  private int getSegmentAmountForRepairRun(UUID repairRunId) {
    return context.storage.getRepairSegmentDao().getSegmentAmountForRepairRunWithState(repairRunId,
        RepairSegment.State.DONE);
  }
  private Response startRun(UriInfo uriInfo, RepairRun repairRun) throws ReaperException {
    LOG.info("Starting run {}", repairRun.getId());
    final RepairRun newRun = context.repairManager.startRepairRun(repairRun);
    return Response.ok().location(buildRepairRunUri(uriInfo, newRun)).build();
  }
  private Response pauseRun(UriInfo uriInfo, RepairRun repairRun) throws ReaperException {
    LOG.info("Pausing run {}", repairRun.getId());
    final RepairRun newRun = context.repairManager.pauseRepairRun(repairRun);
    return Response.ok().location(buildRepairRunUri(uriInfo, newRun)).build();
  }
  private Response resumeRun(UriInfo uriInfo, RepairRun repairRun) throws ReaperException {
    LOG.info("Resuming run {}", repairRun.getId());
    final RepairRun newRun = context.repairManager.startRepairRun(repairRun);
    return Response.ok().location(buildRepairRunUri(uriInfo, newRun)).build();
  }
  private Response abortRun(UriInfo uriInfo, RepairRun repairRun) throws ReaperException {
    LOG.info("Aborting run {}", repairRun.getId());
    final RepairRun newRun = context.repairManager.abortRepairRun(repairRun);
    return Response.ok().location(buildRepairRunUri(uriInfo, newRun)).build();
  }
  private Response updateRunIntensity(UriInfo uriInfo, RepairRun run, double intensity)
      throws ReaperException {
    LOG.info("Editing run {}", run.getId());
    RepairRun newRun = context.repairManager.updateRepairRunIntensity(run, intensity);
    return Response.ok().location(buildRepairRunUri(uriInfo, newRun)).build();
  }
  @GET
  @Path("/{id}")
  public Response getRepairRun(
      @PathParam("id") UUID repairRunId) {
    LOG.debug("get repair_run called with: id = {}", repairRunId);
    final Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
    if (repairRun.isPresent()) {
      RepairRunStatus repairRunStatus = getRepairRunStatus(repairRun.get());
      return Response.ok().entity(repairRunStatus).build();
    } else {
      return Response.status(404).entity("repair run " + repairRunId + " doesn't exist").build();
    }
  }
  @GET
  @Path("/{id}/segments")
  public Response getRepairRunSegments(@PathParam("id") UUID repairRunId) {
    LOG.debug("get repair_run called with: id = {}", repairRunId);
    final Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
    if (repairRun.isPresent()) {
      Collection<RepairSegment> segments = context.storage.getRepairSegmentDao().getRepairSegmentsForRun(repairRunId);
      return Response.ok().entity(segments).build();
    } else {
      return Response.status(404).entity("repair run " + repairRunId + " doesn't exist").build();
    }
  }
  @POST
  @Path("/{id}/segments/abort/{segment_id}")
  public Response abortRepairRunSegment(@PathParam("id") UUID repairRunId, @PathParam("segment_id") UUID segmentId) {
    LOG.debug("abort segment called with: run id = {} and segment id = {}", repairRunId, segmentId);
    final Optional<RepairRun> repairRun = repairRunDao.getRepairRun(repairRunId);
    if (repairRun.isPresent()) {
      if (RepairRun.RunState.RUNNING == repairRun.get().getRunState()
          || RepairRun.RunState.PAUSED == repairRun.get().getRunState()) {
        try {
          RepairSegment segment = context.repairManager.abortSegment(repairRunId, segmentId);
          return Response.ok().entity(segment).build();
        } catch (ReaperException ex) {
          return Response.status(Response.Status.CONFLICT)
              .entity("Cannot abort segment " + ex.getLocalizedMessage())
              .build();
        }
      } else {
        return Response.status(Response.Status.CONFLICT)
            .entity("Cannot abort segment on repair run with status " + repairRun.get().getRunState())
            .build();
      }
    } else {
      return Response.status(404).entity("repair run " + repairRunId + " doesn't exist").build();
    }
  }
  @GET
  @Path("/cluster/{clusterName}")
  public Response getRepairRunsForCluster(
      @PathParam("clusterName") String clusterName,
      @QueryParam("limit") Optional<Integer> limit) {
    LOG.debug("get repair run for cluster called with: cluster_name = {}", clusterName);
    final Collection<RepairRun> repairRuns = repairRunDao
        .getRepairRunsForClusterPrioritiseRunning(clusterName, limit);
    final Collection<RepairRunStatus> repairRunViews = new ArrayList<>();
    for (final RepairRun repairRun : repairRuns) {
      repairRunViews.add(getRepairRunStatus(repairRun));
    }
    return Response.ok().entity(repairRunViews).build();
  }
  private RepairRunStatus getRepairRunStatus(RepairRun repairRun) {
    RepairUnit repairUnit = context.storage.getRepairUnitDao().getRepairUnit(repairRun.getRepairUnitId());
    int segmentsRepaired = getSegmentAmountForRepairRun(repairRun.getId());
    return new RepairRunStatus(repairRun, repairUnit, segmentsRepaired);
  }
  @GET
  public Response listRepairRuns(
      @QueryParam("state") Optional<String> state,
      @QueryParam("cluster_name") Optional<String> cluster,
      @QueryParam("keyspace_name") Optional<String> keyspace,
      @QueryParam("limit") Optional<Integer> limit) {
    try {
      final Set<String> desiredStates = splitStateParam(state);
      if (desiredStates == null) {
        return Response.status(Response.Status.BAD_REQUEST).build();
      }
      Collection<Cluster> clusters = cluster.isPresent() && !cluster.get().equals("all")
          ? Collections.singleton(context.storage.getClusterDao().getCluster(cluster.get()))
          : context.storage.getClusterDao().getClusters();
      List<RepairRun> repairRuns = Lists.newArrayList();
      clusters.forEach(clstr -> repairRuns.addAll(
          repairRunDao.getRepairRunsForClusterPrioritiseRunning(clstr.getName(), limit))
      );
      List<RepairRunStatus> runStatuses = Lists.newArrayList();
      RepairRunService.sortByRunState(repairRuns);
      runStatuses.addAll(
          getRunStatuses(
              repairRuns.subList(0, min(repairRuns.size(), limit.orElse(1000))), desiredStates)
              .stream()
              .filter((run) -> !keyspace.isPresent()
                  || ((RepairRunStatus) run).getKeyspaceName().equals(keyspace.get()))
              .collect(Collectors.toList()));
      return Response.ok().entity(runStatuses).build();
    } catch (IllegalArgumentException e) {
      return Response.serverError().entity("Failed find cluster " + cluster.get()).build();
    }
  }
  private List<RepairRunStatus> getRunStatuses(Collection<RepairRun> runs, Set<String> desiredStates) {
    final List<RepairRunStatus> runStatuses = Lists.newArrayList();
    for (final RepairRun run : runs) {
      if (!desiredStates.isEmpty() && !desiredStates.contains(run.getRunState().name())) {
        continue;
      }
      RepairUnit runsUnit = context.storage.getRepairUnitDao().getRepairUnit(run.getRepairUnitId());
      int segmentsRepaired = run.getSegmentCount();
      if (!run.getRunState().equals(RepairRun.RunState.DONE)) {
        segmentsRepaired = getSegmentAmountForRepairRun(run.getId());
      }
      runStatuses.add(new RepairRunStatus(run, runsUnit, segmentsRepaired));
    }
    return runStatuses;
  }
  @DELETE
  @Path("/{id}")
  public Response deleteRepairRun(
      @PathParam("id") UUID runId,
      @QueryParam("owner") Optional<String> owner) {
    LOG.info("delete repair run called with runId: {}, and owner: {}", runId, owner);
    if (!owner.isPresent()) {
      return Response.status(Response.Status.BAD_REQUEST)
          .entity("required query parameter \"owner\" is missing")
          .build();
    }
    final Optional<RepairRun> runToDelete = repairRunDao.getRepairRun(runId);
    if (runToDelete.isPresent()) {
      if (RepairRun.RunState.RUNNING == runToDelete.get().getRunState()) {
        return Response.status(Response.Status.CONFLICT)
            .entity("Repair run with id \"" + runId + "\" is currently running, and must be stopped before deleting")
            .build();
      }
      if (!runToDelete.get().getOwner().equalsIgnoreCase(owner.get())) {
        String msg = String.format("Repair run %s is not owned by the user you defined %s", runId, owner.get());
        return Response.status(Response.Status.CONFLICT).entity(msg).build();
      }
      if (context.storage.getRepairSegmentDao().getSegmentAmountForRepairRunWithState(runId,
          RepairSegment.State.RUNNING) > 0) {
        String msg = String.format("Repair run %s has running segments, which must finish before deleting", runId);
        return Response.status(Response.Status.CONFLICT).entity(msg).build();
      }
      repairRunDao.deleteRepairRun(runId);
      return Response.accepted().build();
    }
    try {
      repairRunDao.deleteRepairRun(runId);
    } catch (RuntimeException ignore) {
    }
    return Response.status(Response.Status.NOT_FOUND).entity("Repair run %s" + runId + " not found").build();
  }
  @POST
  @Path("/purge")
  public Response purgeRepairRuns() throws ReaperException {
    int purgedRepairs = PurgeService.create(context, repairRunDao).purgeDatabase();
    return Response.ok().entity(purgedRepairs).build();
  }
}
package io.cassandrareaper.resources;
import io.cassandrareaper.AppContext;
import io.cassandrareaper.ReaperException;
import io.cassandrareaper.core.Cluster;
import io.cassandrareaper.core.EditableRepairSchedule;
import io.cassandrareaper.core.PercentRepairedMetric;
import io.cassandrareaper.core.RepairSchedule;
import io.cassandrareaper.core.RepairUnit;
import io.cassandrareaper.resources.view.RepairRunStatus;
import io.cassandrareaper.resources.view.RepairScheduleStatus;
import io.cassandrareaper.service.RepairRunService;
import io.cassandrareaper.service.RepairScheduleService;
import io.cassandrareaper.service.RepairUnitService;
import io.cassandrareaper.storage.repairrun.IRepairRunDao;
import java.net.URI;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.UUID;
import javax.validation.Valid;
import javax.validation.constraints.NotNull;
import javax.ws.rs.Consumes;
import javax.ws.rs.DELETE;
import javax.ws.rs.GET;
import javax.ws.rs.POST;
import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.PathParam;
import javax.ws.rs.Produces;
import javax.ws.rs.QueryParam;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;
import javax.ws.rs.core.UriInfo;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.dropwizard.jersey.PATCH;
import io.dropwizard.jersey.validation.ValidationErrorMessage;
import org.apache.cassandra.repair.RepairParallelism;
import org.joda.time.DateTime;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
@Path("/repair_schedule")
@Produces(MediaType.APPLICATION_JSON)
public final class RepairScheduleResource {
  private static final Logger LOG = LoggerFactory.getLogger(RepairScheduleResource.class);
  private final AppContext context;
  private final RepairUnitService repairUnitService;
  private final RepairScheduleService repairScheduleService;
  private final RepairRunService repairRunService;
  private final IRepairRunDao repairRunDao;
  public RepairScheduleResource(AppContext context, IRepairRunDao repairRunDao) {
    this.context = context;
    this.repairUnitService = RepairUnitService.create(context);
    this.repairScheduleService = RepairScheduleService.create(context, repairRunDao);
    this.repairRunService = RepairRunService.create(context, repairRunDao);
    this.repairRunDao = repairRunDao;
  }
  private static boolean isPausing(RepairSchedule.State oldState, RepairSchedule.State newState) {
    return oldState == RepairSchedule.State.ACTIVE && newState == RepairSchedule.State.PAUSED;
  }
  private static boolean isResuming(RepairSchedule.State oldState, RepairSchedule.State newState) {
    return oldState == RepairSchedule.State.PAUSED && newState == RepairSchedule.State.ACTIVE;
  }
  private static URI buildRepairScheduleUri(UriInfo uriInfo, RepairSchedule repairSchedule) {
    return uriInfo.getBaseUriBuilder().path("repair_schedule").path(repairSchedule.getId().toString()).build();
  }
  protected static RepairSchedule applyRepairPatchParams(
      final RepairSchedule repairSchedule,
      final String owner,
      final RepairParallelism repairParallelism,
      final Double intensity,
      final Integer scheduleDaysBetween,
      final Integer segmentCountPerNode,
      final Boolean adaptive,
      final Integer percentUnrepairedThreshold
  ) {
    if (repairSchedule == null) {
      return null;
    }
    return repairSchedule.with()
        .owner(owner != null ? owner.trim() : repairSchedule.getOwner())
        .repairParallelism(repairParallelism != null ? repairParallelism : repairSchedule.getRepairParallelism())
        .intensity(intensity != null ? intensity : repairSchedule.getIntensity())
        .daysBetween(scheduleDaysBetween != null ? scheduleDaysBetween : repairSchedule.getDaysBetween())
        .segmentCountPerNode(segmentCountPerNode != null
            ? segmentCountPerNode
            : repairSchedule.getSegmentCountPerNode())
        .percentUnrepairedThreshold(percentUnrepairedThreshold)
        .adaptive(adaptive != null ? adaptive : false)
        .build(repairSchedule.getId());
  }
  @PATCH
  @Produces(MediaType.APPLICATION_JSON)
  @Consumes(MediaType.APPLICATION_JSON)
  @Path("/{id}")
  public Response patchRepairSchedule(
      @Context UriInfo uriInfo,
      @PathParam("id") UUID repairScheduleId,
      @NotNull @Valid EditableRepairSchedule editableRepairSchedule
  ) {
    if (repairScheduleId == null) {
      ValidationErrorMessage errorMessage = new ValidationErrorMessage(
          ImmutableList.copyOf(
              Lists.newArrayList("id must not be null or empty")
          )
      );
      return Response.status(400).entity(errorMessage).build();
    }
    if (editableRepairSchedule == null) {
      ValidationErrorMessage errorMessage = new ValidationErrorMessage(
          ImmutableList.copyOf(
              Lists.newArrayList("request body must not be null or empty")
          )
      );
      return Response.status(400).entity(errorMessage).build();
    }
    Optional<RepairSchedule> repairScheduleWrapper = context.storage.getRepairScheduleDao()
        .getRepairSchedule(repairScheduleId);
    RepairSchedule repairSchedule = repairScheduleWrapper.orElse(null);
    if (repairSchedule == null) {
      return Response.status(Response.Status.NOT_FOUND).build();
    }
    RepairSchedule patchedRepairSchedule = applyRepairPatchParams(
        repairSchedule,
        editableRepairSchedule.getOwner(),
        editableRepairSchedule.getRepairParallelism(),
        editableRepairSchedule.getIntensity(),
        editableRepairSchedule.getDaysBetween(),
        editableRepairSchedule.getSegmentCountPerNode(),
        editableRepairSchedule.getAdaptive(),
        editableRepairSchedule.getPercentUnrepairedThreshold()
    );
    boolean updated = context.storage.getRepairScheduleDao().updateRepairSchedule(patchedRepairSchedule);
    if (updated) {
      return Response.status(Response.Status.OK).entity(getRepairScheduleStatus(patchedRepairSchedule)).build();
    } else {
      return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build();
    }
  }
  @POST
  public Response addRepairSchedule(
      @Context UriInfo uriInfo,
      @QueryParam("clusterName") Optional<String> clusterName,
      @QueryParam("keyspace") Optional<String> keyspace,
      @QueryParam("tables") Optional<String> tableNamesParam,
      @QueryParam("owner") Optional<String> owner,
      @QueryParam("segmentCountPerNode") Optional<Integer> segmentCountPerNode,
      @QueryParam("repairParallelism") Optional<String> repairParallelism,
      @QueryParam("intensity") Optional<String> intensityStr,
      @QueryParam("incrementalRepair") Optional<String> incrementalRepairStr,
      @QueryParam("subrangeIncrementalRepair") Optional<String> subrangeIncrementalRepairStr,
      @QueryParam("scheduleDaysBetween") Optional<Integer> scheduleDaysBetween,
      @QueryParam("scheduleTriggerTime") Optional<String> scheduleTriggerTime,
      @QueryParam("nodes") Optional<String> nodesToRepairParam,
      @QueryParam("datacenters") Optional<String> datacentersToRepairParam,
      @QueryParam("blacklistedTables") Optional<String> blacklistedTableNamesParam,
      @QueryParam("repairThreadCount") Optional<Integer> repairThreadCountParam,
      @QueryParam("force") Optional<String> forceParam,
      @QueryParam("timeout") Optional<Integer> timeoutParam,
      @QueryParam("adaptive") Optional<String> adaptiveParam,
      @QueryParam("percentUnrepairedThreshold") Optional<Integer> percentUnrepairedParam) {
    try {
      Response possibleFailResponse = RepairRunResource.checkRequestForAddRepair(
          context,
          clusterName,
          keyspace,
          tableNamesParam,
          owner,
          segmentCountPerNode,
          repairParallelism,
          intensityStr,
          incrementalRepairStr,
          subrangeIncrementalRepairStr,
          nodesToRepairParam,
          datacentersToRepairParam,
          blacklistedTableNamesParam,
          repairThreadCountParam,
          forceParam,
          timeoutParam);
      if (null != possibleFailResponse) {
        return possibleFailResponse;
      }
      DateTime nextActivation;
      try {
        nextActivation = getNextActivationTime(scheduleTriggerTime);
        if (nextActivation.isBefore(DateTime.now().minusMinutes(15))) {
          return Response.status(Response.Status.BAD_REQUEST)
              .entity("given schedule_trigger_time is too far in the past: "
                  + RepairRunStatus.dateTimeToIso8601(nextActivation))
              .build();
        }
      } catch (IllegalArgumentException ex) {
        LOG.info("cannot parse data string: {}", scheduleTriggerTime.get(), ex);
        return Response.status(Response.Status.BAD_REQUEST).entity("invalid schedule_trigger_time").build();
      }
      if (!scheduleDaysBetween.isPresent()) {
        return Response.status(Response.Status.BAD_REQUEST)
            .entity("missing required parameter: scheduleDaysBetween")
            .build();
      }
      Cluster cluster = context.storage.getClusterDao().getCluster(Cluster.toSymbolicName(clusterName.get()));
      Set<String> tableNames;
      try {
        tableNames = repairRunService.getTableNamesBasedOnParam(cluster, keyspace.get(), tableNamesParam);
      } catch (IllegalArgumentException ex) {
        LOG.error(ex.getMessage(), ex);
        return Response.status(Response.Status.NOT_FOUND).entity(ex.getMessage()).build();
      }
      Set<String> blacklistedTableNames;
      try {
        blacklistedTableNames
            = repairRunService.getTableNamesBasedOnParam(cluster, keyspace.get(), blacklistedTableNamesParam);
      } catch (IllegalArgumentException ex) {
        LOG.error(ex.getMessage(), ex);
        return Response.status(Response.Status.NOT_FOUND).entity(ex.getMessage()).build();
      }
      final Set<String> nodesToRepair;
      try {
        nodesToRepair = repairRunService.getNodesToRepairBasedOnParam(cluster, nodesToRepairParam);
      } catch (final IllegalArgumentException ex) {
        LOG.error(ex.getMessage(), ex);
        return Response.status(Response.Status.NOT_FOUND).entity(ex.getMessage()).build();
      }
      final Set<String> datacentersToRepair = RepairRunService
          .getDatacentersToRepairBasedOnParam(datacentersToRepairParam);
      boolean incremental
          = isIncrementalRepair(incrementalRepairStr) || isIncrementalRepair(subrangeIncrementalRepairStr);
      RepairParallelism parallelism = context.config.getRepairParallelism();
      if (repairParallelism.isPresent()) {
        LOG.debug("using given repair parallelism {} over configured value {}", repairParallelism.get(), parallelism);
        parallelism = RepairParallelism.valueOf(repairParallelism.get().toUpperCase());
      }
      if (!parallelism.equals(RepairParallelism.PARALLEL) && incremental) {
        return Response.status(Response.Status.BAD_REQUEST)
            .entity("Incremental repairs only supports PARALLEL parallelism mode.")
            .build();
      }
      if (percentUnrepairedParam.orElse(-1) > 0 && !incremental) {
        return Response.status(Response.Status.BAD_REQUEST)
            .entity("Triggering schedules on % unrepaired threshold is only allowed for incremental repairs.")
            .build();
      }
      boolean force = (forceParam.isPresent() ? Boolean.parseBoolean(forceParam.get()) : false);
      int timeout = timeoutParam.orElse(context.config.getHangingRepairTimeoutMins());
      boolean adaptive = (adaptiveParam.isPresent() ? Boolean.parseBoolean(adaptiveParam.get()) : false);
      boolean subrangeIncremental = isIncrementalRepair(subrangeIncrementalRepairStr) ;
      RepairUnit.Builder unitBuilder = RepairUnit.builder()
          .clusterName(cluster.getName())
          .keyspaceName(keyspace.get())
          .columnFamilies(tableNames)
          .incrementalRepair(incremental)
          .subrangeIncrementalRepair(subrangeIncremental)
          .nodes(nodesToRepair)
          .datacenters(datacentersToRepair)
          .blacklistedTables(blacklistedTableNames)
          .repairThreadCount(repairThreadCountParam.orElse(context.config.getRepairThreadCount()))
          .timeout(timeout);
      return addRepairSchedule(
          cluster,
          unitBuilder,
          getDaysBetween(scheduleDaysBetween),
          owner.get(),
          parallelism,
          uriInfo,
          incremental,
          nextActivation,
          getSegmentCount(segmentCountPerNode),
          getIntensity(intensityStr),
          force,
          adaptive,
          percentUnrepairedParam.orElse(-1));
    } catch (ReaperException e) {
      LOG.error(e.getMessage(), e);
      return Response.serverError().entity(e.getMessage()).build();
    }
  }
  private Response addRepairSchedule(
      Cluster cluster,
      RepairUnit.Builder unitBuilder,
      int days,
      String owner,
      RepairParallelism parallel,
      UriInfo uriInfo,
      boolean incremental,
      DateTime next,
      int segments,
      Double intensity,
      boolean force,
      boolean adaptive,
      int percentUnrepairedThreshold) {
    Optional<RepairSchedule> conflictingRepairSchedule
        = repairScheduleService.identicalRepairUnit(cluster, unitBuilder);
    if (conflictingRepairSchedule.isPresent()) {
      return Response.noContent().location(buildRepairScheduleUri(uriInfo, conflictingRepairSchedule.get())).build();
    }
    conflictingRepairSchedule = repairScheduleService.conflictingRepairSchedule(cluster, unitBuilder);
    if (conflictingRepairSchedule.isPresent()) {
      RepairSchedule existingSchedule = conflictingRepairSchedule.get();
      if (existingSchedule.getDaysBetween() == days
          && existingSchedule.getOwner().equals(owner)
          && existingSchedule.getRepairParallelism() == parallel) {
        return Response.noContent().location(buildRepairScheduleUri(uriInfo, existingSchedule)).build();
      }
      if (!force) {
        String msg = String.format(
            "A repair schedule already exists for cluster \"%s\", keyspace \"%s\", and column families: %s",
            cluster.getName(),
            unitBuilder.keyspaceName,
            unitBuilder.columnFamilies);
        return Response
            .status(Response.Status.CONFLICT)
            .location(buildRepairScheduleUri(uriInfo, existingSchedule))
            .entity(msg)
            .build();
      }
    }
    Optional<RepairUnit> maybeUnit = repairUnitService.getOrCreateRepairUnit(cluster, unitBuilder, force);
    if (maybeUnit.isPresent()) {
      RepairUnit unit = maybeUnit.get();
      Preconditions
          .checkState(unit.getIncrementalRepair() == incremental
              || unit.getSubrangeIncrementalRepair() == incremental,
              "%s!=%s", unit.getIncrementalRepair(), incremental);
      Preconditions
          .checkState((percentUnrepairedThreshold > 0 && incremental) || percentUnrepairedThreshold <= 0,
              "Setting a % repaired threshold can only be done on incremental schedules");
      RepairSchedule newRepairSchedule = repairScheduleService
          .storeNewRepairSchedule(
              cluster, unit, days, next, owner, segments,
              parallel, intensity, force, adaptive, percentUnrepairedThreshold);
      return Response.created(buildRepairScheduleUri(uriInfo, newRepairSchedule)).build();
    }
    return Response
        .status(Response.Status.NO_CONTENT)
        .entity("Repair schedule couldn't be created as an existing repair unit seems to conflict with it.")
        .build();
  }
  private int getDaysBetween(Optional<Integer> scheduleDaysBetween) {
    int daysBetween = context.config.getScheduleDaysBetween();
    if (scheduleDaysBetween.isPresent()) {
      LOG.debug(
          "using given schedule days between {} instead of configured value {}",
          scheduleDaysBetween.get(),
          context.config.getScheduleDaysBetween());
      daysBetween = scheduleDaysBetween.get();
    }
    return daysBetween;
  }
  private int getSegmentCount(Optional<Integer> segmentCount) {
    int segments = 0;
    if (segmentCount.isPresent()) {
      LOG.debug("using given segment count {}", segmentCount.get());
      segments = segmentCount.get();
    }
    return segments;
  }
  private Boolean isIncrementalRepair(Optional<String> incrementalRepairStr) {
    Boolean incrementalRepair;
    if (incrementalRepairStr.isPresent()) {
      incrementalRepair = Boolean.parseBoolean(incrementalRepairStr.get());
    } else {
      incrementalRepair = context.config.getIncrementalRepair();
      LOG.debug("no incremental repair given, so using default value: {}", incrementalRepair);
    }
    return incrementalRepair;
  }
  private Double getIntensity(Optional<String> intensityStr) throws NumberFormatException {
    Double intensity;
    if (intensityStr.isPresent()) {
      intensity = Double.parseDouble(intensityStr.get());
    } else {
      intensity = context.config.getRepairIntensity();
      LOG.debug("no intensity given, so using default value: {}", intensity);
    }
    return intensity;
  }
  @PUT
  @Path("/{id}")
  public Response modifyState(
      @Context UriInfo uriInfo,
      @PathParam("id") UUID repairScheduleId,
      @QueryParam("state") Optional<String> state) {
    LOG.info("modify repair schedule state called with: id = {}, state = {}", repairScheduleId, state);
    if (!state.isPresent()) {
      return Response.status(Response.Status.BAD_REQUEST.getStatusCode()).entity("\"state\" argument missing").build();
    }
    Optional<RepairSchedule> repairSchedule = context.storage.getRepairScheduleDao()
        .getRepairSchedule(repairScheduleId);
    if (!repairSchedule.isPresent()) {
      return Response.status(Response.Status.NOT_FOUND)
          .entity("repair schedule with id " + repairScheduleId + " not found")
          .build();
    }
    RepairUnit repairUnit = context.storage.getRepairUnitDao().getRepairUnit(repairSchedule.get().getRepairUnitId());
    RepairSchedule.State newState;
    try {
      newState = RepairSchedule.State.valueOf(state.get().toUpperCase());
    } catch (IllegalArgumentException ex) {
      LOG.error(ex.getMessage(), ex);
      return Response.status(Response.Status.BAD_REQUEST.getStatusCode())
          .entity("invalid \"state\" argument: " + state.get())
          .build();
    }
    RepairSchedule.State oldState = repairSchedule.get().getState();
    if (oldState == newState) {
      return Response.noContent().location(buildRepairScheduleUri(uriInfo, repairSchedule.get())).build();
    }
    if (isPausing(oldState, newState)) {
      return pauseSchedule(repairSchedule.get(), uriInfo);
    } else if (isResuming(oldState, newState)) {
      return resumeSchedule(repairSchedule.get(), uriInfo);
    } else {
      String errMsg = String.format("Transition %s->%s not supported.", oldState.toString(), newState.toString());
      LOG.error(errMsg);
      return Response.status(Response.Status.BAD_REQUEST).entity(errMsg).build();
    }
  }
  private Response pauseSchedule(RepairSchedule repairSchedule, UriInfo uriInfo) {
    LOG.info("Pausing schedule {}", repairSchedule.getId());
    context.schedulingManager.pauseRepairSchedule(repairSchedule);
    return Response.ok().location(buildRepairScheduleUri(uriInfo, repairSchedule)).build();
  }
  private Response resumeSchedule(RepairSchedule repairSchedule, UriInfo uriInfo) {
    LOG.info("Resuming schedule {}", repairSchedule.getId());
    context.schedulingManager.resumeRepairSchedule(repairSchedule);
    return Response.ok().location(buildRepairScheduleUri(uriInfo, repairSchedule)).build();
  }
  @GET
  @Path("/{id}")
  public Response getRepairSchedule(
      @PathParam("id") UUID repairScheduleId) {
    LOG.debug("get repair_schedule called with: id = {}", repairScheduleId);
    Optional<RepairSchedule> repairSchedule = context.storage.getRepairScheduleDao()
        .getRepairSchedule(repairScheduleId);
    if (repairSchedule.isPresent()) {
      return Response.ok().entity(getRepairScheduleStatus(repairSchedule.get())).build();
    } else {
      return Response.status(404).entity("repair schedule with id " + repairScheduleId + " doesn't exist").build();
    }
  }
  @POST
  @Path("/start/{id}")
  public Response startRepairSchedule(@PathParam("id") UUID repairScheduleId) {
    LOG.debug("start repair_schedule called with: id = {}", repairScheduleId);
    Optional<RepairSchedule> repairSchedule = context.storage.getRepairScheduleDao()
        .getRepairSchedule(repairScheduleId);
    if (repairSchedule.isPresent()) {
      RepairSchedule newSchedule = repairSchedule.get()
          .with()
          .nextActivation(DateTime.now())
          .build(repairScheduleId);
      context.storage.getRepairScheduleDao().updateRepairSchedule(newSchedule);
      return Response.ok().entity(getRepairScheduleStatus(newSchedule)).build();
    } else {
      return Response.status(404)
          .entity("repair schedule with id " + repairScheduleId + " doesn't exist")
          .build();
    }
  }
  @GET
  @Path("/cluster/{cluster_name}")
  public Response getRepairSchedulesForCluster(
      @PathParam("cluster_name") String clusterName) {
    LOG.debug("get repair schedules for cluster called with: cluster_name = {}", clusterName);
    Collection<RepairSchedule> repairSchedules = context.storage.getRepairScheduleDao()
        .getRepairSchedulesForCluster(clusterName);
    Collection<RepairScheduleStatus> repairScheduleViews = new ArrayList<>();
    for (RepairSchedule repairSchedule : repairSchedules) {
      repairScheduleViews.add(getRepairScheduleStatus(repairSchedule));
    }
    return Response.ok().entity(repairScheduleViews).build();
  }
  private RepairScheduleStatus getRepairScheduleStatus(RepairSchedule repairSchedule) {
    RepairUnit repairUnit = context.storage.getRepairUnitDao().getRepairUnit(repairSchedule.getRepairUnitId());
    return new RepairScheduleStatus(repairSchedule, repairUnit);
  }
  @GET
  public Response listSchedules(
      @QueryParam("clusterName") Optional<String> clusterName,
      @QueryParam("keyspace") Optional<String> keyspaceName) {
    List<RepairScheduleStatus> scheduleStatuses = Lists.newArrayList();
    getScheduleList(clusterName, keyspaceName).forEach((schedule) -> {
      RepairUnit unit = context.storage.getRepairUnitDao().getRepairUnit(schedule.getRepairUnitId());
      scheduleStatuses.add(new RepairScheduleStatus(schedule, unit));
    });
    return Response.ok().entity(scheduleStatuses).build();
  }
  private Collection<RepairSchedule> getScheduleList(Optional<String> clusterName, Optional<String> keyspaceName) {
    Collection<RepairSchedule> schedules;
    if (clusterName.isPresent() && keyspaceName.isPresent()) {
      schedules = context.storage.getRepairScheduleDao()
          .getRepairSchedulesForClusterAndKeyspace(clusterName.get(), keyspaceName.get());
    } else if (clusterName.isPresent()) {
      schedules = context.storage.getRepairScheduleDao().getRepairSchedulesForCluster(clusterName.get());
    } else if (keyspaceName.isPresent()) {
      schedules = context.storage.getRepairScheduleDao().getRepairSchedulesForKeyspace(keyspaceName.get());
    } else {
      schedules = context.storage.getRepairScheduleDao().getAllRepairSchedules();
    }
    return schedules;
  }
  @DELETE
  @Path("/{id}")
  public Response deleteRepairSchedule(
      @PathParam("id") UUID repairScheduleId,
      @QueryParam("owner") Optional<String> owner) {
    LOG.info("delete repair schedule called with repairScheduleId: {}, and owner: {}", repairScheduleId, owner);
    if (!owner.isPresent()) {
      return Response.status(Response.Status.BAD_REQUEST)
          .entity("required query parameter \"owner\" is missing")
          .build();
    }
    Optional<RepairSchedule> scheduleToDelete = context.storage.getRepairScheduleDao()
        .getRepairSchedule(repairScheduleId);
    if (scheduleToDelete.isPresent()) {
      if (RepairSchedule.State.ACTIVE == scheduleToDelete.get().getState()) {
        String msg = String.format("Repair schedule %s currently running. Must be first stopped", repairScheduleId);
        return Response.status(Response.Status.CONFLICT).entity(msg).build();
      }
      if (!scheduleToDelete.get().getOwner().equalsIgnoreCase(owner.get())) {
        String msg = String.format("Repair schedule %s is not owned by %s", repairScheduleId, owner.get());
        return Response.status(Response.Status.CONFLICT).entity(msg).build();
      }
      repairScheduleService.deleteRepairSchedule(repairScheduleId);
      return Response.accepted().build();
    }
    return Response.status(Response.Status.NOT_FOUND)
        .entity("Repair schedule with id \"" + repairScheduleId + "\" not found")
        .build();
  }
  private DateTime getNextActivationTime(Optional<String> scheduleTriggerTime)
      throws IllegalArgumentException {
    DateTime nextActivation;
    if (scheduleTriggerTime.isPresent()) {
      nextActivation = DateTime.parse(scheduleTriggerTime.get());
      LOG.info("first schedule activation will be: {}", RepairRunStatus.dateTimeToIso8601(nextActivation));
    } else {
      nextActivation = DateTime.now().plusDays(1).withTimeAtStartOfDay();
      LOG.info(
          "no schedule_trigger_time given, so setting first scheduling next night: {}",
          RepairRunStatus.dateTimeToIso8601(nextActivation));
    }
    return nextActivation;
  }
  @GET
  @Path("/{clusterName}/{id}/percent_repaired")
  public List<PercentRepairedMetric> getPercentRepairedMetricsForSchedule(
      @PathParam("clusterName") String clusterName,
      @PathParam("id") UUID repairScheduleId) throws IllegalArgumentException {
    long since = DateTime.now().minusHours(1).getMillis();
    return context.storage.getPercentRepairedMetrics(clusterName, repairScheduleId, since);
  }
}