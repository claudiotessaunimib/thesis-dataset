/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import java.util.Collections;
import java.util.Iterator;
/**
 * Standard Deviation aggregator.
 * Can compute without storing all of the data points in memory at the same
 * time.  This implementation is based upon a
 * <a href="http://www.johndcook.com/standard_deviation.html">paper by John
 * D. Cook</a>, which itself is based upon a method that goes back to a 1962
 * paper by B.  P. Welford and is presented in Donald Knuth's Art of
 * Computer Programming, Vol 2, page 232, 3rd edition
 *
 * Converts all longs to double. This will cause a loss of precision for very large long values.
*/
@FeatureComponent(
        name="dev",
		description = "Calculates the standard deviation of the time series."
)
public class StdAggregator extends RangeAggregator
{
	public enum Dev
	{
		POS_SD, NEG_SD, VALUE
	};
	private DoubleDataPointFactory m_dataPointFactory;
	private Dev m_dev;
	private int m_devCount = 1;
	@Inject
	public StdAggregator(DoubleDataPointFactory dataPointFactory)
	{
		m_dataPointFactory = dataPointFactory;
	}
	/**
	 Sets which type of value to return.
	 @param dev
	 */
	public void setReturnType(Dev dev)
	{
		m_dev = dev;
	}
	public void setDevCount(int count)
	{
		m_devCount = count;
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return DataPoint.GROUP_NUMBER.equals(groupType);
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return m_dataPointFactory.getGroupType();
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new StdDataPointAggregator());
	}
	private class StdDataPointAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			int count = 0;
			double average = 0;
			double pwrSumAvg = 0;
			double stdDev = 0;
			while (dataPointRange.hasNext())
			{
				count++;
				DataPoint dp = dataPointRange.next();
				average += (dp.getDoubleValue() - average) / count;
				pwrSumAvg += (dp.getDoubleValue() * dp.getDoubleValue() - pwrSumAvg) / count;
				stdDev = Math.sqrt((pwrSumAvg * count - count * average * average) / (count - 1));
			}
			if (Double.isNaN(stdDev))
				stdDev = 0;
			double ret = 0;
			if (m_dev == Dev.POS_SD)
				ret = average + (stdDev * m_devCount);
			else if (m_dev == Dev.NEG_SD)
				ret = average - (stdDev * m_devCount);
			else
				ret = stdDev;
			return Collections.singletonList(m_dataPointFactory.createDataPoint(returnTime, ret));
		}
	}
}
package org.kairosdb.core;
import org.json.JSONException;
import org.json.JSONWriter;
import org.kairosdb.core.datastore.DataPointGroup;
import java.io.DataOutput;
import java.io.IOException;
public interface DataPoint
{
	public static final String API_LONG = "long";
	public static final String API_DOUBLE = "double";
	public static final String GROUP_NUMBER = "number";
	public long getTimestamp();
	public void setTimestamp(long timestamp);
	public void writeValueToBuffer(DataOutput buffer) throws IOException;
	public void writeValueToJson(JSONWriter writer) throws JSONException;
	/**
		This is used to identify the data type on the wire in json format
	 @return api data type used in json
	 */
	public String getApiDataType();
	/**
	 This is used to identify the data type in the data store.
	 The reason this is different from api data type is you may want to provide
	 a new implementation for storing long values.  So the api type may be 'long'
	 but the data store type may be 'long2'.  this way going forward new
	 incoming long values will be stored as 'long2' but you can still read both
	 'long' and 'long2' from the data store.
	 @return data store type
	 */
	public String getDataStoreDataType();
	public boolean isLong();
	public long getLongValue();
	public boolean isDouble();
	public double getDoubleValue();
	public DataPointGroup getDataPointGroup();
	public void setDataPointGroup(DataPointGroup dataPointGroup);
}
/*
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import org.joda.time.Chronology;
import org.joda.time.DateTime;
import org.joda.time.DateTimeField;
import org.joda.time.DateTimeZone;
import org.joda.time.chrono.GregorianChronology;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureCompoundProperty;
import org.kairosdb.core.annotation.FeatureProperty;
import org.kairosdb.core.datastore.DataPointGroup;
import org.kairosdb.core.datastore.TimeUnit;
import org.kairosdb.plugin.Aggregator;
import javax.validation.Valid;
import javax.validation.constraints.NotNull;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Iterator;
import java.util.TimeZone;
import static java.util.Objects.requireNonNull;
public abstract class RangeAggregator implements Aggregator, TimezoneAware
{
	private long m_startTime = 0L;
	private long m_queryStartTime = 0L;
	private long m_queryEndTime = 0L;
	private boolean m_started = false;
	private boolean m_exhaustive;
	private DateTimeZone m_timeZone = DateTimeZone.UTC;
	private boolean m_alignSampling = true;
	/* used for generic range computations */
	private DateTimeField m_unitField;
	@NotNull
	@Valid
	@FeatureCompoundProperty(
			name = "sampling",
			label = "Sampling",
			order = {"Value", "Unit"}
	)
	protected Sampling m_sampling = new Sampling(1, TimeUnit.MILLISECONDS);
	@FeatureProperty(
			name = "align_start_time",
			label = "Align start time",
			description = "Setting this to true will cause the aggregation range to be aligned based on the sampling"
					+ " size. For example if your sample size is either milliseconds, seconds, minutes or hours then the"
					+ " start of the range will always be at the top of the hour. The effect of setting this to true is"
					+ " that your data will take the same shape when graphed as you refresh the data. Note that"
					+ " align_sampling, align_start_time, and align_end_time are mutually exclusive. If more than one"
					+ " are set, unexpected results will occur.",
			default_value = "false"
	)
	protected boolean m_alignStartTime;
	@FeatureProperty(
			name = "align_end_time",
			label = "Align end time",
			description = "Setting this to true will cause the aggregation range to be aligned based on the sampling"
					+ " size. For example if your sample size is either milliseconds, seconds, minutes or hours then the"
					+ " start of the range will always be at the top of the hour. The difference between align_start_time"
					+ " and align_end_time is that align_end_time sets the timestamp for the datapoint to the beginning of"
					+ " the following period versus the beginning of the current period. As with align_start_time, setting"
					+ " this to true will cause your data to take the same shape when graphed as you refresh the data. Note"
					+ " that align_sampling, align_start_time, and align_end_time are mutually exclusive. If more than one"
					+ " are set, unexpected results will occur.",
			default_value = "false"
	)
	protected boolean m_alignEndTime;
	@FeatureProperty(
			name = "trim",
			label = "Trim",
			description = " Trim off empty ranges before and after the actual data",
			default_value = "false"
	)
	protected boolean m_trim;
	public RangeAggregator()
	{
		this(false);
	}
	public RangeAggregator(boolean exhaustive)
	{
		m_exhaustive = exhaustive;
	}
	public void init()
	{
		Chronology chronology = GregorianChronology.getInstance(m_timeZone);
		TimeUnit tu = m_sampling.getUnit();
		switch (tu)
		{
			case YEARS:
				m_unitField = chronology.year();
				break;
			case MONTHS:
				m_unitField = chronology.monthOfYear();
				break;
			case WEEKS:
				m_unitField = chronology.weekOfWeekyear();
				break;
			case DAYS:
				m_unitField = chronology.dayOfMonth();
				break;
			case HOURS:
				m_unitField = chronology.hourOfDay();
				break;
			case MINUTES:
				m_unitField = chronology.minuteOfHour();
				break;
			case SECONDS:
				m_unitField = chronology.secondOfDay();
				break;
			default:
				m_unitField = chronology.millisOfSecond();
				break;
		}
		if (m_alignSampling)
			m_startTime = alignRangeBoundary(m_startTime);
	}
	public DataPointGroup aggregate(DataPointGroup dataPointGroup)
	{
		requireNonNull(dataPointGroup);
		if (m_exhaustive)
			return (new ExhaustiveRangeDataPointAggregator(dataPointGroup, getSubAggregator()));
		else
			return (new RangeDataPointAggregator(dataPointGroup, getSubAggregator()));
	}
	/**
	 For YEARS, MONTHS, WEEKS, DAYS:
	 Computes the timestamp of the first millisecond of the day
	 of the timestamp.
	 For HOURS,
	 Computes the timestamp of the first millisecond of the hour
	 of the timestamp.
	 For MINUTES,
	 Computes the timestamp of the first millisecond of the minute
	 of the timestamp.
	 For SECONDS,
	 Computes the timestamp of the first millisecond of the second
	 of the timestamp.
	 For MILLISECONDS,
	 returns the timestamp
	 @param timestamp
	 @return
	 */
	@SuppressWarnings("fallthrough")
	private long alignRangeBoundary(long timestamp)
	{
		DateTime dt = new DateTime(timestamp, m_timeZone);
		TimeUnit tu = m_sampling.getUnit();
		switch (tu)
		{
			case YEARS:
				dt = dt.withDayOfYear(1).withMillisOfDay(0);
				break;
			case MONTHS:
				dt = dt.withDayOfMonth(1).withMillisOfDay(0);
				break;
			case WEEKS:
				dt = dt.withDayOfWeek(1).withMillisOfDay(0);
				break;
			case DAYS:
			case HOURS:
			case MINUTES:
			case SECONDS:
				dt = dt.withHourOfDay(0);
				dt = dt.withMinuteOfHour(0);
				dt = dt.withSecondOfMinute(0);
			default:
				dt = dt.withMillisOfSecond(0);
				break;
		}
		return dt.getMillis();
	}
	public void setSampling(Sampling sampling)
	{
		m_sampling = sampling;
	}
	/**
	 When set to true the time for the aggregated data point for each range will
	 fall on the start of the range instead of being the value for the first
	 data point within that range.
	 @param align
	 */
	public void setAlignStartTime(boolean align)
	{
		m_alignStartTime = align;
	}
	/**
	 When set to true the time for the aggregated data point for each range will
	 fall on the end of the range instead of being the value for the first
	 data point within that range.
	 @param align
	 */
	public void setAlignEndTime(boolean align)
	{
		m_alignEndTime = align;
	}
	/**
	 Setting this to true will cause the aggregation range to be aligned based on
	 the sampling size.  For example if your sample size is either milliseconds,
	 seconds, minutes or hours then the start of the range will always be at the top
	 of the hour.  The effect of setting this to true is that your data will
	 take the same shape when graphed as you refresh the data.
	 @param align Set to true to align the range on fixed points instead of
	 the start of the query.
	 */
	public void setAlignSampling(boolean align)
	{
		m_alignSampling = align;
	}
	public boolean is_alignSampling()
	{
		return m_alignSampling;
	}
	/**
	 Start time to calculate the ranges from.  Typically this is the start
	 of the query
	 @param startTime
	 */
	public void setStartTime(long startTime)
	{
		m_startTime = startTime;
		m_queryStartTime = startTime;
	}
	public void setEndTime(long endTime)
	{
		//This is to tell the exhaustive agg when to stop.
		//If the end time is not specified in the query the end time is
		//set to MAX_LONG which causes exhaustive agg to go on forever.
		long now = System.currentTimeMillis();
		if (endTime > now)
			m_queryEndTime = now;
		else
			m_queryEndTime = endTime;
	}
	public void setTrim(boolean trim)
	{
		m_trim = trim;
	}
	/**
	 Return a RangeSubAggregator that will be used to aggregate data over a
	 discrete range of data points.  This is called once per grouped data series.
	 <p>
	 For example, if one metric is queried and no grouping is done this method is
	 called once and the resulting object is called over and over for each range
	 within the results.
	 <p>
	 If the query were grouping by the host tag and host has values of 'A' and 'B'
	 this method will be called twice, once to aggregate results for 'A' and once
	 to aggregate results for 'B'.
	 @return
	 */
	protected abstract RangeSubAggregator getSubAggregator();
	/**
	 Sets the time zone to use for range calculations
	 @param timeZone
	 */
	public void setTimeZone(DateTimeZone timeZone)
	{
		m_timeZone = timeZone;
	}
	public Sampling getSampling()
	{
		return m_sampling;
	}
	public long getStartRange(long timestamp)
	{
		long samplingValue = m_sampling.getValue();
		long numberOfPastPeriods = m_unitField.getDifferenceAsLong(timestamp/*getDataPointTime()*/, m_startTime) / samplingValue;
		return m_unitField.add(m_startTime, numberOfPastPeriods * samplingValue);
	}
	public long getEndRange(long timestamp)
	{
		long samplingValue = m_sampling.getValue();
		long numberOfPastPeriods = m_unitField.getDifferenceAsLong(timestamp/*getDataPointTime()*/, m_startTime) / samplingValue;
		return m_unitField.add(m_startTime, (numberOfPastPeriods + 1) * samplingValue);
	}
	//===========================================================================
	/**
	 *
	 */
	private class RangeDataPointAggregator extends AggregatedDataPointGroupWrapper
	{
		protected RangeSubAggregator m_subAggregator;
		protected Calendar m_calendar = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
		protected Iterator<DataPoint> m_dpIterator;
		public RangeDataPointAggregator(DataPointGroup innerDataPointGroup,
				RangeSubAggregator subAggregator)
		{
			super(innerDataPointGroup);
			m_subAggregator = subAggregator;
			m_dpIterator = new ArrayList<DataPoint>().iterator();
		}
		@Override
		public DataPoint next()
		{
			if (!m_dpIterator.hasNext())
			{
				//We calculate start and end ranges as the ranges may not be
				//consecutive if data does not show up in each range.
				//long startRange = getStartRange(currentDataPoint.getTimestamp());
				long endRange = getEndRange(currentDataPoint.getTimestamp());
				SubRangeIterator subIterator = new SubRangeIterator(
						endRange);
				m_dpIterator = m_subAggregator.getNextDataPoints(getDataPointTime(),
						subIterator).iterator();
			}
			return (m_dpIterator.next());
		}
		/**
		 Computes the data point time for the aggregated value.
		 Different strategies could be added here such as
		 datapoint time = range start time
		 = range end time
		 = range median
		 = current datapoint time
		 @return
		 */
		private long getDataPointTime()
		{
			long datapointTime = currentDataPoint.getTimestamp();
			if (m_alignStartTime)
			{
				datapointTime = getStartRange(datapointTime);
			}
			else if (m_alignEndTime)
			{
				datapointTime = getEndRange(datapointTime);
			}
			return datapointTime;
		}
		/**
		 @return true if there is a subrange left
		 */
		@Override
		public boolean hasNext()
		{
			return (m_dpIterator.hasNext() || super.hasNext());
		}
		//========================================================================
		/**
		 This class provides an iterator over a discrete range of data points
		 */
		protected class SubRangeIterator implements Iterator<DataPoint>
		{
			private long m_endRange;
			public SubRangeIterator(long endRange)
			{
				m_endRange = endRange;
			}
			@Override
			public boolean hasNext()
			{
				return ((currentDataPoint != null) && (currentDataPoint.getTimestamp() < m_endRange));
			}
			@Override
			public DataPoint next()
			{
				DataPoint ret = currentDataPoint;
				if (hasNextInternal())
					currentDataPoint = nextInternal();
				return (ret);
			}
			@Override
			public void remove()
			{
				throw new UnsupportedOperationException();
			}
		}
	}
	//========================================================================
	private class ExhaustiveRangeDataPointAggregator extends RangeDataPointAggregator
	{
		private long m_nextExpectedRangeStartTime;
		public ExhaustiveRangeDataPointAggregator(DataPointGroup innerDataPointGroup, RangeSubAggregator subAggregator)
		{
			super(innerDataPointGroup, subAggregator);
			if (m_trim)
				m_nextExpectedRangeStartTime = m_startTime;
			else
				m_nextExpectedRangeStartTime = m_queryStartTime;
		}
		private void setNextStartTime(long timeStamp)
		{
			m_nextExpectedRangeStartTime = timeStamp;
		}
		@Override
		public boolean hasNext()
		{
			if (m_trim)
				return super.hasNext();
			else
				return (super.hasNext() || m_nextExpectedRangeStartTime <= m_queryEndTime);
		}
		@Override
		public DataPoint next()
		{
			if (!m_dpIterator.hasNext())
			{
				//We calculate start and end ranges as the ranges may not be
				//consecutive if data does not show up in each range.
				long startTime = m_nextExpectedRangeStartTime;
				if (m_trim && !m_started)
				{
					m_started = true;
					startTime = currentDataPoint.getTimestamp();
				}
				long startRange = getStartRange(startTime);
				long endRange = getEndRange(startTime);
				// Next expected range starts just after this end range
				setNextStartTime(endRange);
				SubRangeIterator subIterator = new SubRangeIterator(
						endRange);
				long dataPointTime = Long.MAX_VALUE;
				if (currentDataPoint != null)
					dataPointTime = currentDataPoint.getTimestamp();
				if (m_alignStartTime || endRange <= dataPointTime)
					dataPointTime = startRange;
				m_dpIterator = m_subAggregator.getNextDataPoints(dataPointTime,
						subIterator).iterator();
			}
			return (m_dpIterator.next());
		}
	}
	//===========================================================================
	/**
	 Instances of this object are created once per grouped data series.
	 */
	public interface RangeSubAggregator
	{
		/**
		 Returns an aggregated data point from a range that is passed in
		 as dataPointRange.
		 @param returnTime     Timestamp to use on return data point.  This is currently
		 passing the timestamp of the first data point in the range.
		 @param dataPointRange Range to aggregate over.
		 @return
		 */
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange);
	}
}
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.apache.commons.math3.stat.regression.SimpleRegression;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
@FeatureComponent(
        name = "least_squares",
		description = "Returns a best fit line through the datapoints using the least squares algorithm."
)
public class LeastSquaresAggregator extends RangeAggregator
{
	private DoubleDataPointFactory m_dataPointFactory;
	@Inject
	public LeastSquaresAggregator(DoubleDataPointFactory dataPointFactory)
	{
		m_dataPointFactory = dataPointFactory;
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return DataPoint.GROUP_NUMBER.equals(groupType);
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return m_dataPointFactory.getGroupType();
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return new LeastSquaresDataPointAggregator();
	}
	private class LeastSquaresDataPointAggregator implements RangeSubAggregator
	{
		LeastSquaresDataPointAggregator()
		{
		}
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			long start = -1L;
			long stop = -1L;
			DataPoint first = null;
			DataPoint second = null;
			int count = 0;
			SimpleRegression simpleRegression = new SimpleRegression(true);
			while (dataPointRange.hasNext())
			{
				count ++;
				DataPoint dp = dataPointRange.next();
				if (second == null)
				{
					if (first == null)
						first = dp;
					else
						second = dp;
				}
				stop = dp.getTimestamp();
				if (start == -1L)
					start = dp.getTimestamp();
				simpleRegression.addData(dp.getTimestamp(), dp.getDoubleValue());
			}
			List<DataPoint> ret = new ArrayList<DataPoint>();
			if (count == 1)
			{
				ret.add(first);
			}
			else if (count == 2)
			{
				ret.add(first);
				ret.add(second);
			}
			else if (count != 0)
			{
				ret.add(m_dataPointFactory.createDataPoint(start, simpleRegression.predict(start)));
				ret.add(m_dataPointFactory.createDataPoint(stop, simpleRegression.predict(stop)));
			}
			return (ret);
		}
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.annotation.FeatureProperty;
import org.kairosdb.core.annotation.ValidationProperty;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import org.kairosdb.core.http.rest.validation.NonZero;
import org.kairosdb.util.Reservoir;
import org.kairosdb.util.UniformReservoir;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.util.Arrays;
import java.util.Collections;
import java.util.Iterator;
import static java.lang.Math.floor;
@FeatureComponent(
		name = "percentile",
		description = "Finds the percentile of the data range."
)
public class PercentileAggregator extends RangeAggregator
{
	public static final Logger logger = LoggerFactory.getLogger(PercentileAggregator.class);
	private DoubleDataPointFactory m_dataPointFactory;
	@Inject
	public PercentileAggregator(DoubleDataPointFactory dataPointFactory)
	{
		m_dataPointFactory = dataPointFactory;
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return DataPoint.GROUP_NUMBER.equals(groupType);
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return m_dataPointFactory.getGroupType();
	}
	@NonZero
	@FeatureProperty(
			label = "Percentile",
			description = "Data points returned will be in this percentile.",
			default_value = "0.1",
            validations =  {
					@ValidationProperty(
							expression = "value > 0",
							message = "Percentile must be greater than 0."
					),
					@ValidationProperty(
							expression = "value < 1",
							message = "Percentile must be smaller than 1."
					)
			}
	)
	private double percentile;
	public void setPercentile(double percentile)
	{
		this.percentile = percentile;
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new PercentileDataPointAggregator());
	}
	private class PercentileDataPointAggregator implements RangeSubAggregator
	{
		private double[] values;
		private Reservoir reservoir;
		private double percentileValue;
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			reservoir = new UniformReservoir();
			while (dataPointRange.hasNext())
			{
				reservoir.update(dataPointRange.next().getDoubleValue());
			}
			getAndSortValues(reservoir.getValues());
			percentileValue = getValue(percentile);
			if (logger.isDebugEnabled())
			{
				logger.debug("Aggregating the " + percentile + " percentile");
			}
			return Collections.singletonList(m_dataPointFactory.createDataPoint(returnTime, percentileValue));
		}
		private void getAndSortValues(double[] values){
			this.values = values;
			Arrays.sort(this.values);
		}
		/**
		 * Returns the value at the given quantile.
		 *
		 * @param quantile    a given quantile, in {@code [0..1]}
		 * @return the value in the distribution at {@code quantile}
		 */
		private double getValue(double quantile) {
			if (quantile < 0.0 || quantile > 1.0) {
				throw new IllegalArgumentException(quantile + " is not in [0..1]");
			}
			if (values.length == 0) {
				return 0.0;
			}
			final double pos = quantile * (values.length + 1);
			if (pos < 1) {
				return values[0];
			}
			if (pos >= values.length) {
				return values[values.length - 1];
			}
			final double lower = values[(int) pos - 1];
			final double upper = values[(int) pos];
			return lower + (pos - floor(pos)) * (upper - lower);
		}
	}
}
package org.kairosdb.core.annotation;
import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;
@Target(ElementType.FIELD)
@Retention(RetentionPolicy.RUNTIME)
public @interface FeatureProperty
{
    String name() default "";
    String label() default "";
    String description();
    boolean optional() default false;
    String type() default "";
    String[] options() default {};
    String default_value() default "";
    String autocomplete() default "";
    boolean multiline() default false;
    ValidationProperty[] validations() default { };
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.util.Collections;
import java.util.Iterator;
/**
 * Converts all longs to double. This will cause a loss of precision for very large long values.
 */
@FeatureComponent(
        name = "sum",
		description = "Adds data points together."
)
public class SumAggregator extends RangeAggregator
{
	public static final Logger logger = LoggerFactory.getLogger(SumAggregator.class);
	private DoubleDataPointFactory m_dataPointFactory;
	@Inject
	public SumAggregator(DoubleDataPointFactory dataPointFactory)
	{
		m_dataPointFactory = dataPointFactory;
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return DataPoint.GROUP_NUMBER.equals(groupType);
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return m_dataPointFactory.getGroupType();
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new SumDataPointAggregator());
	}
	private class SumDataPointAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			double sum = 0;
			int counter = 0;
			while (dataPointRange.hasNext())
			{
				sum += dataPointRange.next().getDoubleValue();
				counter ++;
			}
			if (logger.isDebugEnabled())
			{
				logger.debug("Aggregating "+counter+" values");
			}
			return Collections.singletonList(m_dataPointFactory.createDataPoint(returnTime, sum));
		}
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.datastore.DataPointGroup;
import org.kairosdb.core.groupby.GroupByResult;
import java.util.List;
import java.util.Set;
public abstract class AggregatedDataPointGroupWrapper implements DataPointGroup
{
	protected DataPoint currentDataPoint = null;
	private DataPointGroup innerDataPointGroup;
	public AggregatedDataPointGroupWrapper(DataPointGroup innerDataPointGroup)
	{
		this.innerDataPointGroup = innerDataPointGroup;
		if (innerDataPointGroup.hasNext())
			currentDataPoint = innerDataPointGroup.next();
	}
	@Override
	public String getName()
	{
		return (innerDataPointGroup.getName());
	}
	@Override
	public String getAlias()
	{
		return innerDataPointGroup.getAlias();
	}
	@Override
	public Set<String> getTagNames()
	{
		return (innerDataPointGroup.getTagNames());
	}
	@Override
	public Set<String> getTagValues(String tag)
	{
		return (innerDataPointGroup.getTagValues(tag));
	}
	@Override
	public boolean hasNext()
	{
		return currentDataPoint != null;
	}
	@Override
	public abstract DataPoint next();
	protected boolean hasNextInternal()
	{
		boolean hasNext = innerDataPointGroup.hasNext();
		if (!hasNext)
			currentDataPoint = null;
		return hasNext;
	}
	protected DataPoint nextInternal()
	{
		return innerDataPointGroup.next();
	}
	@Override
	public void remove()
	{
		throw new UnsupportedOperationException();
	}
	@Override
	public void close()
	{
		innerDataPointGroup.close();
	}
	@Override
	public List<GroupByResult> getGroupByResult()
	{
		return innerDataPointGroup.getGroupByResult();
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.datastore;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.groupby.GroupByResult;
import java.util.Iterator;
import java.util.List;
import java.util.Set;
/**
 This interface is used to group data points together when reading from the
 datastore
 */
public interface DataPointGroup extends Iterator<DataPoint>, TagSet
{
	/**
	 Returns the metric name for this group
	 @return Metric name
	 */
	public String getName();
	/**
	 * Retuns an alias for this metric that was passed with the query.
	 * @return
	 */
	String getAlias();
	/**
	 * Returns the list of group by results or an empty list if the results are not grouped.
	 *
	 * @return list of group by results
	 */
	public List<GroupByResult> getGroupByResult();
	/**
	 Returns the api data type for this group
	 @return
	 */
	/*public String getAPIDataType();*/
	/**
	 Close any underlying resources held open by this DataPointGroup.  This
	 will be called at the end of a query to free up resources.
	 */
	public void close();
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import java.util.Collections;
import java.util.Iterator;
/**
 Converts all longs to double. This will cause a loss of precision for very large long values.
 */
@FeatureComponent(
        name = "last",
		description = "Returns the last value data point for the time range."
)
public class LastAggregator extends RangeAggregator
{
	@Inject
	public LastAggregator()
	{
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return true;
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return groupType;
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new LastDataPointAggregator());
	}
	private class LastDataPointAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			DataPoint last = null;
			Long lastTime = 0L;
			while (dataPointRange.hasNext())
			{
				last = dataPointRange.next();
			}
			if (last != null)
			{
				if (m_alignStartTime || m_alignEndTime)
					last.setTimestamp(returnTime);
				return Collections.singletonList(last);
			}
			return Collections.emptyList();
		}
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.http.rest.json;
import com.google.common.collect.HashMultimap;
import com.google.common.collect.SetMultimap;
import com.google.common.collect.TreeMultimap;
import com.google.gson.*;
import com.google.gson.annotations.SerializedName;
import com.google.gson.reflect.TypeToken;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonToken;
import com.google.gson.stream.JsonWriter;
import com.google.inject.Inject;
import org.apache.bval.constraints.NotEmpty;
import org.apache.bval.jsr303.ApacheValidationProvider;
import org.joda.time.DateTimeZone;
import org.kairosdb.core.aggregator.*;
import org.kairosdb.core.annotation.Feature;
import org.kairosdb.core.datastore.*;
import org.kairosdb.core.http.rest.BeanValidationException;
import org.kairosdb.core.http.rest.QueryException;
import org.kairosdb.core.processingstage.FeatureProcessingFactory;
import org.kairosdb.core.processingstage.FeatureProcessor;
import org.kairosdb.plugin.Aggregator;
import org.kairosdb.plugin.GroupBy;
import org.kairosdb.rollup.Rollup;
import org.kairosdb.rollup.RollupTask;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import javax.validation.ConstraintViolation;
import javax.validation.Path;
import javax.validation.Validation;
import javax.validation.Validator;
import javax.validation.constraints.NotNull;
import javax.validation.metadata.ConstraintDescriptor;
import java.beans.BeanInfo;
import java.beans.IntrospectionException;
import java.beans.Introspector;
import java.beans.PropertyDescriptor;
import java.io.IOException;
import java.lang.reflect.Method;
import java.lang.reflect.Type;
import java.util.*;
public class QueryParser
{
	protected static final Logger logger = LoggerFactory.getLogger(QueryParser.class);
	private static final Validator VALIDATOR = Validation.byProvider(ApacheValidationProvider.class).configure().buildValidatorFactory().getValidator();
	private FeatureProcessor m_processingChain;
	private QueryPluginFactory m_pluginFactory;
	private Gson m_gson;
	private Map<Class, Map<String, PropertyDescriptor>> m_descriptorMap;
	private final Object m_descriptorMapLock = new Object();
	@Inject
	public QueryParser(FeatureProcessor processingChain, QueryPluginFactory pluginFactory)
	{
		m_processingChain = processingChain;
		m_pluginFactory = pluginFactory;
		m_descriptorMap = new HashMap<>();
		GsonBuilder gsonBuilder = new GsonBuilder();
		gsonBuilder.registerTypeAdapterFactory(new LowercaseEnumTypeAdapterFactory());
		gsonBuilder.registerTypeAdapter(TimeUnit.class, new TimeUnitDeserializer());
		gsonBuilder.registerTypeAdapter(TrimAggregator.Trim.class, new TrimDeserializer());
		gsonBuilder.registerTypeAdapter(FilterAggregator.FilterOperation.class, new FilterOperationDeserializer());
		gsonBuilder.registerTypeAdapter(DateTimeZone.class, new DateTimeZoneDeserializer());
		gsonBuilder.registerTypeAdapter(Metric.class, new MetricDeserializer());
		gsonBuilder.registerTypeAdapter(SetMultimap.class, new SetMultimapDeserializer());
		gsonBuilder.registerTypeAdapter(RelativeTime.class, new RelativeTimeSerializer());
		gsonBuilder.registerTypeAdapter(SetMultimap.class, new SetMultimapSerializer());
		gsonBuilder.setFieldNamingPolicy(FieldNamingPolicy.LOWER_CASE_WITH_UNDERSCORES);
		m_gson = gsonBuilder.create();
	}
	public Gson getGson()
	{
		return m_gson;
	}
	public static String getUnderscorePropertyName(String camelCaseName)
	{
		StringBuilder sb = new StringBuilder();
		for (char c : camelCaseName.toCharArray())
		{
			if (Character.isUpperCase(c))
				sb.append('_').append(Character.toLowerCase(c));
			else
				sb.append(c);
		}
		return (sb.toString());
	}
	private PropertyDescriptor getPropertyDescriptor(Class objClass, String property) throws IntrospectionException
	{
		synchronized (m_descriptorMapLock)
		{
			Map<String, PropertyDescriptor> propMap = m_descriptorMap.get(objClass);
			if (propMap == null)
			{
				propMap = new HashMap<>();
				m_descriptorMap.put(objClass, propMap);
				BeanInfo beanInfo = Introspector.getBeanInfo(objClass);
				PropertyDescriptor[] descriptors = beanInfo.getPropertyDescriptors();
				for (PropertyDescriptor descriptor : descriptors)
				{
					propMap.put(getUnderscorePropertyName(descriptor.getName()), descriptor);
				}
			}
			return (propMap.get(property));
		}
	}
	private long getStartTime(Query request, String context) throws BeanValidationException
	{
		if (request.getStartAbsolute() != null)
		{
			return request.getStartAbsolute();
		}
		else if (request.getStartRelative() != null)
		{
			return request.getStartRelative().getTimeRelativeTo(System.currentTimeMillis());
		}
		else
		{
			throw new BeanValidationException(new SimpleConstraintViolation("start_time", "relative or absolute time must be set"), context);
		}
	}
	private Optional<Long> getEndTime(Query request)
	{
		if (request.getEndAbsolute() != null)
			return Optional.of(request.getEndAbsolute());
		else if (request.getEndRelative() != null)
			return Optional.of(request.getEndRelative().getTimeRelativeTo(System.currentTimeMillis()));
		return Optional.empty();
	}
	private void validateObject(Object object) throws BeanValidationException
	{
		validateObject(object, null);
	}
	private void validateObject(Object object, String context) throws BeanValidationException
	{
		// validate object using the bean validation framework
		Set<ConstraintViolation<Object>> violations = VALIDATOR.validate(object);
		if (!violations.isEmpty())
		{
			throw new BeanValidationException(violations, context);
		}
	}
	public Query parseQueryMetric(String json) throws QueryException, BeanValidationException
	{
		JsonObject obj = JsonParser.parseString(json).getAsJsonObject();
		return parseQueryMetric(obj);
	}
	private Query parseQueryMetric(JsonObject obj) throws QueryException, BeanValidationException
	{
		return parseQueryMetric(obj, "");
	}
	private Query parseQueryMetric(JsonObject obj, String contextPrefix) throws QueryException, BeanValidationException
	{
		Query query;
		try
		{
			query = m_gson.fromJson(obj, Query.class);
			validateObject(query);
		}
		catch (ContextualJsonSyntaxException e)
		{
			throw new BeanValidationException(new SimpleConstraintViolation(e.getContext(), e.getMessage()), "query");
		}
		//Parses plugins for the entire query
		//Initial use was for post processing plugins
		JsonElement queryPlugins = obj.get("plugins");
		if (queryPlugins != null)
		{
			JsonArray pluginArray = queryPlugins.getAsJsonArray();
			if (pluginArray.size() > 0)
				parsePlugins("", query, pluginArray);
		}
		JsonArray metricsArray = obj.getAsJsonArray("metrics");
		if (metricsArray == null)
		{
			throw new BeanValidationException(new SimpleConstraintViolation("metric[]", "must have a size of at least 1"), contextPrefix + "query");
		}
		for (int I = 0; I < metricsArray.size(); I++)
		{
			String context = (!contextPrefix.isEmpty() ? contextPrefix + "." : contextPrefix) + "query.metric[" + I + "]";
			try
			{
				Metric metric = m_gson.fromJson(metricsArray.get(I), Metric.class);
				validateObject(metric, context);
				long startTime = getStartTime(query, context);
				QueryMetric queryMetric = new QueryMetric(startTime, query.getCacheTime(), metric.getName());
				queryMetric.setExcludeTags(metric.isExcludeTags());
				queryMetric.setLimit(metric.getLimit());
				queryMetric.setJsonObj(obj);
				queryMetric.setAlias(metric.getAlias());
				getEndTime(query).ifPresent(queryMetric::setEndTime);
				if (queryMetric.getEndTime() < startTime)
					throw new BeanValidationException(new SimpleConstraintViolation("end_time", "must be greater than the start time"), context);
				queryMetric.setCacheString(query.getCacheString() + metric.getCacheString());
				JsonObject jsMetric = metricsArray.get(I).getAsJsonObject();
				for (FeatureProcessingFactory<?> factory : m_processingChain.getFeatureProcessingFactories())
				{
					String factoryName = factory.getClass().getAnnotation(Feature.class).name();
					JsonElement queryProcessor = jsMetric.get(factoryName);
					if (queryProcessor != null)
					{
						JsonArray queryProcessorArray = queryProcessor.getAsJsonArray();
						parseQueryProcessor(context, factoryName,
								queryProcessorArray, factory.getFeature(),
								queryMetric, query.getTimeZone());
					}
				}
				JsonElement plugins = jsMetric.get("plugins");
				if (plugins != null)
				{
					JsonArray pluginArray = plugins.getAsJsonArray();
					if (pluginArray.size() > 0)
						parsePlugins(context, queryMetric, pluginArray);
				}
				JsonElement order = jsMetric.get("order");
				if (order != null)
					queryMetric.setOrder(Order.fromString(order.getAsString(), context));
				queryMetric.setTags(metric.getTags());
				query.addQueryMetric(queryMetric);
			}
			catch (ContextualJsonSyntaxException e)
			{
				throw new BeanValidationException(new SimpleConstraintViolation(e.getContext(), e.getMessage()), context);
			}
		}
		return (query);
	}
	private void parseSpecificQueryProcessor(Object queryProcessor, QueryMetric queryMetric, DateTimeZone timeZone)
	{
		if (queryProcessor instanceof RangeAggregator)
		{
			RangeAggregator ra = (RangeAggregator) queryProcessor;
			ra.setStartTime(queryMetric.getStartTime());
			ra.setEndTime(queryMetric.getEndTime());
		}
		if (queryProcessor instanceof TimezoneAware)
		{
			TimezoneAware ta = (TimezoneAware) queryProcessor;
			ta.setTimeZone(timeZone);
		}
		if (queryProcessor instanceof GroupByAware)
		{
			GroupByAware groupByAware = (GroupByAware) queryProcessor;
			groupByAware.setGroupBys(queryMetric.getGroupBys());
		}
		if (queryProcessor instanceof GroupBy)
		{
			GroupBy groupBy = (GroupBy) queryProcessor;
			groupBy.setStartDate(queryMetric.getStartTime());
		}
	}
	private void addQueryProcessorToMetric(Object queryProcessor, QueryMetric queryMetric)
	{
		if (queryProcessor instanceof Aggregator)
			queryMetric.addAggregator((Aggregator) queryProcessor);
		if (queryProcessor instanceof GroupBy)
			queryMetric.addGroupBy((GroupBy) queryProcessor);
	}
	private void parseQueryProcessor(String context, String queryProcessorFamilyName,
			JsonArray queryProcessors, Class<?> queryProcessorFamilyType,
			QueryMetric queryMetric, DateTimeZone dateTimeZone)
			throws BeanValidationException, QueryException
	{
		for (int J = 0; J < queryProcessors.size(); J++)
		{
			JsonObject jsQueryProcessor = queryProcessors.get(J).getAsJsonObject();
			JsonElement name = jsQueryProcessor.get("name");
			if (name == null || name.getAsString().isEmpty())
				throw new BeanValidationException(new SimpleConstraintViolation(queryProcessorFamilyName + "[" + J + "]", "must have a name"), context);
			String qpContext = context + "." + queryProcessorFamilyName + "[" + J + "]";
			String qpName = name.getAsString();
			Object queryProcessor = m_processingChain.getFeatureProcessingFactory(queryProcessorFamilyType).createFeatureProcessor(qpName);
			if (queryProcessor == null)
				throw new BeanValidationException(new SimpleConstraintViolation(qpName, "invalid " + queryProcessorFamilyName + " name"), qpContext);
			parseSpecificQueryProcessor(queryProcessor, queryMetric, dateTimeZone);
			deserializeProperties(qpContext, jsQueryProcessor, qpName, queryProcessor);
			validateObject(queryProcessor, qpContext);
			addQueryProcessorToMetric(queryProcessor, queryMetric);
			if (queryProcessor instanceof Aggregator)
			{
				((Aggregator)queryProcessor).init();
			}
		}
	}
	public List<RollupTask> parseRollupTasks(String json) throws BeanValidationException, QueryException
	{
		List<RollupTask> tasks = new ArrayList<>();
		JsonArray rollupTasks = JsonParser.parseString(json).getAsJsonArray();
		for (int i = 0; i < rollupTasks.size(); i++)
		{
			JsonObject taskObject = rollupTasks.get(i).getAsJsonObject();
			RollupTask task = parseRollupTask(taskObject, "tasks[" + i + "]");
			task.addJson(taskObject.toString().replaceAll("\\n", ""));
			tasks.add(task);
		}
		return tasks;
	}
	public RollupTask parseRollupTask(String json) throws BeanValidationException, QueryException
	{
		return parseRollupTask(json, null);
	}
	public RollupTask parseRollupTask(String json, String id) throws BeanValidationException, QueryException
	{
		JsonObject taskObject = JsonParser.parseString(json).getAsJsonObject();
		RollupTask task = parseRollupTask(taskObject, "");
		String newJson = taskObject.toString();
		if (id != null)
		{
			// If updating the task, replace the new Id with the old Id
			newJson = newJson.replace(task.getId(), id);
		}
		task.addJson(newJson.replaceAll("\\n", ""));
		return task;
	}
	private RollupTask parseRollupTask(JsonObject rollupTask, String context) throws BeanValidationException, QueryException
	{
		RollupTask task = m_gson.fromJson(rollupTask.getAsJsonObject(), RollupTask.class);
		validateObject(task);
		JsonArray rollups = rollupTask.getAsJsonObject().getAsJsonArray("rollups");
		if (rollups != null)
		{
			for (int j = 0; j < rollups.size(); j++)
			{
				JsonObject rollupObject = rollups.get(j).getAsJsonObject();
				Rollup rollup = m_gson.fromJson(rollupObject, Rollup.class);
				context = context + "rollup[" + j + "]";
				validateObject(rollup, context);
				JsonObject queryObject = rollupObject.getAsJsonObject("query");
				List<QueryMetric> queries = parseQueryMetric(queryObject, context).getQueryMetrics();
				for (int k = 0; k < queries.size(); k++)
				{
					QueryMetric query = queries.get(k);
					context += ".query[" + k + "]";
					validateHasRangeAggregator(query, context);
					// Add aggregators needed for rollups
					SaveAsAggregator saveAsAggregator = (SaveAsAggregator) m_processingChain.getFeatureProcessingFactory(Aggregator.class).createFeatureProcessor("save_as");
					saveAsAggregator.setMetricName(rollup.getSaveAs());
					saveAsAggregator.setGroupBys(query.getGroupBys());
					query.addAggregator(saveAsAggregator);
				}
				rollup.addQueries(queries);
				task.addRollup(rollup);
			}
		}
		return task;
	}
	private void validateHasRangeAggregator(QueryMetric query, String context) throws BeanValidationException
	{
		boolean hasRangeAggregator = false;
		for (Aggregator aggregator : query.getAggregators())
		{
			if (aggregator instanceof RangeAggregator)
			{
				hasRangeAggregator = true;
				break;
			}
		}
		if (!hasRangeAggregator)
		{
			throw new BeanValidationException(new SimpleConstraintViolation("aggregator", "At least one aggregator must be a range aggregator"), context);
		}
	}
	private void parsePlugins(String context, PluggableQuery queryMetric, JsonArray plugins) throws BeanValidationException, QueryException
	{
		for (int I = 0; I < plugins.size(); I++)
		{
			JsonObject pluginJson = plugins.get(I).getAsJsonObject();
			JsonElement name = pluginJson.get("name");
			if (name == null || name.getAsString().isEmpty())
				throw new BeanValidationException(new SimpleConstraintViolation("plugins[" + I + "]", "must have a name"), context);
			String pluginContext = context + ".plugins[" + I + "]";
			String pluginName = name.getAsString();
			QueryPlugin plugin = m_pluginFactory.createQueryPlugin(pluginName);
			if (plugin == null)
				throw new BeanValidationException(new SimpleConstraintViolation(pluginName, "invalid query plugin name"), pluginContext);
			deserializeProperties(pluginContext, pluginJson, pluginName, plugin);
			validateObject(plugin, pluginContext);
			queryMetric.addPlugin(plugin);
		}
	}
	private void deserializeProperties(String context, JsonObject jsonObject, String name, Object object) throws QueryException, BeanValidationException
	{
		Set<Map.Entry<String, JsonElement>> props = jsonObject.entrySet();
		for (Map.Entry<String, JsonElement> prop : props)
		{
			String property = prop.getKey();
			if (property.equals("name"))
				continue;
			PropertyDescriptor pd = null;
			try
			{
				pd = getPropertyDescriptor(object.getClass(), property);
			}
			catch (IntrospectionException e)
			{
				logger.error("Introspection error on " + object.getClass(), e);
			}
			if (pd == null)
			{
				String msg = "Property '" + property + "' was specified for object '" + name +
						"' but no matching setter was found on '" + object.getClass() + "'";
				throw new QueryException(msg);
			}
			Class<?> propClass = pd.getPropertyType();
			Object propValue;
			try
			{
				propValue = m_gson.fromJson(prop.getValue(), propClass);
				validateObject(propValue, context + "." + property);
			}
			catch (ContextualJsonSyntaxException e)
			{
				throw new BeanValidationException(new SimpleConstraintViolation(e.getContext(), e.getMessage()), context);
			}
			catch (NumberFormatException e)
			{
				throw new BeanValidationException(new SimpleConstraintViolation(property, e.getMessage()), context);
			}
			Method method = pd.getWriteMethod();
			if (method == null)
			{
				String msg = "Property '" + property + "' was specified for object '" + name +
						"' but no matching setter was found on '" + object.getClass().getName() + "'";
				throw new QueryException(msg);
			}
			try
			{
				method.invoke(object, propValue);
			}
			catch (Exception e)
			{
				logger.error("Invocation error: ", e);
				String msg = "Call to " + object.getClass().getName() + ":" + method.getName() +
						" failed with message: " + e.getMessage();
				throw new QueryException(msg);
			}
		}
	}
	//===========================================================================
	private static class Metric
	{
		@NotNull
		@NotEmpty()
		@SerializedName("name")
		private final String name;
		@SerializedName("alias")
		private String alias;
		@SerializedName("tags")
		private final SetMultimap<String, String> tags;
		@SerializedName("exclude_tags")
		private final boolean exclude_tags;
		@SerializedName("limit")
		private int limit;
		public Metric(String name, boolean exclude_tags, TreeMultimap<String, String> tags)
		{
			this.name = name;
			this.tags = tags;
			this.exclude_tags = exclude_tags;
			this.limit = 0;
		}
		public String getName()
		{
			return name;
		}
		public String getAlias()
		{
			return alias;
		}
		public void setAlias(String alias)
		{
			this.alias = alias;
		}
		public int getLimit()
		{
			return limit;
		}
		public void setLimit(int limit)
		{
			this.limit = limit;
		}
		private boolean isExcludeTags()
		{
			return exclude_tags;
		}
		String getCacheString()
		{
			StringBuilder sb = new StringBuilder();
			sb.append(name).append(":");
			for (Map.Entry<String, String> tagEntry : tags.entries())
			{
				sb.append(tagEntry.getKey()).append("=");
				sb.append(tagEntry.getValue()).append(":");
			}
			return (sb.toString());
		}
		public SetMultimap<String, String> getTags()
		{
			if (tags != null)
			{
				return tags;
			}
			else
			{
				return HashMultimap.create();
			}
		}
	}
	//===========================================================================
	private static class LowercaseEnumTypeAdapterFactory implements TypeAdapterFactory
	{
		public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type)
		{
			@SuppressWarnings("unchecked")
			Class<T> rawType = (Class<T>) type.getRawType();
			if (!rawType.isEnum())
			{
				return null;
			}
			final Map<String, T> lowercaseToConstant = new HashMap<>();
			for (T constant : rawType.getEnumConstants())
			{
				lowercaseToConstant.put(toLowercase(constant), constant);
			}
			return new TypeAdapter<T>()
			{
				public void write(JsonWriter out, T value) throws IOException
				{
					if (value == null)
					{
						out.nullValue();
					}
					else
					{
						out.value(toLowercase(value));
					}
				}
				public T read(JsonReader reader) throws IOException
				{
					if (reader.peek() == JsonToken.NULL)
					{
						reader.nextNull();
						return null;
					}
					else
					{
						return lowercaseToConstant.get(reader.nextString());
					}
				}
			};
		}
		private String toLowercase(Object o)
		{
			return o.toString().toLowerCase(Locale.US);
		}
	}
	//===========================================================================
	private static class TimeUnitDeserializer implements JsonDeserializer<TimeUnit>
	{
		public TimeUnit deserialize(JsonElement json, Type typeOfT, JsonDeserializationContext context)
				throws JsonParseException
		{
			String unit = json.getAsString();
			TimeUnit tu;
			try
			{
				tu = TimeUnit.from(unit);
			}
			catch (IllegalArgumentException e)
			{
				throw new ContextualJsonSyntaxException(unit,
						"is not a valid time unit, must be one of " + TimeUnit.toValueNames());
			}
			return tu;
		}
	}
	private static abstract class EnumDeserializer<TEnum extends Enum<TEnum>> implements JsonDeserializer<TEnum>
	{
		TEnum genericDeserializer(JsonElement json, Class<TEnum> type)
				throws JsonParseException
		{
			String jsValue = json.getAsString();
			TEnum[] enumDefinition = type.getEnumConstants();
			for (TEnum value : enumDefinition)
				if (value.toString().equalsIgnoreCase(jsValue))
					return value;
			StringBuilder values = new StringBuilder("is not a valid trim type, must be ");
			for (int i = 0; i < enumDefinition.length; i++)
			{
				values.append("'").append(enumDefinition[i].toString().toLowerCase()).append("'");
				if (i < enumDefinition.length - 2)
					values.append(", ");
				else if (i < enumDefinition.length - 1)
					values.append(" or ");
				else
					values.append(".");
			}
			throw new ContextualJsonSyntaxException(jsValue, values.toString());
		}
	}
	private static class TrimDeserializer extends EnumDeserializer<TrimAggregator.Trim>
	{
		public TrimAggregator.Trim deserialize(JsonElement json, Type typeOfT, JsonDeserializationContext context)
				throws JsonParseException
		{
			return genericDeserializer(json, TrimAggregator.Trim.class);
		}
	}
	private static class FilterOperationDeserializer extends EnumDeserializer<FilterAggregator.FilterOperation>
	{
		public FilterAggregator.FilterOperation deserialize(JsonElement json, Type typeOfT, JsonDeserializationContext context)
				throws JsonParseException
		{
			return genericDeserializer(json, FilterAggregator.FilterOperation.class);
		}
	}
	//===========================================================================
	private static class DateTimeZoneDeserializer implements JsonDeserializer<DateTimeZone>
	{
		public DateTimeZone deserialize(JsonElement json, Type typeOfT, JsonDeserializationContext context)
				throws JsonParseException
		{
			if (json.isJsonNull())
				return null;
			String tz = json.getAsString();
			if (tz.isEmpty()) // defaults to UTC
				return DateTimeZone.UTC;
			DateTimeZone timeZone;
			try
			{
				// check if time zone is valid
				timeZone = DateTimeZone.forID(tz);
			}
			catch (IllegalArgumentException e)
			{
				throw new ContextualJsonSyntaxException(tz,
						"is not a valid time zone, must be one of " + DateTimeZone.getAvailableIDs());
			}
			return timeZone;
		}
	}
	//===========================================================================
	private static class MetricDeserializer implements JsonDeserializer<Metric>
	{
		@Override
		public Metric deserialize(JsonElement jsonElement, Type type, JsonDeserializationContext jsonDeserializationContext)
				throws JsonParseException
		{
			JsonObject jsonObject = jsonElement.getAsJsonObject();
			String name = null;
			if (jsonObject.get("name") != null)
				name = jsonObject.get("name").getAsString();
			boolean exclude_tags = false;
			if (jsonObject.get("exclude_tags") != null)
				exclude_tags = jsonObject.get("exclude_tags").getAsBoolean();
			TreeMultimap<String, String> tags = TreeMultimap.create();
			JsonElement jeTags = jsonObject.get("tags");
			if (jeTags != null)
			{
				JsonObject joTags = jeTags.getAsJsonObject();
				int count = 0;
				for (Map.Entry<String, JsonElement> tagEntry : joTags.entrySet())
				{
					String context = "tags[" + count + "]";
					if (tagEntry.getKey().isEmpty())
						throw new ContextualJsonSyntaxException(context, "name must not be empty");
					if (tagEntry.getValue().isJsonArray())
					{
						for (JsonElement element : tagEntry.getValue().getAsJsonArray())
						{
							if (element.isJsonNull() || element.getAsString().isEmpty())
								throw new ContextualJsonSyntaxException(context + "." + tagEntry.getKey(), "value must not be null or empty");
							tags.put(tagEntry.getKey(), element.getAsString());
						}
					}
					else
					{
						if (tagEntry.getValue().isJsonNull() || tagEntry.getValue().getAsString().isEmpty())
							throw new ContextualJsonSyntaxException(context + "." + tagEntry.getKey(), "value must not be null or empty");
						tags.put(tagEntry.getKey(), tagEntry.getValue().getAsString());
					}
					count++;
				}
			}
			Metric ret = new Metric(name, exclude_tags, tags);
			JsonElement limit = jsonObject.get("limit");
			if (limit != null)
				ret.setLimit(limit.getAsInt());
			JsonElement alias = jsonObject.get("alias");
			if (alias != null)
				ret.setAlias(alias.getAsString());
			return (ret);
		}
	}
	//===========================================================================
	private static class ContextualJsonSyntaxException extends RuntimeException
	{
		private final String context;
		private ContextualJsonSyntaxException(String context, String msg)
		{
			super(msg);
			this.context = context;
		}
		private String getContext()
		{
			return context;
		}
	}
	//===========================================================================
	public static class SimpleConstraintViolation implements ConstraintViolation<Object>
	{
		private final String message;
		private final String context;
		public SimpleConstraintViolation(String context, String message)
		{
			this.message = message;
			this.context = context;
		}
		@Override
		public String getMessage()
		{
			return message;
		}
		@Override
		public String getMessageTemplate()
		{
			return null;
		}
		@Override
		public Object getRootBean()
		{
			return null;
		}
		@Override
		public Class<Object> getRootBeanClass()
		{
			return null;
		}
		@Override
		public Object getLeafBean()
		{
			return null;
		}
		@Override
		public Path getPropertyPath()
		{
			return new SimplePath(context);
		}
		@Override
		public Object getInvalidValue()
		{
			return null;
		}
		@Override
		public ConstraintDescriptor<?> getConstraintDescriptor()
		{
			return null;
		}
	}
	private static class SimplePath implements Path
	{
		private final String context;
		private SimplePath(String context)
		{
			this.context = context;
		}
		@Override
		public Iterator<Node> iterator()
		{
			return null;
		}
		@Override
		public String toString()
		{
			return context;
		}
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.plugin;
import org.kairosdb.core.datastore.DataPointGroup;
public interface Aggregator
{
	DataPointGroup aggregate(DataPointGroup dataPointGroup);
	boolean canAggregate(String groupType);
	String getAggregatedGroupType(String groupType);
	/**
	 This is called after the aggregator is created and properties have been set.
	 */
	void init();
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import org.kairosdb.core.exception.KairosDBException;
import java.util.Collections;
import java.util.Iterator;
/**
 * Converts all longs to double. This will cause a loss of precision for very large long values.
 */
@FeatureComponent(
		name = "avg",
		label = "AVG",
		description = "Averages the data points together."
)
public class AvgAggregator extends RangeAggregator
{
	DoubleDataPointFactory m_dataPointFactory;
	@Inject
	public AvgAggregator(DoubleDataPointFactory dataPointFactory) throws KairosDBException
	{
		m_dataPointFactory = dataPointFactory;
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new AvgDataPointAggregator());
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return DataPoint.GROUP_NUMBER.equals(groupType);
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return m_dataPointFactory.getGroupType();
	}
	private class AvgDataPointAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			int count = 0;
			double sum = 0;
			while (dataPointRange.hasNext())
			{
				DataPoint dp = dataPointRange.next();
				if (dp.isDouble())
				{
					sum += dp.getDoubleValue();
					count++;
				}
			}
			return Collections.singletonList(m_dataPointFactory.createDataPoint(returnTime, sum / count));
		}
	}
}
package org.kairosdb.rollup;
import org.joda.time.DateTimeZone;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.aggregator.RangeAggregator;
import org.kairosdb.core.aggregator.Sampling;
import org.kairosdb.core.datastore.*;
import org.kairosdb.core.exception.DatastoreException;
import org.kairosdb.plugin.Aggregator;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.util.Date;
import java.util.List;
import static java.util.Objects.requireNonNull;
public class RollupProcessorImpl implements RollupProcessor
{
	private static final Logger log = LoggerFactory.getLogger(RollupProcessorImpl.class);
	private final KairosDatastore datastore;
	private boolean interrupted;
	public RollupProcessorImpl(KairosDatastore datastore)
	{
		this.datastore = requireNonNull(datastore, "datastore must not be null");
	}
	/*
		Rollup algorithm
		Sampling size is calculated from the last sampling aggregator for the rollup
		1 - Query for last rollup
			2a - No rollup and no status (First time) - set start time to now - run interval - sampling size
			2b - Rollup found - set start time to be the last rollup time (this will recreate the last rollup)
		4 - Set start and end times on sampling period
		5 - Create a rollup for each sampling interval until you reach now.
	 */
	@Override
	public long process(RollupTaskStatusStore statusStore, RollupTask task, QueryMetric rollupQueryMetric, DateTimeZone timeZone)
			throws RollUpException, DatastoreException, InterruptedException
	{
		long now = now();
		Sampling samplingSize = getSamplingSize(getLastAggregator(rollupQueryMetric.getAggregators()));
		long lastExecutionTime = getLastExecutionTime(statusStore, task, now);
		if (log.isDebugEnabled())
			log.debug("LastExecutionTime = " + new Date(lastExecutionTime));
		long startTime = calculateStartTime(task.getExecutionInterval(), samplingSize, lastExecutionTime, now);
		if (log.isDebugEnabled())
			log.debug("startTime = " + new Date(startTime));
		return process(task, rollupQueryMetric, startTime, now, timeZone);
	}
	@Override
	public long process(RollupTask task, QueryMetric rollupQueryMetric, long startTime, long endTime, DateTimeZone timeZone)
			throws DatastoreException, InterruptedException, RollUpException {
		RangeAggregator lastAggregator = getLastAggregator(rollupQueryMetric.getAggregators());
		List<SamplingPeriod> samplingPeriods;
		samplingPeriods = RollupUtil.getSamplingPeriodsAlignedToUnit(lastAggregator, startTime, endTime, timeZone);
		if (log.isDebugEnabled())
		{
			for (SamplingPeriod samplingPeriod : samplingPeriods)
			{
				log.debug("Sampling period " + samplingPeriod);
			}
		}
		long dpCount = 0;
		// Note: there will always be at least 2 sampling periods (start and end)
		for (SamplingPeriod samplingPeriod : samplingPeriods)
		{
			if (interrupted)
			{
				break;
			}
			rollupQueryMetric.setStartTime(samplingPeriod.getStartTime());
			rollupQueryMetric.setEndTime(samplingPeriod.getEndTime());
			dpCount += executeRollup(rollupQueryMetric);
			log.debug("Rollup Task: " + task.getName() + " for Rollup " + task.getName() + " data point count of " + dpCount);
			Thread.sleep(50);
		}
		return dpCount;
	}
	/**
	 Returns the sampling from the last RangeAggregator in the aggregators list
	 @exception RollUpException if no Range Aggregators exist
	 */
	private static RangeAggregator getLastAggregator(List<Aggregator> aggregators) throws RollUpException {
		for (int i = aggregators.size() - 1; i >= 0; i--)
		{
			Aggregator aggregator = aggregators.get(i);
			if (aggregator instanceof RangeAggregator)
			{
				return ((RangeAggregator) aggregator);
			}
		}
		// should never happen
		throw new RollUpException("Roll-up must have at least one Range aggregator");
	}
	private static Sampling getSamplingSize(RangeAggregator aggregator)
	{
		return aggregator.getSampling();
	}
	private long executeRollup(QueryMetric query) throws DatastoreException
	{
		log.debug("Execute Rollup: " + query.getName() + " Start time: " + new Date(query.getStartTime()) + " End time: " + new Date(query.getEndTime()));
		int dpCount = 0;
		try (DatastoreQuery dq = datastore.createQuery(query))
		{
			List<DataPointGroup> result = dq.execute();
			for (DataPointGroup dataPointGroup : result)
			{
				while (dataPointGroup.hasNext())
				{
					dataPointGroup.next();
					dpCount++;
				}
			}
		}
		return dpCount;
	}
	private static long now()
	{
		return System.currentTimeMillis();
	}
	private static DataPoint performQuery(KairosDatastore datastore, QueryMetric rollupQuery) throws DatastoreException
	{
		try (DatastoreQuery query = datastore.createQuery(rollupQuery))
		{
			List<DataPointGroup> rollupResult = query.execute();
			DataPoint dataPoint = null;
			for (DataPointGroup dataPointGroup : rollupResult)
			{
				while (dataPointGroup.hasNext())
				{
					DataPoint next = dataPointGroup.next();
					if (next.getApiDataType().equals(DataPoint.API_DOUBLE) ||
							next.getApiDataType().equals(DataPoint.API_LONG))
					{
						dataPoint = next;
					}
				}
			}
			return dataPoint;
		}
	}
	private long getLastExecutionTime(RollupTaskStatusStore statusStore, RollupTask task, long now)
			throws RollUpException, DatastoreException
	{
		long lastExecutionTime = 0L;
		// get last status
		RollupTaskStatus status = statusStore.read(task.getId());
		if (status != null)
			return geStatusExecutionTime(status);
		else
		{
			// get last rollup
			DataPoint lastRollup = getLastRollup(datastore, task.getName(), now);
			if (lastRollup != null)
				lastExecutionTime = lastRollup.getTimestamp();
		}
		return lastExecutionTime;
	}
	// Find the last execution time
	private static long geStatusExecutionTime(RollupTaskStatus status)
	{
		if (status == null)
		{
			return 0L;
		}
		long lastExecutionTime = 0L;
		for (RollupQueryMetricStatus metricStatus : status.getStatuses())
		{
			lastExecutionTime = Math.max(lastExecutionTime, metricStatus.getLastExecutionTime());
		}
		return lastExecutionTime;
	}
	/**
	 Returns the last data point the rollup created
	 */
	private static DataPoint getLastRollup(KairosDatastore datastore, String rollupName, long now)
			throws DatastoreException
	{
		QueryMetric rollupQuery = new QueryMetric(0, now, 0, rollupName);
		rollupQuery.setLimit(1);
		rollupQuery.setOrder(Order.DESC);
		return performQuery(datastore, rollupQuery);
	}
	/**
	 Returns the last execution time if there was one or the (run interval + sampling size) from now
	 */
	private static long calculateStartTime(Duration executionInterval, Sampling samplingSize, long lastExecutionTime, long now)
	{
		return lastExecutionTime == 0L ? RollupUtil.subtract(RollupUtil.subtract(now, executionInterval), samplingSize)
				: lastExecutionTime;
	}
	@Override
	public void interrupt()
	{
		interrupted = true;
	}
}
package org.kairosdb.rollup;
import org.joda.time.DateTimeZone;
import org.kairosdb.core.aggregator.RangeAggregator;
import org.kairosdb.core.aggregator.Sampling;
import org.kairosdb.core.datastore.Duration;
import java.time.Instant;
import java.time.LocalDateTime;
import java.time.ZoneOffset;
import java.time.temporal.ChronoUnit;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
public class RollupUtil
{
	private RollupUtil(){}
	/**
	 Returns a list of times for the specified time range aligned to a sampling boundary. For example, if the sampling
	 is an hour, this method returns all the times between the start and end time that fall on the hour boundary.
	 @param aggregator aggregator
	 @param startTime starting time
	 @param endTime ending time
	 @return list of times for the specified time range aligned to the sampling
	 */
	public static List<SamplingPeriod> getSamplingPeriodsAlignedToUnit(RangeAggregator aggregator,
			long startTime, long endTime, DateTimeZone timeZone)
	{
		aggregator.setTimeZone(timeZone);
		aggregator.setAlignSampling(true);
		aggregator.init();
		List<SamplingPeriod> samplingPeriods = new ArrayList<>();
		long startTimeAligned = aggregator.getStartRange(startTime);
		long endTimeAligned = aggregator.getEndRange(endTime);
		if (startTimeAligned == endTimeAligned)
		{
			return Collections.emptyList();
		}
		long nextStartTime = startTimeAligned;
		while( nextStartTime < endTimeAligned)
		{
			long periodStart = nextStartTime;
			nextStartTime = getNextStartTime(aggregator.getSampling(), nextStartTime);
			samplingPeriods.add(new SamplingPeriod(periodStart, nextStartTime));
		}
		return samplingPeriods;
	}
	private static ChronoUnit convertSamplingToChronoUnit(Duration duration)
	{
		switch(duration.getUnit())
		{
			case YEARS:
				return ChronoUnit.YEARS;
			case MONTHS:
				return ChronoUnit.MONTHS;
			case DAYS:
				return ChronoUnit.DAYS;
			case HOURS:
				return ChronoUnit.HOURS;
			case MINUTES:
				return ChronoUnit.MINUTES;
			case SECONDS:
				return ChronoUnit.SECONDS;
			case MILLISECONDS:
				return ChronoUnit.MILLIS;
			default:
				throw new IllegalArgumentException("Cannot convert sampling time to ChronoUnit for " + duration.getUnit().toString());
		}
	}
	private static LocalDateTime trucateTo(LocalDateTime time, Sampling sampling)
	{
		ChronoUnit chronoUnit = convertSamplingToChronoUnit(sampling);
		// NOTE: LocalDateTime.truncatedTo(Month | Year) will throw an exception. Need to special case it
		if (chronoUnit == ChronoUnit.MONTHS)
		{
			return time.truncatedTo(ChronoUnit.DAYS).withDayOfMonth(1);
		}
		else if (chronoUnit == ChronoUnit.YEARS)
		{
			return time.truncatedTo(ChronoUnit.DAYS).withDayOfMonth(1).withMonth(1);
		}
		else
		{
			return time.truncatedTo(chronoUnit);
		}
	}
	private static long getNextStartTime(Sampling sampling, long currentStartTime)
	{
		ChronoUnit chronoUnit = convertSamplingToChronoUnit(sampling);
		LocalDateTime localDateTime =	LocalDateTime.ofInstant(Instant.ofEpochMilli(currentStartTime), ZoneOffset.UTC);
		return localDateTime.plus(sampling.getValue(), chronoUnit).toInstant(ZoneOffset.UTC).toEpochMilli();
	}
	/**
	 Returns the specified time minus the duration.
	 @param time time
	 @param duration amount to subtract from the time
	 @return time minus duration
	 */
	public static long subtract(long time, Duration duration)
	{
		ChronoUnit chronoUnit = convertSamplingToChronoUnit(duration);
		LocalDateTime localDateTime =	LocalDateTime.ofInstant(Instant.ofEpochMilli(time), ZoneOffset.UTC);
		return localDateTime.minus(duration.getValue(), chronoUnit).toInstant(ZoneOffset.UTC).toEpochMilli();
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import org.kairosdb.core.datastore.Duration;
import org.kairosdb.core.datastore.TimeUnit;
public class Sampling extends Duration
{
	public Sampling()
	{
		super();
	}
	public Sampling(int value, TimeUnit unit)
	{
		super(value, unit);
	}
	@Override
	public String toString()
	{
		return "Sampling{" +
				"} " + super.toString();
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import java.util.Collections;
import java.util.Iterator;
/**
 * Converts all longs to double. This will cause a loss of precision for very large long values.
 */
@FeatureComponent(
        name = "min",
		description = "Returns the minimum value data point for the time range."
)
public class MinAggregator extends RangeAggregator
{
	private DoubleDataPointFactory m_dataPointFactory;
	@Inject
	public MinAggregator(DoubleDataPointFactory dataPointFactory)
	{
		m_dataPointFactory = dataPointFactory;
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return DataPoint.GROUP_NUMBER.equals(groupType);
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return m_dataPointFactory.getGroupType();
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new MinDataPointAggregator());
	}
	private class MinDataPointAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			double min = Double.MAX_VALUE;
			while (dataPointRange.hasNext())
			{
				DataPoint dp = dataPointRange.next();
				min = Math.min(min, dp.getDoubleValue());
			}
			return Collections.singletonList(m_dataPointFactory.createDataPoint(returnTime, min));
		}
	}
}
package org.kairosdb.core.aggregator;
import com.google.common.collect.ImmutableList;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.annotation.FeatureProperty;
import org.kairosdb.core.datapoints.LongDataPoint;
import org.kairosdb.core.datapoints.NullDataPoint;
import java.util.Collections;
import java.util.Iterator;
@FeatureComponent(
		name = "pad",
		description = "Pads each range that is missing data with a value (default is zero)."
)
public class PadAggregator extends RangeAggregator
{
	@FeatureProperty(
			name = "pad_value",
			label = "Pad value",
			description = "Value to pad each range with.",
			default_value = "0"
	)
	private long m_padValue = 0L;
	@Inject
	public PadAggregator()
	{
		super(true);
	}
	public void setPadValue(long value)
	{
		m_padValue = value;
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return new AddPadAggregator();
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return true;
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return groupType;
	}
	private class AddPadAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			if (dataPointRange.hasNext())
			{
				return ImmutableList.copyOf(dataPointRange);
			}
			return Collections.singletonList(new LongDataPoint(returnTime, m_padValue));
		}
	}
}
/*
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.common.collect.ImmutableList;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.NullDataPoint;
import java.util.Collections;
import java.util.Iterator;
@FeatureComponent(
        name = "gaps",
		description = "Marks gaps in data according to sampling rate with a null data point."
)
public class DataGapsMarkingAggregator extends RangeAggregator
{
	@Inject
	public DataGapsMarkingAggregator()
	{
		super(true);
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new MarkDataGapsAggregator());
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return true;
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return groupType;
	}
	private static class MarkDataGapsAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			if (dataPointRange.hasNext())
			{
				return ImmutableList.copyOf(dataPointRange);
			}
			return Collections.singletonList(new NullDataPoint(returnTime));
		}
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import java.util.Collections;
import java.util.Iterator;
/**
 * Converts all longs to double. This will cause a loss of precision for very large long values.
 */
@FeatureComponent(
        name = "max",
		description = "Returns the maximum value data point for the time range."
)
public class MaxAggregator extends RangeAggregator
{
	private DoubleDataPointFactory m_dataPointFactory;
	@Inject
	public MaxAggregator(DoubleDataPointFactory dataPointFactory)
	{
		m_dataPointFactory = dataPointFactory;
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return DataPoint.GROUP_NUMBER.equals(groupType);
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return m_dataPointFactory.getGroupType();
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new MaxDataPointAggregator());
	}
	private class MaxDataPointAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			double max = -Double.MAX_VALUE;
			while (dataPointRange.hasNext())
			{
				max = Math.max(max, dataPointRange.next().getDoubleValue());
			}
			return Collections.singletonList(m_dataPointFactory.createDataPoint(returnTime, max));
		}
	}
}
package org.kairosdb.core.annotation;
import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;
/**
 * Aggregators cannot be recursive meaning that FeatureProperty cannot contain a FeatureProperty.
 * Thus this class exists to allow for properties within properties.
 */
@Target(ElementType.FIELD)
@Retention(RetentionPolicy.RUNTIME)
public @interface FeatureCompoundProperty
{
    String name() default "";
	String label();
	FeatureProperty[] properties() default {};
	String[] order() default {};
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.DoubleDataPointFactory;
import java.util.Collections;
import java.util.Iterator;
/**
 Converts all longs to double. This will cause a loss of precision for very large long values.
 */
@FeatureComponent(
        name = "first",
		description = "Returns the first value data point for the time range."
)
public class FirstAggregator extends RangeAggregator
{
	@Inject
	public FirstAggregator()
	{
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return true;
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return groupType;
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new FirstDataPointAggregator());
	}
	private class FirstDataPointAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			Iterable<DataPoint> ret;
			if (dataPointRange.hasNext())
			{
				DataPoint next = dataPointRange.next();
				next.setTimestamp(returnTime);
				ret = Collections.singletonList(next);
			}
			else
				ret = Collections.emptyList();
			//Chew up the rest of the data points in range
			while (dataPointRange.hasNext())
			{
				dataPointRange.next();
			}
			return ret;
		}
	}
}
/*
 * Copyright 2016 KairosDB Authors
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        http://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package org.kairosdb.core.aggregator;
import com.google.inject.Inject;
import org.kairosdb.core.DataPoint;
import org.kairosdb.core.annotation.FeatureComponent;
import org.kairosdb.core.datapoints.LongDataPointFactory;
import java.util.Collections;
import java.util.Iterator;
@FeatureComponent(
        name = "count",
		description = "Counts the number of data points."
)
public class CountAggregator extends RangeAggregator
{
	LongDataPointFactory m_dataPointFactory;
	@Inject
	public CountAggregator(LongDataPointFactory dataPointFactory)
	{
		m_dataPointFactory = dataPointFactory;
	}
	@Override
	protected RangeSubAggregator getSubAggregator()
	{
		return (new CountDataPointAggregator());
	}
	@Override
	public boolean canAggregate(String groupType)
	{
		return true;
	}
	@Override
	public String getAggregatedGroupType(String groupType)
	{
		return m_dataPointFactory.getGroupType();
	}
	private class CountDataPointAggregator implements RangeSubAggregator
	{
		@Override
		public Iterable<DataPoint> getNextDataPoints(long returnTime, Iterator<DataPoint> dataPointRange)
		{
			long count = 0;
			while (dataPointRange.hasNext())
			{
				count++;
				dataPointRange.next();
			}
			return Collections.singletonList(m_dataPointFactory.createDataPoint(returnTime, count));
		}
	}
}